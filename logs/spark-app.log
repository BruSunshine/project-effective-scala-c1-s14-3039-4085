23/12/01 14:03:17 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:03:17 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:03:17 INFO xnio: XNIO version 3.8.7.Final
23/12/01 14:03:17 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/01 14:03:17 INFO threads: JBoss Threads version 3.1.0.Final
23/12/01 14:03:17 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:03:17 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:03:17 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:03:17 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:03:17 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:03:17 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:03:17 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:03:17 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:03:17 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:03:17 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:06:03 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:06:03 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:06:03 INFO xnio: XNIO version 3.8.7.Final
23/12/01 14:06:03 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/01 14:06:03 INFO threads: JBoss Threads version 3.1.0.Final
23/12/01 14:06:03 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:06:03 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:06:03 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:06:03 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:06:03 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:06:03 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:06:03 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:06:03 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:06:03 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:06:03 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:06:15 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/01 14:06:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/01 14:06:15 INFO SparkContext: Running Spark version 3.5.0
23/12/01 14:06:15 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/01 14:06:15 INFO SparkContext: Java version 11.0.20
23/12/01 14:06:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/01 14:06:15 INFO ResourceUtils: ==============================================================
23/12/01 14:06:15 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/01 14:06:15 INFO ResourceUtils: ==============================================================
23/12/01 14:06:15 INFO SparkContext: Submitted application: sparkApp
23/12/01 14:06:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/01 14:06:15 INFO ResourceProfile: Limiting resource is cpu
23/12/01 14:06:15 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/01 14:06:15 INFO SecurityManager: Changing view acls to: bsoleille
23/12/01 14:06:15 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/01 14:06:15 INFO SecurityManager: Changing view acls groups to: 
23/12/01 14:06:15 INFO SecurityManager: Changing modify acls groups to: 
23/12/01 14:06:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/01 14:06:16 INFO Utils: Successfully started service 'sparkDriver' on port 46553.
23/12/01 14:06:16 INFO SparkEnv: Registering MapOutputTracker
23/12/01 14:06:16 INFO SparkEnv: Registering BlockManagerMaster
23/12/01 14:06:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/01 14:06:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/01 14:06:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/01 14:06:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dd78f5f2-a984-4124-8d67-bdb3735edc5f
23/12/01 14:06:16 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/12/01 14:06:16 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/01 14:06:16 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/01 14:06:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/01 14:06:16 INFO Utils: Successfully started service 'SparkUI' on port 4041.
23/12/01 14:06:16 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/01 14:06:16 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/01 14:06:16 INFO Executor: Java version 11.0.20
23/12/01 14:06:16 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/01 14:06:16 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@98ac0fe for default.
23/12/01 14:06:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42381.
23/12/01 14:06:16 INFO NettyBlockTransferService: Server created on 10.25.86.80:42381
23/12/01 14:06:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/01 14:06:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 42381, None)
23/12/01 14:06:16 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:42381 with 434.4 MiB RAM, BlockManagerId(driver, 10.25.86.80, 42381, None)
23/12/01 14:06:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 42381, None)
23/12/01 14:06:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 42381, None)
23/12/01 14:06:16 WARN SharedState: URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
23/12/01 14:06:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/01 14:06:16 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/01 14:06:17 INFO CodeGenerator: Code generated in 95.170813 ms
23/12/01 14:06:18 INFO SparkContext: Starting job: show at ApiTest.worksheet.sc:166
23/12/01 14:06:18 INFO DAGScheduler: Got job 0 (show at ApiTest.worksheet.sc:166) with 1 output partitions
23/12/01 14:06:18 INFO DAGScheduler: Final stage: ResultStage 0 (show at ApiTest.worksheet.sc:166)
23/12/01 14:06:18 INFO DAGScheduler: Parents of final stage: List()
23/12/01 14:06:18 INFO DAGScheduler: Missing parents: List()
23/12/01 14:06:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at show at ApiTest.worksheet.sc:166), which has no missing parents
23/12/01 14:06:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 15.5 KiB, free 434.4 MiB)
23/12/01 14:06:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.4 MiB)
23/12/01 14:06:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:42381 (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 14:06:18 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/01 14:06:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at show at ApiTest.worksheet.sc:166) (first 15 tasks are for partitions Vector(0))
23/12/01 14:06:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/01 14:06:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 14:06:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/01 14:06:18 INFO CodeGenerator: Code generated in 9.855458 ms
23/12/01 14:06:18 INFO CodeGenerator: Code generated in 27.07453 ms
23/12/01 14:06:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1677 bytes result sent to driver
23/12/01 14:06:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 149 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 14:06:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/01 14:06:18 INFO DAGScheduler: ResultStage 0 (show at ApiTest.worksheet.sc:166) finished in 0.249 s
23/12/01 14:06:18 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 14:06:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/01 14:06:18 INFO DAGScheduler: Job 0 finished: show at ApiTest.worksheet.sc:166, took 0.269522 s
23/12/01 14:06:18 INFO CodeGenerator: Code generated in 11.845163 ms
23/12/01 14:06:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.25.86.80:42381 in memory (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 14:06:18 INFO CodeGenerator: Code generated in 7.796168 ms
23/12/01 14:06:18 INFO SparkContext: Starting job: show at ApiTest.worksheet.sc:166
23/12/01 14:06:18 INFO DAGScheduler: Got job 1 (show at ApiTest.worksheet.sc:166) with 1 output partitions
23/12/01 14:06:18 INFO DAGScheduler: Final stage: ResultStage 1 (show at ApiTest.worksheet.sc:166)
23/12/01 14:06:18 INFO DAGScheduler: Parents of final stage: List()
23/12/01 14:06:18 INFO DAGScheduler: Missing parents: List()
23/12/01 14:06:18 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at show at ApiTest.worksheet.sc:166), which has no missing parents
23/12/01 14:06:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 15.5 KiB, free 434.4 MiB)
23/12/01 14:06:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.4 MiB)
23/12/01 14:06:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.25.86.80:42381 (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 14:06:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/12/01 14:06:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at show at ApiTest.worksheet.sc:166) (first 15 tasks are for partitions Vector(0))
23/12/01 14:06:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/12/01 14:06:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/01 14:06:18 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/12/01 14:06:18 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1634 bytes result sent to driver
23/12/01 14:06:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 14:06:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/01 14:06:18 INFO DAGScheduler: ResultStage 1 (show at ApiTest.worksheet.sc:166) finished in 0.018 s
23/12/01 14:06:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 14:06:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/12/01 14:06:18 INFO DAGScheduler: Job 1 finished: show at ApiTest.worksheet.sc:166, took 0.019744 s
23/12/01 14:06:18 INFO CodeGenerator: Code generated in 7.586941 ms
23/12/01 14:06:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.25.86.80:42381 in memory (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 14:06:57 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:06:57 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:06:57 INFO xnio: XNIO version 3.8.7.Final
23/12/01 14:06:57 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/01 14:06:57 INFO threads: JBoss Threads version 3.1.0.Final
23/12/01 14:06:57 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:06:57 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:06:57 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:06:57 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:06:57 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:06:57 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:06:57 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:06:57 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:06:57 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 14:06:57 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 14:47:04 INFO CodeGenerator: Code generated in 8.44424 ms
23/12/01 14:47:04 INFO SparkContext: Starting job: show at ApiTest.worksheet.sc:166
23/12/01 14:47:04 INFO DAGScheduler: Got job 2 (show at ApiTest.worksheet.sc:166) with 1 output partitions
23/12/01 14:47:04 INFO DAGScheduler: Final stage: ResultStage 2 (show at ApiTest.worksheet.sc:166)
23/12/01 14:47:04 INFO DAGScheduler: Parents of final stage: List()
23/12/01 14:47:04 INFO DAGScheduler: Missing parents: List()
23/12/01 14:47:04 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at show at ApiTest.worksheet.sc:166), which has no missing parents
23/12/01 14:47:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.5 KiB, free 434.4 MiB)
23/12/01 14:47:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.4 MiB)
23/12/01 14:47:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.25.86.80:42381 (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 14:47:04 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
23/12/01 14:47:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at show at ApiTest.worksheet.sc:166) (first 15 tasks are for partitions Vector(0))
23/12/01 14:47:04 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
23/12/01 14:47:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/01 14:47:04 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
23/12/01 14:47:05 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1677 bytes result sent to driver
23/12/01 14:47:05 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 17 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 14:47:05 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
23/12/01 14:47:05 INFO DAGScheduler: ResultStage 2 (show at ApiTest.worksheet.sc:166) finished in 0.059 s
23/12/01 14:47:05 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 14:47:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
23/12/01 14:47:05 INFO DAGScheduler: Job 2 finished: show at ApiTest.worksheet.sc:166, took 0.061582 s
23/12/01 14:47:05 INFO CodeGenerator: Code generated in 7.660221 ms
23/12/01 14:47:05 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.25.86.80:42381 in memory (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 15:37:12 INFO CodeGenerator: Code generated in 8.801434 ms
23/12/01 15:37:12 INFO SparkContext: Starting job: show at ApiTest.worksheet.sc:166
23/12/01 15:37:12 INFO DAGScheduler: Got job 3 (show at ApiTest.worksheet.sc:166) with 1 output partitions
23/12/01 15:37:12 INFO DAGScheduler: Final stage: ResultStage 3 (show at ApiTest.worksheet.sc:166)
23/12/01 15:37:12 INFO DAGScheduler: Parents of final stage: List()
23/12/01 15:37:12 INFO DAGScheduler: Missing parents: List()
23/12/01 15:37:12 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at show at ApiTest.worksheet.sc:166), which has no missing parents
23/12/01 15:37:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.4 MiB)
23/12/01 15:37:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.4 MiB)
23/12/01 15:37:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.25.86.80:42381 (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 15:37:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
23/12/01 15:37:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at show at ApiTest.worksheet.sc:166) (first 15 tasks are for partitions Vector(0))
23/12/01 15:37:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
23/12/01 15:37:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/01 15:37:12 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
23/12/01 15:37:12 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1677 bytes result sent to driver
23/12/01 15:37:12 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 20 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 15:37:12 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
23/12/01 15:37:12 INFO DAGScheduler: ResultStage 3 (show at ApiTest.worksheet.sc:166) finished in 0.073 s
23/12/01 15:37:12 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 15:37:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
23/12/01 15:37:12 INFO DAGScheduler: Job 3 finished: show at ApiTest.worksheet.sc:166, took 0.076059 s
23/12/01 15:37:12 INFO CodeGenerator: Code generated in 7.669022 ms
23/12/01 15:37:12 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.25.86.80:42381 in memory (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:07:58 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/01 16:07:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/01 16:07:58 INFO SparkContext: Running Spark version 3.5.0
23/12/01 16:07:58 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/01 16:07:58 INFO SparkContext: Java version 11.0.20
23/12/01 16:07:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/01 16:07:59 INFO ResourceUtils: ==============================================================
23/12/01 16:07:59 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/01 16:07:59 INFO ResourceUtils: ==============================================================
23/12/01 16:07:59 INFO SparkContext: Submitted application: Spark Parquet Example
23/12/01 16:07:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/01 16:07:59 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
23/12/01 16:07:59 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/01 16:07:59 INFO SecurityManager: Changing view acls to: bsoleille
23/12/01 16:07:59 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/01 16:07:59 INFO SecurityManager: Changing view acls groups to: 
23/12/01 16:07:59 INFO SecurityManager: Changing modify acls groups to: 
23/12/01 16:07:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/01 16:07:59 INFO Utils: Successfully started service 'sparkDriver' on port 40271.
23/12/01 16:07:59 INFO SparkEnv: Registering MapOutputTracker
23/12/01 16:07:59 INFO SparkEnv: Registering BlockManagerMaster
23/12/01 16:07:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/01 16:07:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/01 16:07:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/01 16:07:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-82212edc-6bea-4532-94f2-9ee9645b988b
23/12/01 16:07:59 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/12/01 16:07:59 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/01 16:07:59 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/01 16:07:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/01 16:07:59 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/01 16:07:59 INFO Utils: Successfully started service 'SparkUI' on port 4042.
23/12/01 16:07:59 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/01 16:07:59 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/01 16:07:59 INFO Executor: Java version 11.0.20
23/12/01 16:07:59 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/01 16:07:59 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5d46bed7 for default.
23/12/01 16:07:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34297.
23/12/01 16:07:59 INFO NettyBlockTransferService: Server created on 10.25.86.80:34297
23/12/01 16:07:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/01 16:07:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 34297, None)
23/12/01 16:07:59 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:34297 with 434.4 MiB RAM, BlockManagerId(driver, 10.25.86.80, 34297, None)
23/12/01 16:07:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 34297, None)
23/12/01 16:07:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 34297, None)
23/12/01 16:07:59 WARN SharedState: URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
23/12/01 16:07:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/01 16:07:59 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/01 16:08:01 INFO CodeGenerator: Code generated in 119.825005 ms
23/12/01 16:08:01 INFO SparkContext: Starting job: show at SparkTest.worksheet.sc:62
23/12/01 16:08:01 INFO DAGScheduler: Got job 0 (show at SparkTest.worksheet.sc:62) with 1 output partitions
23/12/01 16:08:01 INFO DAGScheduler: Final stage: ResultStage 0 (show at SparkTest.worksheet.sc:62)
23/12/01 16:08:01 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:01 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:01 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at show at SparkTest.worksheet.sc:62), which has no missing parents
23/12/01 16:08:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 15.5 KiB, free 434.4 MiB)
23/12/01 16:08:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.4 MiB)
23/12/01 16:08:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:34297 (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 16:08:01 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at show at SparkTest.worksheet.sc:62) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/01 16:08:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:08:01 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/01 16:08:01 INFO CodeGenerator: Code generated in 9.331211 ms
23/12/01 16:08:01 INFO CodeGenerator: Code generated in 27.657261 ms
23/12/01 16:08:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1677 bytes result sent to driver
23/12/01 16:08:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 143 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/01 16:08:01 INFO DAGScheduler: ResultStage 0 (show at SparkTest.worksheet.sc:62) finished in 0.302 s
23/12/01 16:08:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/01 16:08:01 INFO DAGScheduler: Job 0 finished: show at SparkTest.worksheet.sc:62, took 0.329990 s
23/12/01 16:08:01 INFO CodeGenerator: Code generated in 7.616697 ms
23/12/01 16:08:01 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/01 16:08:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/12/01 16:08:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/12/01 16:08:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/01 16:08:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/12/01 16:08:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/12/01 16:08:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/01 16:08:01 INFO CodeGenerator: Code generated in 6.182013 ms
23/12/01 16:08:01 INFO SparkContext: Starting job: parquet at SparkTest.worksheet.sc:72
23/12/01 16:08:01 INFO DAGScheduler: Got job 1 (parquet at SparkTest.worksheet.sc:72) with 1 output partitions
23/12/01 16:08:01 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at SparkTest.worksheet.sc:72)
23/12/01 16:08:01 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:01 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:01 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at SparkTest.worksheet.sc:72), which has no missing parents
23/12/01 16:08:01 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 215.6 KiB, free 434.2 MiB)
23/12/01 16:08:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.25.86.80:34297 in memory (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 16:08:01 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 76.7 KiB, free 434.1 MiB)
23/12/01 16:08:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.25.86.80:34297 (size: 76.7 KiB, free: 434.3 MiB)
23/12/01 16:08:01 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at SparkTest.worksheet.sc:72) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/12/01 16:08:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/01 16:08:01 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/12/01 16:08:02 INFO CodeGenerator: Code generated in 6.376131 ms
23/12/01 16:08:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/12/01 16:08:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/12/01 16:08:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/01 16:08:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/12/01 16:08:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/12/01 16:08:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/01 16:08:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "doubleField",
    "type" : "double",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "stringField",
    "type" : "string",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "intField",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 index;
  required double doubleField;
  required binary stringField (STRING);
  required int32 intField;
}

       
23/12/01 16:08:02 INFO CodecPool: Got brand-new compressor [.snappy]
23/12/01 16:08:02 INFO FileOutputCommitter: Saved output of task 'attempt_202312011608014166816365865289635_0001_m_000000_1' to file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/file.parquet/_temporary/0/task_202312011608014166816365865289635_0001_m_000000
23/12/01 16:08:02 INFO SparkHadoopMapRedUtil: attempt_202312011608014166816365865289635_0001_m_000000_1: Committed. Elapsed time: 0 ms.
23/12/01 16:08:02 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2785 bytes result sent to driver
23/12/01 16:08:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 549 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:02 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/01 16:08:02 INFO DAGScheduler: ResultStage 1 (parquet at SparkTest.worksheet.sc:72) finished in 0.591 s
23/12/01 16:08:02 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/12/01 16:08:02 INFO DAGScheduler: Job 1 finished: parquet at SparkTest.worksheet.sc:72, took 0.594133 s
23/12/01 16:08:02 INFO FileFormatWriter: Start to commit write Job aecacddf-8b99-4682-8172-557c073396a7.
23/12/01 16:08:02 INFO FileFormatWriter: Write Job aecacddf-8b99-4682-8172-557c073396a7 committed. Elapsed time: 10 ms.
23/12/01 16:08:02 INFO FileFormatWriter: Finished processing stats for write job aecacddf-8b99-4682-8172-557c073396a7.
23/12/01 16:08:02 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
23/12/01 16:08:02 INFO SparkContext: Starting job: parquet at SparkTest.worksheet.sc:76
23/12/01 16:08:02 INFO DAGScheduler: Got job 2 (parquet at SparkTest.worksheet.sc:76) with 1 output partitions
23/12/01 16:08:02 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at SparkTest.worksheet.sc:76)
23/12/01 16:08:02 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:02 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:02 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at parquet at SparkTest.worksheet.sc:76), which has no missing parents
23/12/01 16:08:02 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.9 KiB, free 434.0 MiB)
23/12/01 16:08:02 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.0 MiB)
23/12/01 16:08:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.25.86.80:34297 (size: 36.9 KiB, free: 434.3 MiB)
23/12/01 16:08:02 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at parquet at SparkTest.worksheet.sc:76) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:02 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
23/12/01 16:08:02 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8013 bytes) 
23/12/01 16:08:02 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
23/12/01 16:08:02 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2128 bytes result sent to driver
23/12/01 16:08:02 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 57 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:02 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
23/12/01 16:08:02 INFO DAGScheduler: ResultStage 2 (parquet at SparkTest.worksheet.sc:76) finished in 0.069 s
23/12/01 16:08:02 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
23/12/01 16:08:02 INFO DAGScheduler: Job 2 finished: parquet at SparkTest.worksheet.sc:76, took 0.071356 s
23/12/01 16:08:02 INFO FileSourceStrategy: Pushed Filters: 
23/12/01 16:08:02 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/01 16:08:02 INFO CodeGenerator: Code generated in 19.697954 ms
23/12/01 16:08:02 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.8 KiB, free 433.8 MiB)
23/12/01 16:08:02 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 433.7 MiB)
23/12/01 16:08:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.25.86.80:34297 (size: 34.9 KiB, free: 434.3 MiB)
23/12/01 16:08:02 INFO SparkContext: Created broadcast 3 from show at SparkTest.worksheet.sc:81
23/12/01 16:08:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/01 16:08:02 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.25.86.80:34297 in memory (size: 76.7 KiB, free: 434.3 MiB)
23/12/01 16:08:02 INFO SparkContext: Starting job: show at SparkTest.worksheet.sc:81
23/12/01 16:08:02 INFO DAGScheduler: Got job 3 (show at SparkTest.worksheet.sc:81) with 1 output partitions
23/12/01 16:08:02 INFO DAGScheduler: Final stage: ResultStage 3 (show at SparkTest.worksheet.sc:81)
23/12/01 16:08:02 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:02 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:02 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at show at SparkTest.worksheet.sc:81), which has no missing parents
23/12/01 16:08:02 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 16.6 KiB, free 434.0 MiB)
23/12/01 16:08:02 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.0 MiB)
23/12/01 16:08:02 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.25.86.80:34297 (size: 6.8 KiB, free: 434.3 MiB)
23/12/01 16:08:02 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at show at SparkTest.worksheet.sc:81) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:02 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
23/12/01 16:08:02 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8419 bytes) 
23/12/01 16:08:02 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
23/12/01 16:08:02 INFO CodeGenerator: Code generated in 15.732603 ms
23/12/01 16:08:02 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/file.parquet/part-00000-f284260d-afb0-4820-980b-c93c8a88bce7-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/01 16:08:02 INFO CodecPool: Got brand-new decompressor [.snappy]
23/12/01 16:08:02 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.25.86.80:34297 in memory (size: 36.9 KiB, free: 434.4 MiB)
23/12/01 16:08:02 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1971 bytes result sent to driver
23/12/01 16:08:02 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 115 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:02 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
23/12/01 16:08:02 INFO DAGScheduler: ResultStage 3 (show at SparkTest.worksheet.sc:81) finished in 0.137 s
23/12/01 16:08:02 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
23/12/01 16:08:02 INFO DAGScheduler: Job 3 finished: show at SparkTest.worksheet.sc:81, took 0.142872 s
23/12/01 16:08:03 INFO CodeGenerator: Code generated in 7.974989 ms
23/12/01 16:08:03 INFO SparkContext: Starting job: show at SparkTest.worksheet.sc:94
23/12/01 16:08:03 INFO DAGScheduler: Got job 4 (show at SparkTest.worksheet.sc:94) with 1 output partitions
23/12/01 16:08:03 INFO DAGScheduler: Final stage: ResultStage 4 (show at SparkTest.worksheet.sc:94)
23/12/01 16:08:03 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:03 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:03 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[13] at show at SparkTest.worksheet.sc:94), which has no missing parents
23/12/01 16:08:03 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.7 KiB, free 434.1 MiB)
23/12/01 16:08:03 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.1 MiB)
23/12/01 16:08:03 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.25.86.80:34297 (size: 6.5 KiB, free: 434.4 MiB)
23/12/01 16:08:03 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at show at SparkTest.worksheet.sc:94) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:03 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/12/01 16:08:03 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/01 16:08:03 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
23/12/01 16:08:03 INFO CodeGenerator: Code generated in 7.397198 ms
23/12/01 16:08:03 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1640 bytes result sent to driver
23/12/01 16:08:03 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 17 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:03 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/12/01 16:08:03 INFO DAGScheduler: ResultStage 4 (show at SparkTest.worksheet.sc:94) finished in 0.023 s
23/12/01 16:08:03 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/12/01 16:08:03 INFO DAGScheduler: Job 4 finished: show at SparkTest.worksheet.sc:94, took 0.025370 s
23/12/01 16:08:03 INFO CodeGenerator: Code generated in 5.995176 ms
23/12/01 16:08:03 INFO CodeGenerator: Code generated in 11.596649 ms
23/12/01 16:08:03 INFO DAGScheduler: Registering RDD 15 (collect at SparkTest.worksheet.sc:102) as input to shuffle 0
23/12/01 16:08:03 INFO DAGScheduler: Got map stage job 5 (collect at SparkTest.worksheet.sc:102) with 1 output partitions
23/12/01 16:08:03 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (collect at SparkTest.worksheet.sc:102)
23/12/01 16:08:03 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:03 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:03 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[15] at collect at SparkTest.worksheet.sc:102), which has no missing parents
23/12/01 16:08:03 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 19.2 KiB, free 434.1 MiB)
23/12/01 16:08:03 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 434.1 MiB)
23/12/01 16:08:03 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.25.86.80:34297 (size: 8.2 KiB, free: 434.3 MiB)
23/12/01 16:08:03 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.25.86.80:34297 in memory (size: 6.5 KiB, free: 434.4 MiB)
23/12/01 16:08:03 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[15] at collect at SparkTest.worksheet.sc:102) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:03 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.25.86.80:34297 in memory (size: 34.9 KiB, free: 434.4 MiB)
23/12/01 16:08:03 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
23/12/01 16:08:03 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.25.86.80:34297 in memory (size: 6.8 KiB, free: 434.4 MiB)
23/12/01 16:08:03 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/01 16:08:03 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
23/12/01 16:08:03 INFO CodeGenerator: Code generated in 10.114158 ms
23/12/01 16:08:03 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1930 bytes result sent to driver
23/12/01 16:08:03 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 57 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:03 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
23/12/01 16:08:03 INFO DAGScheduler: ShuffleMapStage 5 (collect at SparkTest.worksheet.sc:102) finished in 0.110 s
23/12/01 16:08:03 INFO DAGScheduler: looking for newly runnable stages
23/12/01 16:08:03 INFO DAGScheduler: running: HashSet()
23/12/01 16:08:03 INFO DAGScheduler: waiting: HashSet()
23/12/01 16:08:03 INFO DAGScheduler: failed: HashSet()
23/12/01 16:08:03 INFO CodeGenerator: Code generated in 9.612574 ms
23/12/01 16:08:03 INFO SparkContext: Starting job: collect at SparkTest.worksheet.sc:102
23/12/01 16:08:03 INFO DAGScheduler: Got job 6 (collect at SparkTest.worksheet.sc:102) with 1 output partitions
23/12/01 16:08:03 INFO DAGScheduler: Final stage: ResultStage 7 (collect at SparkTest.worksheet.sc:102)
23/12/01 16:08:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
23/12/01 16:08:03 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:03 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[18] at collect at SparkTest.worksheet.sc:102), which has no missing parents
23/12/01 16:08:03 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.3 KiB, free 434.4 MiB)
23/12/01 16:08:03 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 434.4 MiB)
23/12/01 16:08:03 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.25.86.80:34297 (size: 6.6 KiB, free: 434.4 MiB)
23/12/01 16:08:03 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[18] at collect at SparkTest.worksheet.sc:102) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:03 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
23/12/01 16:08:03 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/01 16:08:03 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)
23/12/01 16:08:03 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
23/12/01 16:08:03 INFO CodeGenerator: Code generated in 6.711882 ms
23/12/01 16:08:03 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 4084 bytes result sent to driver
23/12/01 16:08:03 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 52 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:03 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
23/12/01 16:08:03 INFO DAGScheduler: ResultStage 7 (collect at SparkTest.worksheet.sc:102) finished in 0.060 s
23/12/01 16:08:03 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
23/12/01 16:08:03 INFO DAGScheduler: Job 6 finished: collect at SparkTest.worksheet.sc:102, took 0.069896 s
23/12/01 16:08:03 INFO CodeGenerator: Code generated in 4.618758 ms
23/12/01 16:08:03 INFO SparkContext: Starting job: show at Ast.scala:95
23/12/01 16:08:03 INFO DAGScheduler: Got job 7 (show at Ast.scala:95) with 1 output partitions
23/12/01 16:08:03 INFO DAGScheduler: Final stage: ResultStage 8 (show at Ast.scala:95)
23/12/01 16:08:03 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:03 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:03 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[20] at show at Ast.scala:95), which has no missing parents
23/12/01 16:08:03 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 15.5 KiB, free 434.3 MiB)
23/12/01 16:08:03 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.3 MiB)
23/12/01 16:08:03 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.25.86.80:34297 (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 16:08:03 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[20] at show at Ast.scala:95) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:03 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
23/12/01 16:08:03 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:08:03 INFO Executor: Running task 0.0 in stage 8.0 (TID 7)
23/12/01 16:08:03 INFO Executor: Finished task 0.0 in stage 8.0 (TID 7). 1634 bytes result sent to driver
23/12/01 16:08:03 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 7 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:03 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
23/12/01 16:08:03 INFO DAGScheduler: ResultStage 8 (show at Ast.scala:95) finished in 0.014 s
23/12/01 16:08:03 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
23/12/01 16:08:03 INFO DAGScheduler: Job 7 finished: show at Ast.scala:95, took 0.017157 s
23/12/01 16:08:03 INFO SparkContext: Starting job: show at Ast.scala:96
23/12/01 16:08:03 INFO DAGScheduler: Got job 8 (show at Ast.scala:96) with 1 output partitions
23/12/01 16:08:03 INFO DAGScheduler: Final stage: ResultStage 9 (show at Ast.scala:96)
23/12/01 16:08:03 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:03 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:03 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[22] at show at Ast.scala:96), which has no missing parents
23/12/01 16:08:03 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.5 KiB, free 434.3 MiB)
23/12/01 16:08:03 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.3 MiB)
23/12/01 16:08:03 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.25.86.80:34297 (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 16:08:03 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[22] at show at Ast.scala:96) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:03 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
23/12/01 16:08:03 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:08:03 INFO Executor: Running task 0.0 in stage 9.0 (TID 8)
23/12/01 16:08:03 INFO Executor: Finished task 0.0 in stage 9.0 (TID 8). 1634 bytes result sent to driver
23/12/01 16:08:03 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 7 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:03 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
23/12/01 16:08:03 INFO DAGScheduler: ResultStage 9 (show at Ast.scala:96) finished in 0.014 s
23/12/01 16:08:03 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
23/12/01 16:08:03 INFO DAGScheduler: Job 8 finished: show at Ast.scala:96, took 0.016066 s
23/12/01 16:08:03 INFO DAGScheduler: Registering RDD 24 (show at Ast.scala:108) as input to shuffle 1
23/12/01 16:08:03 INFO DAGScheduler: Got map stage job 9 (show at Ast.scala:108) with 1 output partitions
23/12/01 16:08:03 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (show at Ast.scala:108)
23/12/01 16:08:03 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:03 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:03 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[24] at show at Ast.scala:108), which has no missing parents
23/12/01 16:08:03 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 17.7 KiB, free 434.3 MiB)
23/12/01 16:08:03 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 434.3 MiB)
23/12/01 16:08:03 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.25.86.80:34297 (size: 7.6 KiB, free: 434.4 MiB)
23/12/01 16:08:03 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.25.86.80:34297 in memory (size: 8.2 KiB, free: 434.4 MiB)
23/12/01 16:08:03 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[24] at show at Ast.scala:108) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:03 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
23/12/01 16:08:03 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.25.86.80:34297 in memory (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 16:08:03 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/01 16:08:03 INFO Executor: Running task 0.0 in stage 10.0 (TID 9)
23/12/01 16:08:03 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.25.86.80:34297 in memory (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 16:08:03 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.25.86.80:34297 in memory (size: 6.6 KiB, free: 434.4 MiB)
23/12/01 16:08:03 INFO CodeGenerator: Code generated in 6.584303 ms
23/12/01 16:08:03 INFO Executor: Finished task 0.0 in stage 10.0 (TID 9). 1991 bytes result sent to driver
23/12/01 16:08:03 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 25 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:03 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
23/12/01 16:08:03 INFO DAGScheduler: ShuffleMapStage 10 (show at Ast.scala:108) finished in 0.044 s
23/12/01 16:08:03 INFO DAGScheduler: looking for newly runnable stages
23/12/01 16:08:03 INFO DAGScheduler: running: HashSet()
23/12/01 16:08:03 INFO DAGScheduler: waiting: HashSet()
23/12/01 16:08:03 INFO DAGScheduler: failed: HashSet()
23/12/01 16:08:03 INFO ShufflePartitionsUtil: For shuffle(1, 1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/01 16:08:03 INFO CodeGenerator: Code generated in 19.930057 ms
23/12/01 16:08:03 INFO CodeGenerator: Code generated in 6.776074 ms
23/12/01 16:08:03 INFO CodeGenerator: Code generated in 5.711414 ms
23/12/01 16:08:03 INFO SparkContext: Starting job: show at Ast.scala:108
23/12/01 16:08:03 INFO DAGScheduler: Got job 10 (show at Ast.scala:108) with 1 output partitions
23/12/01 16:08:03 INFO DAGScheduler: Final stage: ResultStage 12 (show at Ast.scala:108)
23/12/01 16:08:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
23/12/01 16:08:03 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:03 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[31] at show at Ast.scala:108), which has no missing parents
23/12/01 16:08:03 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 40.1 KiB, free 434.3 MiB)
23/12/01 16:08:03 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 434.3 MiB)
23/12/01 16:08:03 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.25.86.80:34297 (size: 16.3 KiB, free: 434.4 MiB)
23/12/01 16:08:03 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[31] at show at Ast.scala:108) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:03 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
23/12/01 16:08:03 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 8193 bytes) 
23/12/01 16:08:03 INFO Executor: Running task 0.0 in stage 12.0 (TID 10)
23/12/01 16:08:03 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
23/12/01 16:08:03 INFO CodeGenerator: Code generated in 6.275348 ms
23/12/01 16:08:03 INFO CodeGenerator: Code generated in 14.634942 ms
23/12/01 16:08:03 INFO CodeGenerator: Code generated in 3.977138 ms
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 6.754925 ms
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 11.223649 ms
23/12/01 16:08:04 INFO Executor: Finished task 0.0 in stage 12.0 (TID 10). 4898 bytes result sent to driver
23/12/01 16:08:04 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 99 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:04 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
23/12/01 16:08:04 INFO DAGScheduler: ResultStage 12 (show at Ast.scala:108) finished in 0.110 s
23/12/01 16:08:04 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
23/12/01 16:08:04 INFO DAGScheduler: Job 10 finished: show at Ast.scala:108, took 0.113726 s
23/12/01 16:08:04 INFO DAGScheduler: Registering RDD 33 (show at Ast.scala:112) as input to shuffle 2
23/12/01 16:08:04 INFO DAGScheduler: Got map stage job 11 (show at Ast.scala:112) with 1 output partitions
23/12/01 16:08:04 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (show at Ast.scala:112)
23/12/01 16:08:04 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:04 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:04 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[33] at show at Ast.scala:112), which has no missing parents
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 17.7 KiB, free 434.3 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 434.3 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.25.86.80:34297 (size: 7.6 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[33] at show at Ast.scala:112) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:04 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
23/12/01 16:08:04 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 11) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/01 16:08:04 INFO Executor: Running task 0.0 in stage 13.0 (TID 11)
23/12/01 16:08:04 INFO Executor: Finished task 0.0 in stage 13.0 (TID 11). 1991 bytes result sent to driver
23/12/01 16:08:04 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 11) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:04 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
23/12/01 16:08:04 INFO DAGScheduler: ShuffleMapStage 13 (show at Ast.scala:112) finished in 0.018 s
23/12/01 16:08:04 INFO DAGScheduler: looking for newly runnable stages
23/12/01 16:08:04 INFO DAGScheduler: running: HashSet()
23/12/01 16:08:04 INFO DAGScheduler: waiting: HashSet()
23/12/01 16:08:04 INFO DAGScheduler: failed: HashSet()
23/12/01 16:08:04 INFO ShufflePartitionsUtil: For shuffle(2, 2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/01 16:08:04 INFO SparkContext: Starting job: show at Ast.scala:112
23/12/01 16:08:04 INFO DAGScheduler: Got job 12 (show at Ast.scala:112) with 1 output partitions
23/12/01 16:08:04 INFO DAGScheduler: Final stage: ResultStage 15 (show at Ast.scala:112)
23/12/01 16:08:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
23/12/01 16:08:04 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:04 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[40] at show at Ast.scala:112), which has no missing parents
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 40.1 KiB, free 434.3 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 434.2 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.25.86.80:34297 (size: 16.3 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[40] at show at Ast.scala:112) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:04 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
23/12/01 16:08:04 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 8193 bytes) 
23/12/01 16:08:04 INFO Executor: Running task 0.0 in stage 15.0 (TID 12)
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/01 16:08:04 INFO Executor: Finished task 0.0 in stage 15.0 (TID 12). 4898 bytes result sent to driver
23/12/01 16:08:04 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:04 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
23/12/01 16:08:04 INFO DAGScheduler: ResultStage 15 (show at Ast.scala:112) finished in 0.021 s
23/12/01 16:08:04 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
23/12/01 16:08:04 INFO DAGScheduler: Job 12 finished: show at Ast.scala:112, took 0.024646 s
23/12/01 16:08:04 INFO SparkContext: Starting job: show at Ast.scala:113
23/12/01 16:08:04 INFO DAGScheduler: Got job 13 (show at Ast.scala:113) with 1 output partitions
23/12/01 16:08:04 INFO DAGScheduler: Final stage: ResultStage 16 (show at Ast.scala:113)
23/12/01 16:08:04 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:04 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:04 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[42] at show at Ast.scala:113), which has no missing parents
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.2 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.25.86.80:34297 (size: 6.4 KiB, free: 434.3 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 10.25.86.80:34297 in memory (size: 16.3 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[42] at show at Ast.scala:113) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:04 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
23/12/01 16:08:04 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 13) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:08:04 INFO Executor: Running task 0.0 in stage 16.0 (TID 13)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 10.25.86.80:34297 in memory (size: 16.3 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 10.25.86.80:34297 in memory (size: 7.6 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 10.25.86.80:34297 in memory (size: 7.6 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO Executor: Finished task 0.0 in stage 16.0 (TID 13). 1634 bytes result sent to driver
23/12/01 16:08:04 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 13) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:04 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
23/12/01 16:08:04 INFO DAGScheduler: ResultStage 16 (show at Ast.scala:113) finished in 0.026 s
23/12/01 16:08:04 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
23/12/01 16:08:04 INFO DAGScheduler: Job 13 finished: show at Ast.scala:113, took 0.028865 s
23/12/01 16:08:04 INFO DAGScheduler: Registering RDD 44 (show at Ast.scala:126) as input to shuffle 3
23/12/01 16:08:04 INFO DAGScheduler: Got map stage job 14 (show at Ast.scala:126) with 1 output partitions
23/12/01 16:08:04 INFO DAGScheduler: Final stage: ShuffleMapStage 17 (show at Ast.scala:126)
23/12/01 16:08:04 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:04 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:04 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[44] at show at Ast.scala:126), which has no missing parents
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 17.7 KiB, free 434.4 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 434.4 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.25.86.80:34297 (size: 7.6 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[44] at show at Ast.scala:126) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:04 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
23/12/01 16:08:04 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 14) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/01 16:08:04 INFO Executor: Running task 0.0 in stage 17.0 (TID 14)
23/12/01 16:08:04 INFO Executor: Finished task 0.0 in stage 17.0 (TID 14). 1991 bytes result sent to driver
23/12/01 16:08:04 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 14) in 14 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:04 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
23/12/01 16:08:04 INFO DAGScheduler: ShuffleMapStage 17 (show at Ast.scala:126) finished in 0.023 s
23/12/01 16:08:04 INFO DAGScheduler: looking for newly runnable stages
23/12/01 16:08:04 INFO DAGScheduler: running: HashSet()
23/12/01 16:08:04 INFO DAGScheduler: waiting: HashSet()
23/12/01 16:08:04 INFO DAGScheduler: failed: HashSet()
23/12/01 16:08:04 INFO ShufflePartitionsUtil: For shuffle(3, 3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 11.220167 ms
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 9.983091 ms
23/12/01 16:08:04 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/01 16:08:04 INFO DAGScheduler: Got job 15 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/01 16:08:04 INFO DAGScheduler: Final stage: ResultStage 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/01 16:08:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)
23/12/01 16:08:04 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:04 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[51] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 38.7 KiB, free 434.3 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.25.86.80:34297 (size: 15.9 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[51] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:04 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
23/12/01 16:08:04 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 8193 bytes) 
23/12/01 16:08:04 INFO Executor: Running task 0.0 in stage 19.0 (TID 15)
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 7.920069 ms
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 10.494762 ms
23/12/01 16:08:04 INFO Executor: Finished task 0.0 in stage 19.0 (TID 15). 4897 bytes result sent to driver
23/12/01 16:08:04 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 34 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:04 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
23/12/01 16:08:04 INFO DAGScheduler: ResultStage 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.040 s
23/12/01 16:08:04 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
23/12/01 16:08:04 INFO DAGScheduler: Job 15 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.060623 s
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 4.251137 ms
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 1024.1 KiB, free 433.3 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 337.0 B, free 433.3 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.25.86.80:34297 (size: 337.0 B, free: 434.4 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 17 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/01 16:08:04 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 8.084337 ms
23/12/01 16:08:04 INFO SparkContext: Starting job: show at Ast.scala:126
23/12/01 16:08:04 INFO DAGScheduler: Got job 16 (show at Ast.scala:126) with 1 output partitions
23/12/01 16:08:04 INFO DAGScheduler: Final stage: ResultStage 21 (show at Ast.scala:126)
23/12/01 16:08:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
23/12/01 16:08:04 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:04 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[54] at show at Ast.scala:126), which has no missing parents
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 15.1 KiB, free 433.3 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 433.3 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.25.86.80:34297 (size: 6.9 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[54] at show at Ast.scala:126) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:04 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
23/12/01 16:08:04 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 16) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7714 bytes) 
23/12/01 16:08:04 INFO Executor: Running task 0.0 in stage 21.0 (TID 16)
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 7.452804 ms
23/12/01 16:08:04 INFO Executor: Finished task 0.0 in stage 21.0 (TID 16). 4165 bytes result sent to driver
23/12/01 16:08:04 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 16) in 14 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:04 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
23/12/01 16:08:04 INFO DAGScheduler: ResultStage 21 (show at Ast.scala:126) finished in 0.020 s
23/12/01 16:08:04 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
23/12/01 16:08:04 INFO DAGScheduler: Job 16 finished: show at Ast.scala:126, took 0.022829 s
23/12/01 16:08:04 INFO DAGScheduler: Registering RDD 56 (show at SparkTest.worksheet.sc:115) as input to shuffle 4
23/12/01 16:08:04 INFO DAGScheduler: Got map stage job 17 (show at SparkTest.worksheet.sc:115) with 1 output partitions
23/12/01 16:08:04 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (show at SparkTest.worksheet.sc:115)
23/12/01 16:08:04 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:04 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:04 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[56] at show at SparkTest.worksheet.sc:115), which has no missing parents
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 17.7 KiB, free 433.3 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.3 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.25.86.80:34297 (size: 7.6 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 10.25.86.80:34297 in memory (size: 7.6 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[56] at show at SparkTest.worksheet.sc:115) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:04 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
23/12/01 16:08:04 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/01 16:08:04 INFO Executor: Running task 0.0 in stage 22.0 (TID 17)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 10.25.86.80:34297 in memory (size: 337.0 B, free: 434.4 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 10.25.86.80:34297 in memory (size: 6.9 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 10.25.86.80:34297 in memory (size: 15.9 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 10.25.86.80:34297 in memory (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO Executor: Finished task 0.0 in stage 22.0 (TID 17). 1991 bytes result sent to driver
23/12/01 16:08:04 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:04 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
23/12/01 16:08:04 INFO DAGScheduler: ShuffleMapStage 22 (show at SparkTest.worksheet.sc:115) finished in 0.029 s
23/12/01 16:08:04 INFO DAGScheduler: looking for newly runnable stages
23/12/01 16:08:04 INFO DAGScheduler: running: HashSet()
23/12/01 16:08:04 INFO DAGScheduler: waiting: HashSet()
23/12/01 16:08:04 INFO DAGScheduler: failed: HashSet()
23/12/01 16:08:04 INFO ShufflePartitionsUtil: For shuffle(4, 4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/01 16:08:04 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/01 16:08:04 INFO DAGScheduler: Got job 18 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/01 16:08:04 INFO DAGScheduler: Final stage: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/01 16:08:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)
23/12/01 16:08:04 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:04 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[63] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 38.7 KiB, free 434.3 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 434.3 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.25.86.80:34297 (size: 15.9 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[63] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:04 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
23/12/01 16:08:04 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 18) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 8193 bytes) 
23/12/01 16:08:04 INFO Executor: Running task 0.0 in stage 24.0 (TID 18)
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/01 16:08:04 INFO Executor: Finished task 0.0 in stage 24.0 (TID 18). 4897 bytes result sent to driver
23/12/01 16:08:04 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 18) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:04 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
23/12/01 16:08:04 INFO DAGScheduler: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.016 s
23/12/01 16:08:04 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
23/12/01 16:08:04 INFO DAGScheduler: Job 18 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.018536 s
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 1024.1 KiB, free 433.3 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 337.0 B, free 433.3 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.25.86.80:34297 (size: 337.0 B, free: 434.4 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 21 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/01 16:08:04 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/01 16:08:04 INFO SparkContext: Starting job: show at SparkTest.worksheet.sc:115
23/12/01 16:08:04 INFO DAGScheduler: Got job 19 (show at SparkTest.worksheet.sc:115) with 1 output partitions
23/12/01 16:08:04 INFO DAGScheduler: Final stage: ResultStage 26 (show at SparkTest.worksheet.sc:115)
23/12/01 16:08:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
23/12/01 16:08:04 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:04 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[66] at show at SparkTest.worksheet.sc:115), which has no missing parents
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 15.1 KiB, free 433.3 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 433.3 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.25.86.80:34297 (size: 6.9 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[66] at show at SparkTest.worksheet.sc:115) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:04 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
23/12/01 16:08:04 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 19) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7714 bytes) 
23/12/01 16:08:04 INFO Executor: Running task 0.0 in stage 26.0 (TID 19)
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/01 16:08:04 INFO Executor: Finished task 0.0 in stage 26.0 (TID 19). 4165 bytes result sent to driver
23/12/01 16:08:04 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 19) in 7 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:04 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
23/12/01 16:08:04 INFO DAGScheduler: ResultStage 26 (show at SparkTest.worksheet.sc:115) finished in 0.014 s
23/12/01 16:08:04 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
23/12/01 16:08:04 INFO DAGScheduler: Job 19 finished: show at SparkTest.worksheet.sc:115, took 0.015829 s
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 3.884453 ms
23/12/01 16:08:04 INFO DAGScheduler: Registering RDD 68 (isEmpty at SparkTest.worksheet.sc:120) as input to shuffle 5
23/12/01 16:08:04 INFO DAGScheduler: Got map stage job 20 (isEmpty at SparkTest.worksheet.sc:120) with 1 output partitions
23/12/01 16:08:04 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (isEmpty at SparkTest.worksheet.sc:120)
23/12/01 16:08:04 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:04 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:04 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[68] at isEmpty at SparkTest.worksheet.sc:120), which has no missing parents
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 17.3 KiB, free 433.3 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 433.3 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.25.86.80:34297 (size: 7.5 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[68] at isEmpty at SparkTest.worksheet.sc:120) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:04 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
23/12/01 16:08:04 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 20) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/01 16:08:04 INFO Executor: Running task 0.0 in stage 27.0 (TID 20)
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 3.777078 ms
23/12/01 16:08:04 INFO Executor: Finished task 0.0 in stage 27.0 (TID 20). 1991 bytes result sent to driver
23/12/01 16:08:04 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 20) in 17 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:04 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
23/12/01 16:08:04 INFO DAGScheduler: ShuffleMapStage 27 (isEmpty at SparkTest.worksheet.sc:120) finished in 0.021 s
23/12/01 16:08:04 INFO DAGScheduler: looking for newly runnable stages
23/12/01 16:08:04 INFO DAGScheduler: running: HashSet()
23/12/01 16:08:04 INFO DAGScheduler: waiting: HashSet()
23/12/01 16:08:04 INFO DAGScheduler: failed: HashSet()
23/12/01 16:08:04 INFO ShufflePartitionsUtil: For shuffle(5, 5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 23.968396 ms
23/12/01 16:08:04 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/01 16:08:04 INFO DAGScheduler: Got job 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/01 16:08:04 INFO DAGScheduler: Final stage: ResultStage 29 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/01 16:08:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
23/12/01 16:08:04 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:04 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[75] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 35.3 KiB, free 433.2 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 14.5 KiB, free 433.2 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.25.86.80:34297 (size: 14.5 KiB, free: 434.3 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[75] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:04 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
23/12/01 16:08:04 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 21) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 8193 bytes) 
23/12/01 16:08:04 INFO Executor: Running task 0.0 in stage 29.0 (TID 21)
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Getting 1 (300.0 B) non-empty blocks including 1 (300.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Getting 1 (300.0 B) non-empty blocks including 1 (300.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 7.013913 ms
23/12/01 16:08:04 INFO Executor: Finished task 0.0 in stage 29.0 (TID 21). 4817 bytes result sent to driver
23/12/01 16:08:04 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 21) in 17 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:04 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
23/12/01 16:08:04 INFO DAGScheduler: ResultStage 29 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.022 s
23/12/01 16:08:04 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
23/12/01 16:08:04 INFO DAGScheduler: Job 21 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.025596 s
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 1024.1 KiB, free 432.2 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 251.0 B, free 432.2 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.25.86.80:34297 (size: 251.0 B, free: 434.3 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 25 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/01 16:08:04 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 4.138849 ms
23/12/01 16:08:04 INFO SparkContext: Starting job: isEmpty at SparkTest.worksheet.sc:120
23/12/01 16:08:04 INFO DAGScheduler: Got job 22 (isEmpty at SparkTest.worksheet.sc:120) with 1 output partitions
23/12/01 16:08:04 INFO DAGScheduler: Final stage: ResultStage 31 (isEmpty at SparkTest.worksheet.sc:120)
23/12/01 16:08:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)
23/12/01 16:08:04 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:04 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[78] at isEmpty at SparkTest.worksheet.sc:120), which has no missing parents
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 13.3 KiB, free 432.2 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 432.2 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.25.86.80:34297 (size: 6.4 KiB, free: 434.3 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[78] at isEmpty at SparkTest.worksheet.sc:120) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:04 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
23/12/01 16:08:04 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 22) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7714 bytes) 
23/12/01 16:08:04 INFO Executor: Running task 0.0 in stage 31.0 (TID 22)
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Getting 1 (300.0 B) non-empty blocks including 1 (300.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 4.760254 ms
23/12/01 16:08:04 INFO Executor: Finished task 0.0 in stage 31.0 (TID 22). 3262 bytes result sent to driver
23/12/01 16:08:04 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 22) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:04 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
23/12/01 16:08:04 INFO DAGScheduler: ResultStage 31 (isEmpty at SparkTest.worksheet.sc:120) finished in 0.015 s
23/12/01 16:08:04 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
23/12/01 16:08:04 INFO DAGScheduler: Job 22 finished: isEmpty at SparkTest.worksheet.sc:120, took 0.017356 s
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 4.147157 ms
23/12/01 16:08:04 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/01 16:08:04 INFO DAGScheduler: Got job 23 (collect at Ast.scala:253) with 1 output partitions
23/12/01 16:08:04 INFO DAGScheduler: Final stage: ResultStage 32 (collect at Ast.scala:253)
23/12/01 16:08:04 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:04 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:04 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[82] at collect at Ast.scala:253), which has no missing parents
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 17.5 KiB, free 432.2 MiB)
23/12/01 16:08:04 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 432.2 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 10.25.86.80:34297 in memory (size: 15.9 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.25.86.80:34297 (size: 7.4 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[82] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:04 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 10.25.86.80:34297 in memory (size: 6.9 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 23) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:08:04 INFO Executor: Running task 0.0 in stage 32.0 (TID 23)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 10.25.86.80:34297 in memory (size: 7.6 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 10.25.86.80:34297 in memory (size: 337.0 B, free: 434.4 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 10.25.86.80:34297 in memory (size: 251.0 B, free: 434.4 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 10.25.86.80:34297 in memory (size: 7.5 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 10.25.86.80:34297 in memory (size: 14.5 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 10.25.86.80:34297 in memory (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:08:04 INFO CodeGenerator: Code generated in 4.797122 ms
23/12/01 16:08:04 INFO Executor: Finished task 0.0 in stage 32.0 (TID 23). 1826 bytes result sent to driver
23/12/01 16:08:04 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 23) in 20 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:04 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
23/12/01 16:08:04 INFO DAGScheduler: ResultStage 32 (collect at Ast.scala:253) finished in 0.037 s
23/12/01 16:08:04 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
23/12/01 16:08:04 INFO DAGScheduler: Job 23 finished: collect at Ast.scala:253, took 0.039498 s
23/12/01 16:08:05 INFO DAGScheduler: Registering RDD 83 (collect at Ast.scala:253) as input to shuffle 6
23/12/01 16:08:05 INFO DAGScheduler: Got map stage job 24 (collect at Ast.scala:253) with 1 output partitions
23/12/01 16:08:05 INFO DAGScheduler: Final stage: ShuffleMapStage 33 (collect at Ast.scala:253)
23/12/01 16:08:05 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:05 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:05 INFO DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[83] at collect at Ast.scala:253), which has no missing parents
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 19.0 KiB, free 434.4 MiB)
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 434.3 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.25.86.80:34297 (size: 8.2 KiB, free: 434.4 MiB)
23/12/01 16:08:05 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[83] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:05 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
23/12/01 16:08:05 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 24) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/01 16:08:05 INFO Executor: Running task 0.0 in stage 33.0 (TID 24)
23/12/01 16:08:05 INFO Executor: Finished task 0.0 in stage 33.0 (TID 24). 1793 bytes result sent to driver
23/12/01 16:08:05 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 24) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:05 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
23/12/01 16:08:05 INFO DAGScheduler: ShuffleMapStage 33 (collect at Ast.scala:253) finished in 0.019 s
23/12/01 16:08:05 INFO DAGScheduler: looking for newly runnable stages
23/12/01 16:08:05 INFO DAGScheduler: running: HashSet()
23/12/01 16:08:05 INFO DAGScheduler: waiting: HashSet()
23/12/01 16:08:05 INFO DAGScheduler: failed: HashSet()
23/12/01 16:08:05 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/01 16:08:05 INFO CodeGenerator: Code generated in 6.031625 ms
23/12/01 16:08:05 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/01 16:08:05 INFO DAGScheduler: Got job 25 (collect at Ast.scala:253) with 1 output partitions
23/12/01 16:08:05 INFO DAGScheduler: Final stage: ResultStage 35 (collect at Ast.scala:253)
23/12/01 16:08:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)
23/12/01 16:08:05 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:05 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[86] at collect at Ast.scala:253), which has no missing parents
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 24.9 KiB, free 434.3 MiB)
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.3 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.25.86.80:34297 (size: 10.8 KiB, free: 434.4 MiB)
23/12/01 16:08:05 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[86] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:05 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
23/12/01 16:08:05 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 25) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/01 16:08:05 INFO Executor: Running task 0.0 in stage 35.0 (TID 25)
23/12/01 16:08:05 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/01 16:08:05 INFO CodeGenerator: Code generated in 5.46639 ms
23/12/01 16:08:05 INFO Executor: Finished task 0.0 in stage 35.0 (TID 25). 4494 bytes result sent to driver
23/12/01 16:08:05 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 25) in 14 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:05 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
23/12/01 16:08:05 INFO DAGScheduler: ResultStage 35 (collect at Ast.scala:253) finished in 0.018 s
23/12/01 16:08:05 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
23/12/01 16:08:05 INFO DAGScheduler: Job 25 finished: collect at Ast.scala:253, took 0.020155 s
23/12/01 16:08:05 INFO CodeGenerator: Code generated in 5.815795 ms
23/12/01 16:08:05 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/01 16:08:05 INFO DAGScheduler: Got job 26 (collect at Ast.scala:253) with 1 output partitions
23/12/01 16:08:05 INFO DAGScheduler: Final stage: ResultStage 36 (collect at Ast.scala:253)
23/12/01 16:08:05 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:05 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:05 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[90] at collect at Ast.scala:253), which has no missing parents
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 17.5 KiB, free 434.3 MiB)
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.3 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.25.86.80:34297 (size: 7.4 KiB, free: 434.4 MiB)
23/12/01 16:08:05 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[90] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:05 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
23/12/01 16:08:05 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 26) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:08:05 INFO Executor: Running task 0.0 in stage 36.0 (TID 26)
23/12/01 16:08:05 INFO Executor: Finished task 0.0 in stage 36.0 (TID 26). 1826 bytes result sent to driver
23/12/01 16:08:05 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 26) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:05 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
23/12/01 16:08:05 INFO DAGScheduler: ResultStage 36 (collect at Ast.scala:253) finished in 0.015 s
23/12/01 16:08:05 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
23/12/01 16:08:05 INFO DAGScheduler: Job 26 finished: collect at Ast.scala:253, took 0.016196 s
23/12/01 16:08:05 INFO DAGScheduler: Registering RDD 91 (collect at Ast.scala:253) as input to shuffle 7
23/12/01 16:08:05 INFO DAGScheduler: Got map stage job 27 (collect at Ast.scala:253) with 1 output partitions
23/12/01 16:08:05 INFO DAGScheduler: Final stage: ShuffleMapStage 37 (collect at Ast.scala:253)
23/12/01 16:08:05 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:05 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:05 INFO DAGScheduler: Submitting ShuffleMapStage 37 (MapPartitionsRDD[91] at collect at Ast.scala:253), which has no missing parents
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 19.0 KiB, free 434.3 MiB)
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 434.3 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.25.86.80:34297 (size: 8.2 KiB, free: 434.4 MiB)
23/12/01 16:08:05 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[91] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:05 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
23/12/01 16:08:05 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 27) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/01 16:08:05 INFO Executor: Running task 0.0 in stage 37.0 (TID 27)
23/12/01 16:08:05 INFO Executor: Finished task 0.0 in stage 37.0 (TID 27). 1793 bytes result sent to driver
23/12/01 16:08:05 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 27) in 23 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:05 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
23/12/01 16:08:05 INFO DAGScheduler: ShuffleMapStage 37 (collect at Ast.scala:253) finished in 0.027 s
23/12/01 16:08:05 INFO DAGScheduler: looking for newly runnable stages
23/12/01 16:08:05 INFO DAGScheduler: running: HashSet()
23/12/01 16:08:05 INFO DAGScheduler: waiting: HashSet()
23/12/01 16:08:05 INFO DAGScheduler: failed: HashSet()
23/12/01 16:08:05 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/01 16:08:05 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/01 16:08:05 INFO DAGScheduler: Got job 28 (collect at Ast.scala:253) with 1 output partitions
23/12/01 16:08:05 INFO DAGScheduler: Final stage: ResultStage 39 (collect at Ast.scala:253)
23/12/01 16:08:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)
23/12/01 16:08:05 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:05 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[94] at collect at Ast.scala:253), which has no missing parents
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 24.9 KiB, free 434.2 MiB)
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 434.2 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.25.86.80:34297 (size: 10.7 KiB, free: 434.3 MiB)
23/12/01 16:08:05 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[94] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:05 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
23/12/01 16:08:05 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 28) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/01 16:08:05 INFO Executor: Running task 0.0 in stage 39.0 (TID 28)
23/12/01 16:08:05 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/01 16:08:05 INFO Executor: Finished task 0.0 in stage 39.0 (TID 28). 4494 bytes result sent to driver
23/12/01 16:08:05 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 28) in 6 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:05 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
23/12/01 16:08:05 INFO DAGScheduler: ResultStage 39 (collect at Ast.scala:253) finished in 0.009 s
23/12/01 16:08:05 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
23/12/01 16:08:05 INFO DAGScheduler: Job 28 finished: collect at Ast.scala:253, took 0.011702 s
23/12/01 16:08:05 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/01 16:08:05 INFO DAGScheduler: Got job 29 (collect at Ast.scala:253) with 1 output partitions
23/12/01 16:08:05 INFO DAGScheduler: Final stage: ResultStage 40 (collect at Ast.scala:253)
23/12/01 16:08:05 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:05 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:05 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[98] at collect at Ast.scala:253), which has no missing parents
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 17.5 KiB, free 434.2 MiB)
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.2 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.25.86.80:34297 (size: 7.4 KiB, free: 434.3 MiB)
23/12/01 16:08:05 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[98] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:05 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
23/12/01 16:08:05 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 29) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:08:05 INFO Executor: Running task 0.0 in stage 40.0 (TID 29)
23/12/01 16:08:05 INFO Executor: Finished task 0.0 in stage 40.0 (TID 29). 1826 bytes result sent to driver
23/12/01 16:08:05 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 29) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:05 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
23/12/01 16:08:05 INFO DAGScheduler: ResultStage 40 (collect at Ast.scala:253) finished in 0.012 s
23/12/01 16:08:05 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
23/12/01 16:08:05 INFO DAGScheduler: Job 29 finished: collect at Ast.scala:253, took 0.013996 s
23/12/01 16:08:05 INFO DAGScheduler: Registering RDD 99 (collect at Ast.scala:253) as input to shuffle 8
23/12/01 16:08:05 INFO DAGScheduler: Got map stage job 30 (collect at Ast.scala:253) with 1 output partitions
23/12/01 16:08:05 INFO DAGScheduler: Final stage: ShuffleMapStage 41 (collect at Ast.scala:253)
23/12/01 16:08:05 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:05 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:05 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[99] at collect at Ast.scala:253), which has no missing parents
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 19.0 KiB, free 434.2 MiB)
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 434.2 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.25.86.80:34297 (size: 8.2 KiB, free: 434.3 MiB)
23/12/01 16:08:05 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[99] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:05 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0
23/12/01 16:08:05 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 30) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/01 16:08:05 INFO Executor: Running task 0.0 in stage 41.0 (TID 30)
23/12/01 16:08:05 INFO Executor: Finished task 0.0 in stage 41.0 (TID 30). 1793 bytes result sent to driver
23/12/01 16:08:05 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 30) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:05 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
23/12/01 16:08:05 INFO DAGScheduler: ShuffleMapStage 41 (collect at Ast.scala:253) finished in 0.015 s
23/12/01 16:08:05 INFO DAGScheduler: looking for newly runnable stages
23/12/01 16:08:05 INFO DAGScheduler: running: HashSet()
23/12/01 16:08:05 INFO DAGScheduler: waiting: HashSet()
23/12/01 16:08:05 INFO DAGScheduler: failed: HashSet()
23/12/01 16:08:05 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/01 16:08:05 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/01 16:08:05 INFO DAGScheduler: Got job 31 (collect at Ast.scala:253) with 1 output partitions
23/12/01 16:08:05 INFO DAGScheduler: Final stage: ResultStage 43 (collect at Ast.scala:253)
23/12/01 16:08:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 42)
23/12/01 16:08:05 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:05 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[102] at collect at Ast.scala:253), which has no missing parents
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 24.9 KiB, free 434.2 MiB)
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.1 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.25.86.80:34297 (size: 10.8 KiB, free: 434.3 MiB)
23/12/01 16:08:05 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[102] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:05 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
23/12/01 16:08:05 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 31) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/01 16:08:05 INFO Executor: Running task 0.0 in stage 43.0 (TID 31)
23/12/01 16:08:05 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/01 16:08:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/01 16:08:05 INFO Executor: Finished task 0.0 in stage 43.0 (TID 31). 4494 bytes result sent to driver
23/12/01 16:08:05 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 31) in 8 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:05 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
23/12/01 16:08:05 INFO DAGScheduler: ResultStage 43 (collect at Ast.scala:253) finished in 0.011 s
23/12/01 16:08:05 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished
23/12/01 16:08:05 INFO DAGScheduler: Job 31 finished: collect at Ast.scala:253, took 0.013565 s
23/12/01 16:08:05 INFO SparkContext: Starting job: show at SparkTest.worksheet.sc:150
23/12/01 16:08:05 INFO DAGScheduler: Got job 32 (show at SparkTest.worksheet.sc:150) with 1 output partitions
23/12/01 16:08:05 INFO DAGScheduler: Final stage: ResultStage 44 (show at SparkTest.worksheet.sc:150)
23/12/01 16:08:05 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:05 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:05 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[104] at show at SparkTest.worksheet.sc:150), which has no missing parents
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 15.7 KiB, free 434.1 MiB)
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.1 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.25.86.80:34297 (size: 6.5 KiB, free: 434.3 MiB)
23/12/01 16:08:05 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[104] at show at SparkTest.worksheet.sc:150) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:05 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks resource profile 0
23/12/01 16:08:05 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 32) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:08:05 INFO Executor: Running task 0.0 in stage 44.0 (TID 32)
23/12/01 16:08:05 INFO Executor: Finished task 0.0 in stage 44.0 (TID 32). 1683 bytes result sent to driver
23/12/01 16:08:05 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 10.25.86.80:34297 in memory (size: 7.4 KiB, free: 434.3 MiB)
23/12/01 16:08:05 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 32) in 18 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:05 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
23/12/01 16:08:05 INFO DAGScheduler: ResultStage 44 (show at SparkTest.worksheet.sc:150) finished in 0.021 s
23/12/01 16:08:05 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished
23/12/01 16:08:05 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 10.25.86.80:34297 in memory (size: 7.4 KiB, free: 434.3 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 10.25.86.80:34297 in memory (size: 8.2 KiB, free: 434.3 MiB)
23/12/01 16:08:05 INFO DAGScheduler: Job 32 finished: show at SparkTest.worksheet.sc:150, took 0.024068 s
23/12/01 16:08:05 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 10.25.86.80:34297 in memory (size: 8.2 KiB, free: 434.3 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 10.25.86.80:34297 in memory (size: 10.8 KiB, free: 434.4 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 10.25.86.80:34297 in memory (size: 8.2 KiB, free: 434.4 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 10.25.86.80:34297 in memory (size: 10.7 KiB, free: 434.4 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 10.25.86.80:34297 in memory (size: 7.4 KiB, free: 434.4 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 10.25.86.80:34297 in memory (size: 10.8 KiB, free: 434.4 MiB)
23/12/01 16:08:05 INFO CodeGenerator: Code generated in 7.442277 ms
23/12/01 16:08:05 INFO SparkContext: Starting job: collect at SparkTest.worksheet.sc:159
23/12/01 16:08:05 INFO DAGScheduler: Got job 33 (collect at SparkTest.worksheet.sc:159) with 1 output partitions
23/12/01 16:08:05 INFO DAGScheduler: Final stage: ResultStage 45 (collect at SparkTest.worksheet.sc:159)
23/12/01 16:08:05 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:05 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:05 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[112] at collect at SparkTest.worksheet.sc:159), which has no missing parents
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 15.4 KiB, free 434.4 MiB)
23/12/01 16:08:05 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.4 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.25.86.80:34297 (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:08:05 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[112] at collect at SparkTest.worksheet.sc:159) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:05 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0
23/12/01 16:08:05 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 33) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:08:05 INFO Executor: Running task 0.0 in stage 45.0 (TID 33)
23/12/01 16:08:05 INFO CodeGenerator: Code generated in 6.143042 ms
23/12/01 16:08:05 INFO Executor: Finished task 0.0 in stage 45.0 (TID 33). 1650 bytes result sent to driver
23/12/01 16:08:05 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 33) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:05 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
23/12/01 16:08:05 INFO DAGScheduler: ResultStage 45 (collect at SparkTest.worksheet.sc:159) finished in 0.016 s
23/12/01 16:08:05 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
23/12/01 16:08:05 INFO DAGScheduler: Job 33 finished: collect at SparkTest.worksheet.sc:159, took 0.018172 s
23/12/01 16:08:05 INFO CodeGenerator: Code generated in 5.201567 ms
23/12/01 16:08:05 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 10.25.86.80:34297 in memory (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:08:05 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 10.25.86.80:34297 in memory (size: 6.5 KiB, free: 434.4 MiB)
23/12/01 16:08:06 INFO CodeGenerator: Code generated in 14.492557 ms
23/12/01 16:08:06 INFO SparkContext: Starting job: show at ApiTest.worksheet.sc:166
23/12/01 16:08:06 INFO DAGScheduler: Got job 4 (show at ApiTest.worksheet.sc:166) with 1 output partitions
23/12/01 16:08:06 INFO DAGScheduler: Final stage: ResultStage 4 (show at ApiTest.worksheet.sc:166)
23/12/01 16:08:06 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:06 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:06 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at show at ApiTest.worksheet.sc:166), which has no missing parents
23/12/01 16:08:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 15.5 KiB, free 434.4 MiB)
23/12/01 16:08:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.4 MiB)
23/12/01 16:08:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.25.86.80:42381 (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:08:06 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at show at ApiTest.worksheet.sc:166) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:06 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/12/01 16:08:06 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/01 16:08:06 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
23/12/01 16:08:06 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1634 bytes result sent to driver
23/12/01 16:08:06 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:06 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/12/01 16:08:06 INFO DAGScheduler: ResultStage 4 (show at ApiTest.worksheet.sc:166) finished in 0.044 s
23/12/01 16:08:06 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/12/01 16:08:06 INFO DAGScheduler: Job 4 finished: show at ApiTest.worksheet.sc:166, took 0.045205 s
23/12/01 16:08:06 INFO CodeGenerator: Code generated in 6.628521 ms
23/12/01 16:08:06 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.25.86.80:42381 in memory (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:08:27 INFO CodeGenerator: Code generated in 6.548793 ms
23/12/01 16:08:27 INFO SparkContext: Starting job: show at ApiTest.worksheet.sc:166
23/12/01 16:08:27 INFO DAGScheduler: Got job 5 (show at ApiTest.worksheet.sc:166) with 1 output partitions
23/12/01 16:08:27 INFO DAGScheduler: Final stage: ResultStage 5 (show at ApiTest.worksheet.sc:166)
23/12/01 16:08:27 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:08:27 INFO DAGScheduler: Missing parents: List()
23/12/01 16:08:27 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[23] at show at ApiTest.worksheet.sc:166), which has no missing parents
23/12/01 16:08:27 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 434.4 MiB)
23/12/01 16:08:27 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.4 MiB)
23/12/01 16:08:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.25.86.80:42381 (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:08:27 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
23/12/01 16:08:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at show at ApiTest.worksheet.sc:166) (first 15 tasks are for partitions Vector(0))
23/12/01 16:08:27 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
23/12/01 16:08:27 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/01 16:08:27 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
23/12/01 16:08:27 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1634 bytes result sent to driver
23/12/01 16:08:27 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 6 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:08:27 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
23/12/01 16:08:27 INFO DAGScheduler: ResultStage 5 (show at ApiTest.worksheet.sc:166) finished in 0.019 s
23/12/01 16:08:27 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:08:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
23/12/01 16:08:27 INFO DAGScheduler: Job 5 finished: show at ApiTest.worksheet.sc:166, took 0.020319 s
23/12/01 16:08:27 INFO CodeGenerator: Code generated in 6.346334 ms
23/12/01 16:08:27 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.25.86.80:42381 in memory (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:09:18 INFO CodeGenerator: Code generated in 6.466909 ms
23/12/01 16:09:18 INFO SparkContext: Starting job: show at ApiTest.worksheet.sc:170
23/12/01 16:09:18 INFO DAGScheduler: Got job 6 (show at ApiTest.worksheet.sc:170) with 1 output partitions
23/12/01 16:09:18 INFO DAGScheduler: Final stage: ResultStage 6 (show at ApiTest.worksheet.sc:170)
23/12/01 16:09:18 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:09:18 INFO DAGScheduler: Missing parents: List()
23/12/01 16:09:18 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at show at ApiTest.worksheet.sc:170), which has no missing parents
23/12/01 16:09:18 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 15.5 KiB, free 434.4 MiB)
23/12/01 16:09:18 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.4 MiB)
23/12/01 16:09:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.25.86.80:42381 (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:09:18 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
23/12/01 16:09:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at show at ApiTest.worksheet.sc:170) (first 15 tasks are for partitions Vector(0))
23/12/01 16:09:18 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
23/12/01 16:09:18 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:09:18 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
23/12/01 16:09:18 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1634 bytes result sent to driver
23/12/01 16:09:18 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 6 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:09:18 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
23/12/01 16:09:18 INFO DAGScheduler: ResultStage 6 (show at ApiTest.worksheet.sc:170) finished in 0.015 s
23/12/01 16:09:18 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:09:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
23/12/01 16:09:18 INFO DAGScheduler: Job 6 finished: show at ApiTest.worksheet.sc:170, took 0.016153 s
23/12/01 16:09:18 INFO CodeGenerator: Code generated in 6.224049 ms
23/12/01 16:09:18 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.25.86.80:42381 in memory (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:09:25 INFO CodeGenerator: Code generated in 6.951902 ms
23/12/01 16:09:25 INFO SparkContext: Starting job: show at ApiTest.worksheet.sc:170
23/12/01 16:09:25 INFO DAGScheduler: Got job 7 (show at ApiTest.worksheet.sc:170) with 1 output partitions
23/12/01 16:09:25 INFO DAGScheduler: Final stage: ResultStage 7 (show at ApiTest.worksheet.sc:170)
23/12/01 16:09:25 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:09:25 INFO DAGScheduler: Missing parents: List()
23/12/01 16:09:25 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at show at ApiTest.worksheet.sc:170), which has no missing parents
23/12/01 16:09:25 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 15.5 KiB, free 434.4 MiB)
23/12/01 16:09:25 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.4 MiB)
23/12/01 16:09:25 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.25.86.80:42381 (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:09:25 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
23/12/01 16:09:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at show at ApiTest.worksheet.sc:170) (first 15 tasks are for partitions Vector(0))
23/12/01 16:09:25 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
23/12/01 16:09:25 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:09:25 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
23/12/01 16:09:25 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1634 bytes result sent to driver
23/12/01 16:09:25 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:09:25 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
23/12/01 16:09:25 INFO DAGScheduler: ResultStage 7 (show at ApiTest.worksheet.sc:170) finished in 0.019 s
23/12/01 16:09:25 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:09:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
23/12/01 16:09:25 INFO DAGScheduler: Job 7 finished: show at ApiTest.worksheet.sc:170, took 0.021162 s
23/12/01 16:09:25 INFO CodeGenerator: Code generated in 5.702782 ms
23/12/01 16:09:25 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.25.86.80:42381 in memory (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:09:47 INFO CodeGenerator: Code generated in 5.729062 ms
23/12/01 16:09:47 INFO SparkContext: Starting job: show at ApiTest.worksheet.sc:170
23/12/01 16:09:47 INFO DAGScheduler: Got job 8 (show at ApiTest.worksheet.sc:170) with 1 output partitions
23/12/01 16:09:47 INFO DAGScheduler: Final stage: ResultStage 8 (show at ApiTest.worksheet.sc:170)
23/12/01 16:09:47 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:09:47 INFO DAGScheduler: Missing parents: List()
23/12/01 16:09:47 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[35] at show at ApiTest.worksheet.sc:170), which has no missing parents
23/12/01 16:09:47 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 15.5 KiB, free 434.4 MiB)
23/12/01 16:09:47 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.4 MiB)
23/12/01 16:09:47 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.25.86.80:42381 (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:09:47 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
23/12/01 16:09:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at show at ApiTest.worksheet.sc:170) (first 15 tasks are for partitions Vector(0))
23/12/01 16:09:47 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
23/12/01 16:09:47 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:09:47 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
23/12/01 16:09:47 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1634 bytes result sent to driver
23/12/01 16:09:47 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:09:47 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
23/12/01 16:09:47 INFO DAGScheduler: ResultStage 8 (show at ApiTest.worksheet.sc:170) finished in 0.031 s
23/12/01 16:09:47 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:09:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
23/12/01 16:09:47 INFO DAGScheduler: Job 8 finished: show at ApiTest.worksheet.sc:170, took 0.032614 s
23/12/01 16:09:47 INFO CodeGenerator: Code generated in 6.048962 ms
23/12/01 16:09:47 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.25.86.80:42381 in memory (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:09:56 INFO CodeGenerator: Code generated in 5.580613 ms
23/12/01 16:09:56 INFO SparkContext: Starting job: show at ApiTest.worksheet.sc:170
23/12/01 16:09:56 INFO DAGScheduler: Got job 9 (show at ApiTest.worksheet.sc:170) with 1 output partitions
23/12/01 16:09:56 INFO DAGScheduler: Final stage: ResultStage 9 (show at ApiTest.worksheet.sc:170)
23/12/01 16:09:56 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:09:56 INFO DAGScheduler: Missing parents: List()
23/12/01 16:09:56 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[39] at show at ApiTest.worksheet.sc:170), which has no missing parents
23/12/01 16:09:56 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.5 KiB, free 434.4 MiB)
23/12/01 16:09:56 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.4 MiB)
23/12/01 16:09:56 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.25.86.80:42381 (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:09:56 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
23/12/01 16:09:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[39] at show at ApiTest.worksheet.sc:170) (first 15 tasks are for partitions Vector(0))
23/12/01 16:09:56 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
23/12/01 16:09:56 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:09:56 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
23/12/01 16:09:56 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 1634 bytes result sent to driver
23/12/01 16:09:56 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 6 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:09:56 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
23/12/01 16:09:56 INFO DAGScheduler: ResultStage 9 (show at ApiTest.worksheet.sc:170) finished in 0.013 s
23/12/01 16:09:56 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:09:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
23/12/01 16:09:56 INFO DAGScheduler: Job 9 finished: show at ApiTest.worksheet.sc:170, took 0.014806 s
23/12/01 16:09:56 INFO CodeGenerator: Code generated in 5.397501 ms
23/12/01 16:09:56 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.25.86.80:42381 in memory (size: 6.4 KiB, free: 434.4 MiB)
23/12/01 16:12:30 INFO CodeGenerator: Code generated in 6.060437 ms
23/12/01 16:12:30 INFO SparkContext: Starting job: show at ApiTest.worksheet.sc:170
23/12/01 16:12:30 INFO DAGScheduler: Got job 10 (show at ApiTest.worksheet.sc:170) with 1 output partitions
23/12/01 16:12:30 INFO DAGScheduler: Final stage: ResultStage 10 (show at ApiTest.worksheet.sc:170)
23/12/01 16:12:30 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:12:30 INFO DAGScheduler: Missing parents: List()
23/12/01 16:12:30 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[45] at show at ApiTest.worksheet.sc:170), which has no missing parents
23/12/01 16:12:30 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 15.5 KiB, free 434.4 MiB)
23/12/01 16:12:30 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.4 MiB)
23/12/01 16:12:30 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.25.86.80:42381 (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 16:12:30 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580
23/12/01 16:12:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[45] at show at ApiTest.worksheet.sc:170) (first 15 tasks are for partitions Vector(0))
23/12/01 16:12:30 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
23/12/01 16:12:30 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:12:30 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
23/12/01 16:12:30 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 1634 bytes result sent to driver
23/12/01 16:12:30 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 7 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:12:30 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
23/12/01 16:12:30 INFO DAGScheduler: ResultStage 10 (show at ApiTest.worksheet.sc:170) finished in 0.019 s
23/12/01 16:12:30 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:12:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
23/12/01 16:12:30 INFO DAGScheduler: Job 10 finished: show at ApiTest.worksheet.sc:170, took 0.020348 s
23/12/01 16:12:30 INFO CodeGenerator: Code generated in 5.422827 ms
23/12/01 16:12:30 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 10.25.86.80:42381 in memory (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 16:12:42 INFO CodeGenerator: Code generated in 5.537141 ms
23/12/01 16:12:42 INFO SparkContext: Starting job: show at ApiTest.worksheet.sc:170
23/12/01 16:12:42 INFO DAGScheduler: Got job 11 (show at ApiTest.worksheet.sc:170) with 1 output partitions
23/12/01 16:12:42 INFO DAGScheduler: Final stage: ResultStage 11 (show at ApiTest.worksheet.sc:170)
23/12/01 16:12:42 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:12:42 INFO DAGScheduler: Missing parents: List()
23/12/01 16:12:42 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[51] at show at ApiTest.worksheet.sc:170), which has no missing parents
23/12/01 16:12:42 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 15.5 KiB, free 434.4 MiB)
23/12/01 16:12:42 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.4 MiB)
23/12/01 16:12:42 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.25.86.80:42381 (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 16:12:42 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580
23/12/01 16:12:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[51] at show at ApiTest.worksheet.sc:170) (first 15 tasks are for partitions Vector(0))
23/12/01 16:12:42 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
23/12/01 16:12:42 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:12:42 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
23/12/01 16:12:43 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 1677 bytes result sent to driver
23/12/01 16:12:43 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 17 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:12:43 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
23/12/01 16:12:43 INFO DAGScheduler: ResultStage 11 (show at ApiTest.worksheet.sc:170) finished in 0.024 s
23/12/01 16:12:43 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:12:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
23/12/01 16:12:43 INFO DAGScheduler: Job 11 finished: show at ApiTest.worksheet.sc:170, took 0.026085 s
23/12/01 16:12:43 INFO CodeGenerator: Code generated in 5.34749 ms
23/12/01 16:12:43 INFO CodeGenerator: Code generated in 4.077823 ms
23/12/01 16:12:43 INFO SparkContext: Starting job: collect at SparkJob.scala:13
23/12/01 16:12:43 INFO DAGScheduler: Got job 12 (collect at SparkJob.scala:13) with 1 output partitions
23/12/01 16:12:43 INFO DAGScheduler: Final stage: ResultStage 12 (collect at SparkJob.scala:13)
23/12/01 16:12:43 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:12:43 INFO DAGScheduler: Missing parents: List()
23/12/01 16:12:43 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[57] at collect at SparkJob.scala:13), which has no missing parents
23/12/01 16:12:43 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 14.8 KiB, free 434.4 MiB)
23/12/01 16:12:43 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.4 MiB)
23/12/01 16:12:43 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.25.86.80:42381 (size: 6.1 KiB, free: 434.4 MiB)
23/12/01 16:12:43 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1580
23/12/01 16:12:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[57] at collect at SparkJob.scala:13) (first 15 tasks are for partitions Vector(0))
23/12/01 16:12:43 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
23/12/01 16:12:43 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:12:43 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
23/12/01 16:12:43 INFO CodeGenerator: Code generated in 5.477899 ms
23/12/01 16:12:43 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 1657 bytes result sent to driver
23/12/01 16:12:43 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 14 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:12:43 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
23/12/01 16:12:43 INFO DAGScheduler: ResultStage 12 (collect at SparkJob.scala:13) finished in 0.030 s
23/12/01 16:12:43 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:12:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
23/12/01 16:12:43 INFO DAGScheduler: Job 12 finished: collect at SparkJob.scala:13, took 0.032039 s
23/12/01 16:12:43 INFO CodeGenerator: Code generated in 5.389266 ms
23/12/01 16:12:43 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 10.25.86.80:42381 in memory (size: 6.1 KiB, free: 434.4 MiB)
23/12/01 16:12:43 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 10.25.86.80:42381 in memory (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 16:15:07 INFO CodeGenerator: Code generated in 5.65055 ms
23/12/01 16:15:07 INFO SparkContext: Starting job: show at ApiTest.worksheet.sc:170
23/12/01 16:15:07 INFO DAGScheduler: Got job 13 (show at ApiTest.worksheet.sc:170) with 1 output partitions
23/12/01 16:15:07 INFO DAGScheduler: Final stage: ResultStage 13 (show at ApiTest.worksheet.sc:170)
23/12/01 16:15:07 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:15:07 INFO DAGScheduler: Missing parents: List()
23/12/01 16:15:07 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[61] at show at ApiTest.worksheet.sc:170), which has no missing parents
23/12/01 16:15:07 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 15.5 KiB, free 434.4 MiB)
23/12/01 16:15:07 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.4 MiB)
23/12/01 16:15:07 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.25.86.80:42381 (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 16:15:07 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580
23/12/01 16:15:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[61] at show at ApiTest.worksheet.sc:170) (first 15 tasks are for partitions Vector(0))
23/12/01 16:15:07 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
23/12/01 16:15:07 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:15:07 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
23/12/01 16:15:07 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 1634 bytes result sent to driver
23/12/01 16:15:07 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 6 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:15:07 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
23/12/01 16:15:07 INFO DAGScheduler: ResultStage 13 (show at ApiTest.worksheet.sc:170) finished in 0.013 s
23/12/01 16:15:07 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:15:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
23/12/01 16:15:07 INFO DAGScheduler: Job 13 finished: show at ApiTest.worksheet.sc:170, took 0.015201 s
23/12/01 16:15:07 INFO CodeGenerator: Code generated in 5.882555 ms
23/12/01 16:15:07 INFO CodeGenerator: Code generated in 4.215756 ms
23/12/01 16:15:07 INFO SparkContext: Starting job: collect at SparkJob.scala:13
23/12/01 16:15:07 INFO DAGScheduler: Got job 14 (collect at SparkJob.scala:13) with 1 output partitions
23/12/01 16:15:07 INFO DAGScheduler: Final stage: ResultStage 14 (collect at SparkJob.scala:13)
23/12/01 16:15:07 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:15:07 INFO DAGScheduler: Missing parents: List()
23/12/01 16:15:07 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[67] at collect at SparkJob.scala:13), which has no missing parents
23/12/01 16:15:07 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 10.25.86.80:42381 in memory (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 16:15:07 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 14.8 KiB, free 434.4 MiB)
23/12/01 16:15:07 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 434.4 MiB)
23/12/01 16:15:07 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.25.86.80:42381 (size: 6.2 KiB, free: 434.4 MiB)
23/12/01 16:15:07 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1580
23/12/01 16:15:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[67] at collect at SparkJob.scala:13) (first 15 tasks are for partitions Vector(0))
23/12/01 16:15:07 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
23/12/01 16:15:07 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:15:07 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
23/12/01 16:15:07 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 1657 bytes result sent to driver
23/12/01 16:15:07 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 6 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:15:07 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
23/12/01 16:15:07 INFO DAGScheduler: ResultStage 14 (collect at SparkJob.scala:13) finished in 0.009 s
23/12/01 16:15:07 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:15:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
23/12/01 16:15:07 INFO DAGScheduler: Job 14 finished: collect at SparkJob.scala:13, took 0.019402 s
23/12/01 16:15:07 INFO CodeGenerator: Code generated in 5.433901 ms
23/12/01 16:15:07 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 10.25.86.80:42381 in memory (size: 6.2 KiB, free: 434.4 MiB)
23/12/01 16:17:43 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:17:43 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:17:43 INFO xnio: XNIO version 3.8.7.Final
23/12/01 16:17:43 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/01 16:17:43 INFO threads: JBoss Threads version 3.1.0.Final
23/12/01 16:17:43 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:17:43 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:17:43 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:17:43 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:17:43 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:17:43 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:17:43 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:17:43 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:17:43 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:17:43 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:20:21 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/01 16:20:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/01 16:20:21 INFO SparkContext: Running Spark version 3.5.0
23/12/01 16:20:21 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/01 16:20:21 INFO SparkContext: Java version 11.0.20
23/12/01 16:20:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/01 16:20:22 INFO ResourceUtils: ==============================================================
23/12/01 16:20:22 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/01 16:20:22 INFO ResourceUtils: ==============================================================
23/12/01 16:20:22 INFO SparkContext: Submitted application: sparkApp
23/12/01 16:20:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/01 16:20:22 INFO ResourceProfile: Limiting resource is cpu
23/12/01 16:20:22 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/01 16:20:22 INFO SecurityManager: Changing view acls to: bsoleille
23/12/01 16:20:22 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/01 16:20:22 INFO SecurityManager: Changing view acls groups to: 
23/12/01 16:20:22 INFO SecurityManager: Changing modify acls groups to: 
23/12/01 16:20:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/01 16:20:22 INFO Utils: Successfully started service 'sparkDriver' on port 39933.
23/12/01 16:20:22 INFO SparkEnv: Registering MapOutputTracker
23/12/01 16:20:22 INFO SparkEnv: Registering BlockManagerMaster
23/12/01 16:20:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/01 16:20:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/01 16:20:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/01 16:20:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-be9188a4-7bff-4d06-a0b4-e354a89f54ce
23/12/01 16:20:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/12/01 16:20:22 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/01 16:20:22 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/01 16:20:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/01 16:20:22 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/01 16:20:22 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/01 16:20:22 INFO Utils: Successfully started service 'SparkUI' on port 4043.
23/12/01 16:20:22 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/01 16:20:22 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/01 16:20:22 INFO Executor: Java version 11.0.20
23/12/01 16:20:22 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/01 16:20:22 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@186989dc for default.
23/12/01 16:20:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41709.
23/12/01 16:20:22 INFO NettyBlockTransferService: Server created on 10.25.86.80:41709
23/12/01 16:20:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/01 16:20:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 41709, None)
23/12/01 16:20:22 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:41709 with 434.4 MiB RAM, BlockManagerId(driver, 10.25.86.80, 41709, None)
23/12/01 16:20:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 41709, None)
23/12/01 16:20:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 41709, None)
23/12/01 16:20:22 WARN SharedState: URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
23/12/01 16:20:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/01 16:20:22 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/01 16:20:24 INFO CodeGenerator: Code generated in 129.907236 ms
23/12/01 16:20:24 INFO SparkContext: Starting job: show at ApiTest.worksheet.sc:170
23/12/01 16:20:24 INFO DAGScheduler: Got job 0 (show at ApiTest.worksheet.sc:170) with 1 output partitions
23/12/01 16:20:24 INFO DAGScheduler: Final stage: ResultStage 0 (show at ApiTest.worksheet.sc:170)
23/12/01 16:20:24 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:20:24 INFO DAGScheduler: Missing parents: List()
23/12/01 16:20:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at show at ApiTest.worksheet.sc:170), which has no missing parents
23/12/01 16:20:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 15.5 KiB, free 434.4 MiB)
23/12/01 16:20:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.4 MiB)
23/12/01 16:20:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:41709 (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 16:20:24 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/01 16:20:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at show at ApiTest.worksheet.sc:170) (first 15 tasks are for partitions Vector(0))
23/12/01 16:20:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/01 16:20:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:20:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/01 16:20:24 INFO CodeGenerator: Code generated in 13.511278 ms
23/12/01 16:20:24 INFO CodeGenerator: Code generated in 31.665589 ms
23/12/01 16:20:24 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1677 bytes result sent to driver
23/12/01 16:20:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 196 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:20:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/01 16:20:24 INFO DAGScheduler: ResultStage 0 (show at ApiTest.worksheet.sc:170) finished in 0.341 s
23/12/01 16:20:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:20:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/01 16:20:24 INFO DAGScheduler: Job 0 finished: show at ApiTest.worksheet.sc:170, took 0.385060 s
23/12/01 16:20:24 INFO CodeGenerator: Code generated in 7.978941 ms
23/12/01 16:20:24 INFO CodeGenerator: Code generated in 7.084814 ms
23/12/01 16:20:24 INFO SparkContext: Starting job: collect at SparkJob.scala:13
23/12/01 16:20:24 INFO DAGScheduler: Got job 1 (collect at SparkJob.scala:13) with 1 output partitions
23/12/01 16:20:24 INFO DAGScheduler: Final stage: ResultStage 1 (collect at SparkJob.scala:13)
23/12/01 16:20:24 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:20:24 INFO DAGScheduler: Missing parents: List()
23/12/01 16:20:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at collect at SparkJob.scala:13), which has no missing parents
23/12/01 16:20:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.8 KiB, free 434.4 MiB)
23/12/01 16:20:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.4 MiB)
23/12/01 16:20:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.25.86.80:41709 (size: 6.1 KiB, free: 434.4 MiB)
23/12/01 16:20:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/12/01 16:20:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at collect at SparkJob.scala:13) (first 15 tasks are for partitions Vector(0))
23/12/01 16:20:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/12/01 16:20:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/01 16:20:24 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/12/01 16:20:24 INFO CodeGenerator: Code generated in 6.463147 ms
23/12/01 16:20:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.25.86.80:41709 in memory (size: 6.3 KiB, free: 434.4 MiB)
23/12/01 16:20:24 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1700 bytes result sent to driver
23/12/01 16:20:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 42 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:20:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/01 16:20:24 INFO DAGScheduler: ResultStage 1 (collect at SparkJob.scala:13) finished in 0.046 s
23/12/01 16:20:24 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:20:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/12/01 16:20:24 INFO DAGScheduler: Job 1 finished: collect at SparkJob.scala:13, took 0.048438 s
23/12/01 16:20:24 INFO CodeGenerator: Code generated in 10.889675 ms
23/12/01 16:21:22 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.25.86.80:41709 in memory (size: 6.1 KiB, free: 434.4 MiB)
23/12/01 16:32:49 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:32:49 INFO xnio: XNIO version 3.8.7.Final
23/12/01 16:32:49 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/01 16:32:49 INFO threads: JBoss Threads version 3.1.0.Final
23/12/01 16:32:49 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:32:49 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:32:59 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:33:26 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/01 16:33:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/01 16:33:55 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:34:14 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:34:14 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:34:14 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:34:14 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:34:14 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:34:14 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:34:34 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:38:35 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:38:35 INFO xnio: XNIO version 3.8.7.Final
23/12/01 16:38:35 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/01 16:38:35 INFO threads: JBoss Threads version 3.1.0.Final
23/12/01 16:38:55 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:38:55 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:39:05 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:39:08 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/01 16:39:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/01 16:39:35 INFO SparkContext: Running Spark version 3.5.0
23/12/01 16:39:35 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/01 16:39:35 INFO SparkContext: Java version 11.0.20
23/12/01 16:39:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/01 16:39:35 INFO ResourceUtils: ==============================================================
23/12/01 16:39:35 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/01 16:39:35 INFO ResourceUtils: ==============================================================
23/12/01 16:39:35 INFO SparkContext: Submitted application: sparkApp
23/12/01 16:39:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/01 16:39:35 INFO ResourceProfile: Limiting resource is cpu
23/12/01 16:39:35 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/01 16:39:35 INFO SecurityManager: Changing view acls to: bsoleille
23/12/01 16:39:35 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/01 16:39:35 INFO SecurityManager: Changing view acls groups to: 
23/12/01 16:39:35 INFO SecurityManager: Changing modify acls groups to: 
23/12/01 16:39:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/01 16:39:35 INFO Utils: Successfully started service 'sparkDriver' on port 34317.
23/12/01 16:39:35 INFO SparkEnv: Registering MapOutputTracker
23/12/01 16:39:35 INFO SparkEnv: Registering BlockManagerMaster
23/12/01 16:39:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/01 16:39:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/01 16:39:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/01 16:39:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-327c3112-38c8-4e4f-976e-da4f58bafbcb
23/12/01 16:39:35 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/01 16:39:35 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/01 16:39:35 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/01 16:39:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/01 16:39:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/01 16:39:35 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/01 16:39:35 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/01 16:39:35 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/01 16:39:35 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/01 16:39:35 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/01 16:39:35 INFO Executor: Java version 11.0.20
23/12/01 16:39:35 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/01 16:39:35 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@10c6d899 for default.
23/12/01 16:39:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45783.
23/12/01 16:39:35 INFO NettyBlockTransferService: Server created on 10.25.86.80:45783
23/12/01 16:39:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/01 16:39:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 45783, None)
23/12/01 16:39:35 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:45783 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 45783, None)
23/12/01 16:39:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 45783, None)
23/12/01 16:39:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 45783, None)
23/12/01 16:39:36 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/01 16:39:36 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/01 16:39:51 INFO CodeGenerator: Code generated in 135.796806 ms
23/12/01 16:39:52 INFO SparkContext: Starting job: collect at SparkJob.scala:13
23/12/01 16:39:52 INFO DAGScheduler: Got job 0 (collect at SparkJob.scala:13) with 1 output partitions
23/12/01 16:39:52 INFO DAGScheduler: Final stage: ResultStage 0 (collect at SparkJob.scala:13)
23/12/01 16:39:52 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:39:52 INFO DAGScheduler: Missing parents: List()
23/12/01 16:39:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13), which has no missing parents
23/12/01 16:39:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 14.8 KiB, free 9.1 GiB)
23/12/01 16:39:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 9.1 GiB)
23/12/01 16:39:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:45783 (size: 6.1 KiB, free: 9.1 GiB)
23/12/01 16:39:52 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/01 16:39:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13) (first 15 tasks are for partitions Vector(0))
23/12/01 16:39:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/01 16:39:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:39:52 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/01 16:39:52 INFO CodeGenerator: Code generated in 7.747346 ms
23/12/01 16:39:52 INFO CodeGenerator: Code generated in 32.980556 ms
23/12/01 16:39:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1700 bytes result sent to driver
23/12/01 16:39:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 666 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:39:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/01 16:39:52 INFO DAGScheduler: ResultStage 0 (collect at SparkJob.scala:13) finished in 0.768 s
23/12/01 16:39:52 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:39:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/01 16:39:52 INFO DAGScheduler: Job 0 finished: collect at SparkJob.scala:13, took 0.795147 s
23/12/01 16:39:52 INFO CodeGenerator: Code generated in 7.55883 ms
23/12/01 16:39:59 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:40:07 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:40:07 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:40:07 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:40:07 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:40:07 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:40:07 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:40:11 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:40:11 INFO SparkContext: Invoking stop() from shutdown hook
23/12/01 16:40:11 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/01 16:40:11 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/01 16:40:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/01 16:40:11 INFO MemoryStore: MemoryStore cleared
23/12/01 16:40:11 INFO BlockManager: BlockManager stopped
23/12/01 16:40:11 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/01 16:40:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/01 16:40:11 INFO SparkContext: Successfully stopped SparkContext
23/12/01 16:40:11 INFO ShutdownHookManager: Shutdown hook called
23/12/01 16:40:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-57b6d772-01d6-4a25-89d9-7c5b3d7037d0
23/12/01 16:42:17 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:42:17 INFO xnio: XNIO version 3.8.7.Final
23/12/01 16:42:17 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/01 16:42:17 INFO threads: JBoss Threads version 3.1.0.Final
23/12/01 16:42:23 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:42:23 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:42:26 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/01 16:42:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/01 16:42:33 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:42:52 INFO SparkContext: Running Spark version 3.5.0
23/12/01 16:42:52 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/01 16:42:52 INFO SparkContext: Java version 11.0.20
23/12/01 16:42:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/01 16:42:52 INFO ResourceUtils: ==============================================================
23/12/01 16:42:52 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/01 16:42:52 INFO ResourceUtils: ==============================================================
23/12/01 16:42:52 INFO SparkContext: Submitted application: sparkApp
23/12/01 16:42:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/01 16:42:52 INFO ResourceProfile: Limiting resource is cpu
23/12/01 16:42:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/01 16:42:52 INFO SecurityManager: Changing view acls to: bsoleille
23/12/01 16:42:52 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/01 16:42:52 INFO SecurityManager: Changing view acls groups to: 
23/12/01 16:42:52 INFO SecurityManager: Changing modify acls groups to: 
23/12/01 16:42:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/01 16:42:52 INFO Utils: Successfully started service 'sparkDriver' on port 35303.
23/12/01 16:42:52 INFO SparkEnv: Registering MapOutputTracker
23/12/01 16:42:52 INFO SparkEnv: Registering BlockManagerMaster
23/12/01 16:42:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/01 16:42:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/01 16:42:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/01 16:42:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-07526d0e-5cb3-4aad-be10-1bb8f3863cec
23/12/01 16:42:52 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/01 16:42:52 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/01 16:42:53 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/01 16:42:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/01 16:42:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/01 16:42:53 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/01 16:42:53 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/01 16:42:53 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/01 16:42:53 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/01 16:42:53 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/01 16:42:53 INFO Executor: Java version 11.0.20
23/12/01 16:42:53 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/01 16:42:53 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@71dc044c for default.
23/12/01 16:42:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40739.
23/12/01 16:42:53 INFO NettyBlockTransferService: Server created on 10.25.86.80:40739
23/12/01 16:42:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/01 16:42:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 40739, None)
23/12/01 16:42:53 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:40739 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 40739, None)
23/12/01 16:42:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 40739, None)
23/12/01 16:42:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 40739, None)
23/12/01 16:42:53 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/01 16:42:53 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/01 16:43:05 INFO CodeGenerator: Code generated in 139.313405 ms
23/12/01 16:43:05 INFO SparkContext: Starting job: collect at SparkJob.scala:13
23/12/01 16:43:05 INFO DAGScheduler: Got job 0 (collect at SparkJob.scala:13) with 1 output partitions
23/12/01 16:43:05 INFO DAGScheduler: Final stage: ResultStage 0 (collect at SparkJob.scala:13)
23/12/01 16:43:05 INFO DAGScheduler: Parents of final stage: List()
23/12/01 16:43:05 INFO DAGScheduler: Missing parents: List()
23/12/01 16:43:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13), which has no missing parents
23/12/01 16:43:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 14.8 KiB, free 9.1 GiB)
23/12/01 16:43:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 9.1 GiB)
23/12/01 16:43:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:40739 (size: 6.1 KiB, free: 9.1 GiB)
23/12/01 16:43:05 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/01 16:43:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13) (first 15 tasks are for partitions Vector(0))
23/12/01 16:43:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/01 16:43:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/01 16:43:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/01 16:43:05 INFO CodeGenerator: Code generated in 7.410289 ms
23/12/01 16:43:06 INFO CodeGenerator: Code generated in 28.07724 ms
23/12/01 16:43:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1700 bytes result sent to driver
23/12/01 16:43:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 688 ms on 10.25.86.80 (executor driver) (1/1)
23/12/01 16:43:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/01 16:43:06 INFO DAGScheduler: ResultStage 0 (collect at SparkJob.scala:13) finished in 0.783 s
23/12/01 16:43:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/01 16:43:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/01 16:43:06 INFO DAGScheduler: Job 0 finished: collect at SparkJob.scala:13, took 0.807342 s
23/12/01 16:43:06 INFO CodeGenerator: Code generated in 9.653941 ms
23/12/01 16:44:33 INFO SparkContext: Invoking stop() from shutdown hook
23/12/01 16:44:33 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/01 16:44:33 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/01 16:44:33 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:44:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/01 16:44:33 INFO MemoryStore: MemoryStore cleared
23/12/01 16:44:33 INFO BlockManager: BlockManager stopped
23/12/01 16:44:33 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/01 16:44:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/01 16:44:33 INFO SparkContext: Successfully stopped SparkContext
23/12/01 16:44:33 INFO ShutdownHookManager: Shutdown hook called
23/12/01 16:44:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-dce47a7d-e974-4afc-90fa-58fa8cee69cd
23/12/01 16:44:55 WARN RemoteEndpoint: Unmatched cancel notification for request id 3357
23/12/01 16:45:25 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:45:25 INFO xnio: XNIO version 3.8.7.Final
23/12/01 16:45:25 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/01 16:45:25 INFO threads: JBoss Threads version 3.1.0.Final
23/12/01 16:45:37 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:45:37 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:45:47 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:46:00 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/01 16:46:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/01 16:46:27 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:47:04 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:47:04 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:47:04 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:47:04 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:47:04 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 16:47:04 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/01 16:47:24 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/01 17:24:09 ERROR CompilerAccess: A severe compiler error occurred, full details of the error can be found in the error report /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/.metals/.reports/metals-full/2023-12-01/r_compiler-error_(project-effective-scala-c1-s14-3039-4085)_17-24-09-833.md
23/12/01 23:24:38 WARN RemoteEndpoint: Unmatched cancel notification for request id 6192
23/12/01 23:47:21 WARN RemoteEndpoint: Unmatched cancel notification for request id 6988
23/12/01 23:51:56 WARN RemoteEndpoint: Unmatched cancel notification for request id 7260
23/12/02 13:50:46 WARN RemoteEndpoint: Unmatched cancel notification for request id 9527
23/12/02 13:52:11 INFO GenericEndpoint: Unsupported notification method: $/setTrace
23/12/02 13:52:12 INFO GenericEndpoint: Unsupported notification method: $/setTrace
23/12/02 13:52:15 INFO GenericEndpoint: Unsupported notification method: $/setTrace
23/12/02 13:52:17 INFO GenericEndpoint: Unsupported notification method: $/setTrace
23/12/02 13:52:33 INFO GenericEndpoint: Unsupported notification method: $/setTrace
23/12/02 13:52:36 INFO GenericEndpoint: Unsupported notification method: $/setTrace
23/12/02 13:52:40 INFO GenericEndpoint: Unsupported notification method: $/setTrace
23/12/02 13:52:44 INFO GenericEndpoint: Unsupported notification method: $/setTrace
23/12/02 13:52:47 INFO GenericEndpoint: Unsupported notification method: $/setTrace
23/12/02 22:34:17 WARN RemoteEndpoint: Unmatched cancel notification for request id 10108
23/12/02 22:43:36 WARN RemoteEndpoint: Unmatched cancel notification for request id 10932
23/12/02 22:45:17 WARN RemoteEndpoint: Unmatched cancel notification for request id 11042
23/12/02 22:46:25 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/02 22:46:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/02 22:46:26 INFO SparkContext: Running Spark version 3.5.0
23/12/02 22:46:26 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/02 22:46:26 INFO SparkContext: Java version 11.0.20
23/12/02 22:46:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/02 22:46:26 INFO ResourceUtils: ==============================================================
23/12/02 22:46:26 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/02 22:46:26 INFO ResourceUtils: ==============================================================
23/12/02 22:46:26 INFO SparkContext: Submitted application: sparkApp
23/12/02 22:46:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/02 22:46:26 INFO ResourceProfile: Limiting resource is cpu
23/12/02 22:46:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/02 22:46:26 INFO SecurityManager: Changing view acls to: bsoleille
23/12/02 22:46:26 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/02 22:46:26 INFO SecurityManager: Changing view acls groups to: 
23/12/02 22:46:26 INFO SecurityManager: Changing modify acls groups to: 
23/12/02 22:46:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/02 22:46:26 INFO Utils: Successfully started service 'sparkDriver' on port 42417.
23/12/02 22:46:26 INFO SparkEnv: Registering MapOutputTracker
23/12/02 22:46:26 INFO SparkEnv: Registering BlockManagerMaster
23/12/02 22:46:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/02 22:46:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/02 22:46:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/02 22:46:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c5bc54e6-41d1-433d-a884-a2d1a6394b6c
23/12/02 22:46:26 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/12/02 22:46:26 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/02 22:46:26 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/02 22:46:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/02 22:46:26 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/02 22:46:26 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/02 22:46:26 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/02 22:46:26 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/02 22:46:26 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/02 22:46:26 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/02 22:46:26 INFO Executor: Java version 11.0.20
23/12/02 22:46:26 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/02 22:46:26 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@108052f0 for default.
23/12/02 22:46:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40183.
23/12/02 22:46:26 INFO NettyBlockTransferService: Server created on 10.25.86.80:40183
23/12/02 22:46:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/02 22:46:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 40183, None)
23/12/02 22:46:26 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:40183 with 434.4 MiB RAM, BlockManagerId(driver, 10.25.86.80, 40183, None)
23/12/02 22:46:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 40183, None)
23/12/02 22:46:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 40183, None)
23/12/02 22:46:27 INFO threads: JBoss Threads version 3.1.0.Final
23/12/02 22:46:27 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/02 22:46:27 INFO xnio: XNIO version 3.8.7.Final
23/12/02 22:46:27 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/02 22:46:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/02 22:46:33 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/02 22:46:34 INFO CodeGenerator: Code generated in 127.394045 ms
23/12/02 22:46:34 INFO SparkContext: Starting job: collect at SparkJob.scala:13
23/12/02 22:46:34 INFO DAGScheduler: Got job 0 (collect at SparkJob.scala:13) with 1 output partitions
23/12/02 22:46:34 INFO DAGScheduler: Final stage: ResultStage 0 (collect at SparkJob.scala:13)
23/12/02 22:46:34 INFO DAGScheduler: Parents of final stage: List()
23/12/02 22:46:34 INFO DAGScheduler: Missing parents: List()
23/12/02 22:46:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13), which has no missing parents
23/12/02 22:46:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 14.8 KiB, free 434.4 MiB)
23/12/02 22:46:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.4 MiB)
23/12/02 22:46:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:40183 (size: 6.1 KiB, free: 434.4 MiB)
23/12/02 22:46:34 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/02 22:46:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13) (first 15 tasks are for partitions Vector(0))
23/12/02 22:46:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/02 22:46:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/02 22:46:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/02 22:46:34 INFO CodeGenerator: Code generated in 6.755156 ms
23/12/02 22:46:35 INFO CodeGenerator: Code generated in 26.507991 ms
23/12/02 22:46:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1700 bytes result sent to driver
23/12/02 22:46:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 758 ms on 10.25.86.80 (executor driver) (1/1)
23/12/02 22:46:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/02 22:46:35 INFO DAGScheduler: ResultStage 0 (collect at SparkJob.scala:13) finished in 0.878 s
23/12/02 22:46:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/02 22:46:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/02 22:46:35 INFO DAGScheduler: Job 0 finished: collect at SparkJob.scala:13, took 0.907479 s
23/12/02 22:46:35 INFO CodeGenerator: Code generated in 8.693206 ms
23/12/02 22:46:35 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/02 22:46:35 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/02 22:46:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/02 22:46:35 INFO MemoryStore: MemoryStore cleared
23/12/02 22:46:35 INFO BlockManager: BlockManager stopped
23/12/02 22:46:35 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/02 22:46:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/02 22:46:35 INFO SparkContext: Successfully stopped SparkContext
23/12/03 07:36:12 WARN RemoteEndpoint: Unmatched cancel notification for request id 12545
23/12/03 07:38:07 ERROR RemoteEndpoint: Internal error: java.lang.UnsupportedOperationException: tail of empty list
java.util.concurrent.CompletionException: java.lang.UnsupportedOperationException: tail of empty list
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.UnsupportedOperationException: tail of empty list
	at scala.collection.immutable.Nil$.tail(List.scala:664)
	at scala.collection.immutable.Nil$.tail(List.scala:661)
	at scala.meta.internal.metals.codeactions.CreateCompanionObjectCodeAction.$anonfun$endPosOfTree$1(CreateCompanionObjectCodeAction.scala:77)
	at scala.Option.flatMap(Option.scala:283)
	at scala.meta.internal.metals.codeactions.CreateCompanionObjectCodeAction.endPosOfTree(CreateCompanionObjectCodeAction.scala:75)
	at scala.meta.internal.metals.codeactions.CreateCompanionObjectCodeAction.$anonfun$contribute$5(CreateCompanionObjectCodeAction.scala:61)
	at scala.Option.map(Option.scala:242)
	at scala.meta.internal.metals.codeactions.CreateCompanionObjectCodeAction.$anonfun$contribute$4(CreateCompanionObjectCodeAction.scala:58)
	at scala.Option$WithFilter.flatMap(Option.scala:283)
	at scala.meta.internal.metals.codeactions.CreateCompanionObjectCodeAction.$anonfun$contribute$2(CreateCompanionObjectCodeAction.scala:56)
	at scala.Option.flatMap(Option.scala:283)
	at scala.meta.internal.metals.codeactions.CreateCompanionObjectCodeAction.$anonfun$contribute$1(CreateCompanionObjectCodeAction.scala:55)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	... 3 more
23/12/03 07:39:42 INFO ShutdownHookManager: Shutdown hook called
23/12/03 07:39:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-7bf99f39-956d-4c83-bcfd-5d6f051e0bbb
23/12/03 07:39:42 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_c67fdfa2/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/03 07:39:59 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/03 07:39:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/03 07:39:59 INFO SparkContext: Running Spark version 3.5.0
23/12/03 07:39:59 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/03 07:39:59 INFO SparkContext: Java version 11.0.20
23/12/03 07:39:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/03 07:39:59 INFO ResourceUtils: ==============================================================
23/12/03 07:39:59 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/03 07:39:59 INFO ResourceUtils: ==============================================================
23/12/03 07:39:59 INFO SparkContext: Submitted application: sparkApp
23/12/03 07:39:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/03 07:39:59 INFO ResourceProfile: Limiting resource is cpu
23/12/03 07:39:59 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/03 07:39:59 INFO SecurityManager: Changing view acls to: bsoleille
23/12/03 07:39:59 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/03 07:39:59 INFO SecurityManager: Changing view acls groups to: 
23/12/03 07:39:59 INFO SecurityManager: Changing modify acls groups to: 
23/12/03 07:39:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/03 07:39:59 INFO Utils: Successfully started service 'sparkDriver' on port 39179.
23/12/03 07:39:59 INFO SparkEnv: Registering MapOutputTracker
23/12/03 07:39:59 INFO SparkEnv: Registering BlockManagerMaster
23/12/03 07:39:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/03 07:39:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/03 07:39:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/03 07:39:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5e74cc00-e084-48e5-9177-53a6edbb5a73
23/12/03 07:39:59 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/12/03 07:40:00 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/03 07:40:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/03 07:40:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/03 07:40:00 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/03 07:40:00 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/03 07:40:00 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/03 07:40:00 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/03 07:40:00 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/03 07:40:00 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/03 07:40:00 INFO Executor: Java version 11.0.20
23/12/03 07:40:00 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/03 07:40:00 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1dfc6201 for default.
23/12/03 07:40:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36213.
23/12/03 07:40:00 INFO NettyBlockTransferService: Server created on 10.25.86.80:36213
23/12/03 07:40:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/03 07:40:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 36213, None)
23/12/03 07:40:00 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:36213 with 434.4 MiB RAM, BlockManagerId(driver, 10.25.86.80, 36213, None)
23/12/03 07:40:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 36213, None)
23/12/03 07:40:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 36213, None)
23/12/03 07:40:00 INFO threads: JBoss Threads version 3.1.0.Final
23/12/03 07:40:00 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/03 07:40:00 INFO xnio: XNIO version 3.8.7.Final
23/12/03 07:40:00 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/03 07:40:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/03 07:40:08 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/03 07:40:09 INFO CodeGenerator: Code generated in 120.614594 ms
23/12/03 07:40:09 INFO SparkContext: Starting job: collect at SparkJob.scala:13
23/12/03 07:40:09 INFO DAGScheduler: Got job 0 (collect at SparkJob.scala:13) with 1 output partitions
23/12/03 07:40:09 INFO DAGScheduler: Final stage: ResultStage 0 (collect at SparkJob.scala:13)
23/12/03 07:40:09 INFO DAGScheduler: Parents of final stage: List()
23/12/03 07:40:09 INFO DAGScheduler: Missing parents: List()
23/12/03 07:40:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13), which has no missing parents
23/12/03 07:40:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 14.8 KiB, free 434.4 MiB)
23/12/03 07:40:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.4 MiB)
23/12/03 07:40:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:36213 (size: 6.1 KiB, free: 434.4 MiB)
23/12/03 07:40:09 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/03 07:40:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13) (first 15 tasks are for partitions Vector(0))
23/12/03 07:40:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/03 07:40:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/03 07:40:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/03 07:40:09 INFO CodeGenerator: Code generated in 10.973462 ms
23/12/03 07:40:10 INFO CodeGenerator: Code generated in 25.735866 ms
23/12/03 07:40:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1700 bytes result sent to driver
23/12/03 07:40:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 779 ms on 10.25.86.80 (executor driver) (1/1)
23/12/03 07:40:10 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/03 07:40:10 INFO DAGScheduler: ResultStage 0 (collect at SparkJob.scala:13) finished in 0.877 s
23/12/03 07:40:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/03 07:40:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/03 07:40:10 INFO DAGScheduler: Job 0 finished: collect at SparkJob.scala:13, took 0.906075 s
23/12/03 07:40:10 INFO CodeGenerator: Code generated in 8.29866 ms
23/12/03 07:40:10 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/03 07:40:10 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/03 07:40:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/03 07:40:10 INFO MemoryStore: MemoryStore cleared
23/12/03 07:40:10 INFO BlockManager: BlockManager stopped
23/12/03 07:40:10 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/03 07:40:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/03 07:40:10 INFO SparkContext: Successfully stopped SparkContext
23/12/03 07:40:25 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/03 07:40:25 INFO SparkContext: SparkContext already stopped.
23/12/03 07:40:25 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/03 07:40:25 INFO SparkContext: SparkContext already stopped.
23/12/03 07:49:23 WARN RemoteEndpoint: Unmatched cancel notification for request id 13444
23/12/03 07:50:57 INFO ShutdownHookManager: Shutdown hook called
23/12/03 07:50:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-be278748-b70b-44c8-873a-ad8fd05687f5
23/12/03 07:50:57 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_5a7d4dd9/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/03 08:21:36 WARN RemoteEndpoint: Unmatched cancel notification for request id 15111
23/12/03 08:23:05 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/03 08:23:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/03 08:23:05 INFO SparkContext: Running Spark version 3.5.0
23/12/03 08:23:05 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/03 08:23:05 INFO SparkContext: Java version 11.0.20
23/12/03 08:23:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/03 08:23:06 INFO ResourceUtils: ==============================================================
23/12/03 08:23:06 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/03 08:23:06 INFO ResourceUtils: ==============================================================
23/12/03 08:23:06 INFO SparkContext: Submitted application: sparkApp
23/12/03 08:23:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/03 08:23:06 INFO ResourceProfile: Limiting resource is cpu
23/12/03 08:23:06 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/03 08:23:06 INFO SecurityManager: Changing view acls to: bsoleille
23/12/03 08:23:06 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/03 08:23:06 INFO SecurityManager: Changing view acls groups to: 
23/12/03 08:23:06 INFO SecurityManager: Changing modify acls groups to: 
23/12/03 08:23:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/03 08:23:06 INFO Utils: Successfully started service 'sparkDriver' on port 44889.
23/12/03 08:23:06 INFO SparkEnv: Registering MapOutputTracker
23/12/03 08:23:06 INFO SparkEnv: Registering BlockManagerMaster
23/12/03 08:23:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/03 08:23:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/03 08:23:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/03 08:23:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-38926cc7-5dc5-4ed4-ba4e-b2b14a0a6b33
23/12/03 08:23:06 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/12/03 08:23:06 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/03 08:23:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/03 08:23:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/03 08:23:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/03 08:23:06 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/03 08:23:06 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/03 08:23:06 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/03 08:23:06 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/03 08:23:06 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/03 08:23:06 INFO Executor: Java version 11.0.20
23/12/03 08:23:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/03 08:23:06 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7240ab59 for default.
23/12/03 08:23:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39529.
23/12/03 08:23:06 INFO NettyBlockTransferService: Server created on 10.25.86.80:39529
23/12/03 08:23:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/03 08:23:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 39529, None)
23/12/03 08:23:06 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:39529 with 434.4 MiB RAM, BlockManagerId(driver, 10.25.86.80, 39529, None)
23/12/03 08:23:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 39529, None)
23/12/03 08:23:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 39529, None)
23/12/03 08:23:06 INFO threads: JBoss Threads version 3.1.0.Final
23/12/03 08:23:06 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/03 08:23:06 INFO xnio: XNIO version 3.8.7.Final
23/12/03 08:23:06 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/03 08:23:14 INFO SparkContext: Invoking stop() from shutdown hook
23/12/03 08:23:14 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/03 08:23:14 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/03 08:23:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/03 08:23:14 INFO MemoryStore: MemoryStore cleared
23/12/03 08:23:14 INFO BlockManager: BlockManager stopped
23/12/03 08:23:14 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/03 08:23:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/03 08:23:14 INFO SparkContext: Successfully stopped SparkContext
23/12/03 08:23:14 INFO ShutdownHookManager: Shutdown hook called
23/12/03 08:23:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-6b3c96a5-2822-41a4-aea0-8778b5185c25
23/12/03 08:23:14 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_3dd4ae13/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/03 08:23:23 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/03 08:23:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/03 08:23:23 INFO SparkContext: Running Spark version 3.5.0
23/12/03 08:23:23 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/03 08:23:23 INFO SparkContext: Java version 11.0.20
23/12/03 08:23:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/03 08:23:23 INFO ResourceUtils: ==============================================================
23/12/03 08:23:23 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/03 08:23:23 INFO ResourceUtils: ==============================================================
23/12/03 08:23:23 INFO SparkContext: Submitted application: sparkApp
23/12/03 08:23:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/03 08:23:23 INFO ResourceProfile: Limiting resource is cpu
23/12/03 08:23:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/03 08:23:23 INFO SecurityManager: Changing view acls to: bsoleille
23/12/03 08:23:23 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/03 08:23:23 INFO SecurityManager: Changing view acls groups to: 
23/12/03 08:23:23 INFO SecurityManager: Changing modify acls groups to: 
23/12/03 08:23:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/03 08:23:23 INFO Utils: Successfully started service 'sparkDriver' on port 41851.
23/12/03 08:23:23 INFO SparkEnv: Registering MapOutputTracker
23/12/03 08:23:23 INFO SparkEnv: Registering BlockManagerMaster
23/12/03 08:23:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/03 08:23:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/03 08:23:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/03 08:23:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-442956b1-1610-477e-a193-c79ce04fb55e
23/12/03 08:23:23 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/12/03 08:23:23 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/03 08:23:23 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/03 08:23:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/03 08:23:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/03 08:23:23 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/03 08:23:23 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/03 08:23:23 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/03 08:23:23 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/03 08:23:23 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/03 08:23:23 INFO Executor: Java version 11.0.20
23/12/03 08:23:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/03 08:23:23 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@14d277a4 for default.
23/12/03 08:23:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38801.
23/12/03 08:23:23 INFO NettyBlockTransferService: Server created on 10.25.86.80:38801
23/12/03 08:23:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/03 08:23:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 38801, None)
23/12/03 08:23:23 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:38801 with 434.4 MiB RAM, BlockManagerId(driver, 10.25.86.80, 38801, None)
23/12/03 08:23:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 38801, None)
23/12/03 08:23:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 38801, None)
23/12/03 08:23:24 INFO threads: JBoss Threads version 3.1.0.Final
23/12/03 08:23:24 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/03 08:23:24 INFO xnio: XNIO version 3.8.7.Final
23/12/03 08:23:24 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/03 08:23:37 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/03 08:23:37 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/03 08:23:38 INFO CodeGenerator: Code generated in 136.395477 ms
23/12/03 08:23:39 INFO SparkContext: Starting job: collect at SparkJob.scala:13
23/12/03 08:23:39 INFO DAGScheduler: Got job 0 (collect at SparkJob.scala:13) with 1 output partitions
23/12/03 08:23:39 INFO DAGScheduler: Final stage: ResultStage 0 (collect at SparkJob.scala:13)
23/12/03 08:23:39 INFO DAGScheduler: Parents of final stage: List()
23/12/03 08:23:39 INFO DAGScheduler: Missing parents: List()
23/12/03 08:23:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13), which has no missing parents
23/12/03 08:23:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 14.8 KiB, free 434.4 MiB)
23/12/03 08:23:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.4 MiB)
23/12/03 08:23:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:38801 (size: 6.1 KiB, free: 434.4 MiB)
23/12/03 08:23:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/03 08:23:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13) (first 15 tasks are for partitions Vector(0))
23/12/03 08:23:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/03 08:23:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/03 08:23:39 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/03 08:23:39 INFO CodeGenerator: Code generated in 10.751369 ms
23/12/03 08:23:39 INFO CodeGenerator: Code generated in 26.120237 ms
23/12/03 08:23:39 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1700 bytes result sent to driver
23/12/03 08:23:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 752 ms on 10.25.86.80 (executor driver) (1/1)
23/12/03 08:23:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/03 08:23:39 INFO DAGScheduler: ResultStage 0 (collect at SparkJob.scala:13) finished in 0.864 s
23/12/03 08:23:39 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/03 08:23:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/03 08:23:39 INFO DAGScheduler: Job 0 finished: collect at SparkJob.scala:13, took 0.890316 s
23/12/03 08:23:39 INFO CodeGenerator: Code generated in 9.17196 ms
23/12/03 08:23:39 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/03 08:23:39 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/03 08:23:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/03 08:23:39 INFO MemoryStore: MemoryStore cleared
23/12/03 08:23:39 INFO BlockManager: BlockManager stopped
23/12/03 08:23:39 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/03 08:23:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/03 08:23:39 INFO SparkContext: Successfully stopped SparkContext
23/12/03 22:02:36 INFO ShutdownHookManager: Shutdown hook called
23/12/03 22:02:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-00ef0fa1-d3c7-4d59-aadf-f6acbb2ec2af
23/12/03 22:02:36 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_8b99e99e/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/03 22:03:54 ERROR CompilerAccess: A severe compiler error occurred, full details of the error can be found in the error report /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/.metals/.reports/metals-full/2023-12-03/r_compiler-error_(project-effective-scala-c1-s14-3039-4085)_22-03-54-764.md
23/12/03 22:04:31 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/03 22:04:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/03 22:04:31 INFO SparkContext: Running Spark version 3.5.0
23/12/03 22:04:31 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/03 22:04:31 INFO SparkContext: Java version 11.0.20
23/12/03 22:04:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/03 22:04:31 INFO ResourceUtils: ==============================================================
23/12/03 22:04:31 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/03 22:04:31 INFO ResourceUtils: ==============================================================
23/12/03 22:04:31 INFO SparkContext: Submitted application: sparkApp
23/12/03 22:04:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/03 22:04:31 INFO ResourceProfile: Limiting resource is cpu
23/12/03 22:04:31 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/03 22:04:31 INFO SecurityManager: Changing view acls to: bsoleille
23/12/03 22:04:31 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/03 22:04:31 INFO SecurityManager: Changing view acls groups to: 
23/12/03 22:04:31 INFO SecurityManager: Changing modify acls groups to: 
23/12/03 22:04:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/03 22:04:31 INFO Utils: Successfully started service 'sparkDriver' on port 45341.
23/12/03 22:04:31 INFO SparkEnv: Registering MapOutputTracker
23/12/03 22:04:31 INFO SparkEnv: Registering BlockManagerMaster
23/12/03 22:04:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/03 22:04:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/03 22:04:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/03 22:04:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5e72a832-cdf4-41fa-a4fb-57ca3b85ddc2
23/12/03 22:04:31 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/12/03 22:04:31 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/03 22:04:31 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/03 22:04:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/03 22:04:31 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/03 22:04:31 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/03 22:04:31 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/03 22:04:31 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/03 22:04:32 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/03 22:04:32 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/03 22:04:32 INFO Executor: Java version 11.0.20
23/12/03 22:04:32 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/03 22:04:32 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4a6ada35 for default.
23/12/03 22:04:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33149.
23/12/03 22:04:32 INFO NettyBlockTransferService: Server created on 10.25.86.80:33149
23/12/03 22:04:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/03 22:04:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 33149, None)
23/12/03 22:04:32 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:33149 with 434.4 MiB RAM, BlockManagerId(driver, 10.25.86.80, 33149, None)
23/12/03 22:04:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 33149, None)
23/12/03 22:04:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 33149, None)
23/12/03 22:04:32 INFO threads: JBoss Threads version 3.1.0.Final
23/12/03 22:04:32 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/03 22:04:32 INFO xnio: XNIO version 3.8.7.Final
23/12/03 22:04:32 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/03 22:04:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/03 22:04:40 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/03 22:04:41 INFO CodeGenerator: Code generated in 118.178224 ms
23/12/03 22:04:42 INFO SparkContext: Starting job: collect at SparkJob.scala:13
23/12/03 22:04:42 INFO DAGScheduler: Got job 0 (collect at SparkJob.scala:13) with 1 output partitions
23/12/03 22:04:42 INFO DAGScheduler: Final stage: ResultStage 0 (collect at SparkJob.scala:13)
23/12/03 22:04:42 INFO DAGScheduler: Parents of final stage: List()
23/12/03 22:04:42 INFO DAGScheduler: Missing parents: List()
23/12/03 22:04:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13), which has no missing parents
23/12/03 22:04:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 14.8 KiB, free 434.4 MiB)
23/12/03 22:04:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.4 MiB)
23/12/03 22:04:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:33149 (size: 6.1 KiB, free: 434.4 MiB)
23/12/03 22:04:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/03 22:04:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13) (first 15 tasks are for partitions Vector(0))
23/12/03 22:04:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/03 22:04:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/03 22:04:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/03 22:04:42 INFO CodeGenerator: Code generated in 9.680034 ms
23/12/03 22:04:42 INFO CodeGenerator: Code generated in 26.375605 ms
23/12/03 22:04:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1700 bytes result sent to driver
23/12/03 22:04:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 741 ms on 10.25.86.80 (executor driver) (1/1)
23/12/03 22:04:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/03 22:04:42 INFO DAGScheduler: ResultStage 0 (collect at SparkJob.scala:13) finished in 0.862 s
23/12/03 22:04:42 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/03 22:04:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/03 22:04:42 INFO DAGScheduler: Job 0 finished: collect at SparkJob.scala:13, took 0.886712 s
23/12/03 22:04:42 INFO CodeGenerator: Code generated in 8.623222 ms
23/12/03 22:04:42 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/03 22:04:43 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/03 22:04:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/03 22:04:43 INFO MemoryStore: MemoryStore cleared
23/12/03 22:04:43 INFO BlockManager: BlockManager stopped
23/12/03 22:04:43 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/03 22:04:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/03 22:04:43 INFO SparkContext: Successfully stopped SparkContext
23/12/03 22:05:55 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/03 22:05:55 INFO SparkContext: SparkContext already stopped.
23/12/03 22:06:14 INFO ShutdownHookManager: Shutdown hook called
23/12/03 22:06:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-c3b195e9-dc97-429c-8427-de3184bb3620
23/12/03 22:06:14 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_bbde6e35/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/03 22:06:23 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/03 22:06:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/03 22:06:23 INFO SparkContext: Running Spark version 3.5.0
23/12/03 22:06:23 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/03 22:06:23 INFO SparkContext: Java version 11.0.20
23/12/03 22:06:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/03 22:06:23 INFO ResourceUtils: ==============================================================
23/12/03 22:06:23 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/03 22:06:23 INFO ResourceUtils: ==============================================================
23/12/03 22:06:23 INFO SparkContext: Submitted application: sparkApp
23/12/03 22:06:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/03 22:06:23 INFO ResourceProfile: Limiting resource is cpu
23/12/03 22:06:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/03 22:06:23 INFO SecurityManager: Changing view acls to: bsoleille
23/12/03 22:06:23 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/03 22:06:23 INFO SecurityManager: Changing view acls groups to: 
23/12/03 22:06:23 INFO SecurityManager: Changing modify acls groups to: 
23/12/03 22:06:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/03 22:06:23 INFO Utils: Successfully started service 'sparkDriver' on port 41459.
23/12/03 22:06:23 INFO SparkEnv: Registering MapOutputTracker
23/12/03 22:06:23 INFO SparkEnv: Registering BlockManagerMaster
23/12/03 22:06:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/03 22:06:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/03 22:06:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/03 22:06:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d47df259-78d5-4774-8ce0-58e8ad8cf47d
23/12/03 22:06:23 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/12/03 22:06:23 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/03 22:06:23 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/03 22:06:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/03 22:06:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/03 22:06:23 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/03 22:06:23 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/03 22:06:23 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/03 22:06:24 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/03 22:06:24 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/03 22:06:24 INFO Executor: Java version 11.0.20
23/12/03 22:06:24 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/03 22:06:24 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3d5a2315 for default.
23/12/03 22:06:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39843.
23/12/03 22:06:24 INFO NettyBlockTransferService: Server created on 10.25.86.80:39843
23/12/03 22:06:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/03 22:06:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 39843, None)
23/12/03 22:06:24 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:39843 with 434.4 MiB RAM, BlockManagerId(driver, 10.25.86.80, 39843, None)
23/12/03 22:06:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 39843, None)
23/12/03 22:06:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 39843, None)
23/12/03 22:06:24 INFO threads: JBoss Threads version 3.1.0.Final
23/12/03 22:06:24 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/03 22:06:24 INFO xnio: XNIO version 3.8.7.Final
23/12/03 22:06:24 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/03 22:06:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/03 22:06:30 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/03 22:06:31 INFO CodeGenerator: Code generated in 129.304295 ms
23/12/03 22:06:31 INFO SparkContext: Starting job: collect at SparkJob.scala:13
23/12/03 22:06:31 INFO DAGScheduler: Got job 0 (collect at SparkJob.scala:13) with 1 output partitions
23/12/03 22:06:31 INFO DAGScheduler: Final stage: ResultStage 0 (collect at SparkJob.scala:13)
23/12/03 22:06:31 INFO DAGScheduler: Parents of final stage: List()
23/12/03 22:06:31 INFO DAGScheduler: Missing parents: List()
23/12/03 22:06:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13), which has no missing parents
23/12/03 22:06:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 14.8 KiB, free 434.4 MiB)
23/12/03 22:06:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.4 MiB)
23/12/03 22:06:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:39843 (size: 6.1 KiB, free: 434.4 MiB)
23/12/03 22:06:31 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/03 22:06:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13) (first 15 tasks are for partitions Vector(0))
23/12/03 22:06:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/03 22:06:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/03 22:06:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/03 22:06:31 INFO CodeGenerator: Code generated in 6.516206 ms
23/12/03 22:06:32 INFO CodeGenerator: Code generated in 25.903299 ms
23/12/03 22:06:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1700 bytes result sent to driver
23/12/03 22:06:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 694 ms on 10.25.86.80 (executor driver) (1/1)
23/12/03 22:06:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/03 22:06:32 INFO DAGScheduler: ResultStage 0 (collect at SparkJob.scala:13) finished in 0.792 s
23/12/03 22:06:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/03 22:06:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/03 22:06:32 INFO DAGScheduler: Job 0 finished: collect at SparkJob.scala:13, took 0.815907 s
23/12/03 22:06:32 INFO CodeGenerator: Code generated in 10.747463 ms
23/12/03 22:06:32 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/03 22:06:32 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/03 22:06:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/03 22:06:32 INFO MemoryStore: MemoryStore cleared
23/12/03 22:06:32 INFO BlockManager: BlockManager stopped
23/12/03 22:06:32 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/03 22:06:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/03 22:06:32 INFO SparkContext: Successfully stopped SparkContext
23/12/03 22:08:10 INFO ShutdownHookManager: Shutdown hook called
23/12/03 22:08:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-72c756ea-9c7a-4918-9a4a-3cd093454b81
23/12/03 22:08:10 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_cf2a513a/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/03 22:08:25 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/03 22:08:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/03 22:08:25 INFO SparkContext: Running Spark version 3.5.0
23/12/03 22:08:25 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/03 22:08:25 INFO SparkContext: Java version 11.0.20
23/12/03 22:08:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/03 22:08:25 INFO ResourceUtils: ==============================================================
23/12/03 22:08:25 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/03 22:08:25 INFO ResourceUtils: ==============================================================
23/12/03 22:08:25 INFO SparkContext: Submitted application: sparkApp
23/12/03 22:08:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/03 22:08:25 INFO ResourceProfile: Limiting resource is cpu
23/12/03 22:08:25 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/03 22:08:25 INFO SecurityManager: Changing view acls to: bsoleille
23/12/03 22:08:25 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/03 22:08:25 INFO SecurityManager: Changing view acls groups to: 
23/12/03 22:08:25 INFO SecurityManager: Changing modify acls groups to: 
23/12/03 22:08:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/03 22:08:26 INFO Utils: Successfully started service 'sparkDriver' on port 34719.
23/12/03 22:08:26 INFO SparkEnv: Registering MapOutputTracker
23/12/03 22:08:26 INFO SparkEnv: Registering BlockManagerMaster
23/12/03 22:08:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/03 22:08:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/03 22:08:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/03 22:08:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9a0c6f7a-2be8-4155-a082-204e3ee86b52
23/12/03 22:08:26 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/12/03 22:08:26 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/03 22:08:26 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/03 22:08:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/03 22:08:26 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/03 22:08:26 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/03 22:08:26 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/03 22:08:26 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/03 22:08:26 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/03 22:08:26 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/03 22:08:26 INFO Executor: Java version 11.0.20
23/12/03 22:08:26 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/03 22:08:26 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@15517ade for default.
23/12/03 22:08:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46547.
23/12/03 22:08:26 INFO NettyBlockTransferService: Server created on 10.25.86.80:46547
23/12/03 22:08:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/03 22:08:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 46547, None)
23/12/03 22:08:26 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:46547 with 434.4 MiB RAM, BlockManagerId(driver, 10.25.86.80, 46547, None)
23/12/03 22:08:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 46547, None)
23/12/03 22:08:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 46547, None)
23/12/03 22:08:26 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/03 22:08:26 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/03 22:08:26 INFO xnio: XNIO version 3.8.7.Final
23/12/03 22:08:26 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/03 22:08:26 INFO threads: JBoss Threads version 3.1.0.Final
23/12/03 22:08:26 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/03 22:08:26 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/03 22:08:26 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/03 22:08:26 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/03 22:08:26 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/03 22:08:26 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/03 22:08:26 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/03 22:08:26 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/03 22:08:26 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/03 22:08:27 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/03 22:08:27 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/03 22:08:27 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/03 22:12:53 WARN WadlFeature: JAX-B API not found . WADL feature is disabled.
23/12/04 09:51:30 WARN RemoteEndpoint: Unmatched cancel notification for request id 17436
23/12/04 09:51:32 WARN RemoteEndpoint: Unmatched cancel notification for request id 17441
23/12/04 09:55:41 WARN RemoteEndpoint: Unmatched cancel notification for request id 18017
23/12/04 10:12:45 WARN RemoteEndpoint: Unmatched cancel notification for request id 19290
23/12/04 10:22:10 WARN RemoteEndpoint: Unmatched cancel notification for request id 20675
23/12/04 10:35:22 WARN RemoteEndpoint: Unmatched cancel notification for request id 21146
23/12/04 10:38:45 WARN RemoteEndpoint: Unmatched cancel notification for request id 21575
23/12/04 10:41:46 WARN RemoteEndpoint: Unmatched cancel notification for request id 21910
23/12/04 10:41:46 WARN RemoteEndpoint: Unmatched cancel notification for request id 21911
23/12/04 10:42:05 ERROR RemoteEndpoint: Internal error: scala.meta.internal.metals.ScalafixProvider$ScalafixRunException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/.metals/.tmp/src/main/scala/sparkjobs/DataframesAndExpressions.scala:6: error: identifier expected but , found
import org.apache.spark.sql.{, Dataset, Row, DataFrame}
                             ^
java.util.concurrent.CompletionException: scala.meta.internal.metals.ScalafixProvider$ScalafixRunException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/.metals/.tmp/src/main/scala/sparkjobs/DataframesAndExpressions.scala:6: error: identifier expected but , found
import org.apache.spark.sql.{, Dataset, Row, DataFrame}
                             ^
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: scala.meta.internal.metals.ScalafixProvider$ScalafixRunException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/.metals/.tmp/src/main/scala/sparkjobs/DataframesAndExpressions.scala:6: error: identifier expected but , found
import org.apache.spark.sql.{, Dataset, Row, DataFrame}
                             ^
	at scala.meta.internal.metals.ScalafixProvider.$anonfun$runScalafixRules$1(ScalafixProvider.scala:159)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)
	... 3 more
23/12/04 10:42:05 ERROR RemoteEndpoint: Internal error: scala.meta.internal.metals.ScalafixProvider$ScalafixRunException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/.metals/.tmp/src/main/scala/sparkjobs/DataframesAndExpressions.scala:6: error: identifier expected but , found
import org.apache.spark.sql.{, Dataset, Row, DataFrame}
                             ^
java.util.concurrent.CompletionException: scala.meta.internal.metals.ScalafixProvider$ScalafixRunException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/.metals/.tmp/src/main/scala/sparkjobs/DataframesAndExpressions.scala:6: error: identifier expected but , found
import org.apache.spark.sql.{, Dataset, Row, DataFrame}
                             ^
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:704)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: scala.meta.internal.metals.ScalafixProvider$ScalafixRunException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/.metals/.tmp/src/main/scala/sparkjobs/DataframesAndExpressions.scala:6: error: identifier expected but , found
import org.apache.spark.sql.{, Dataset, Row, DataFrame}
                             ^
	at scala.meta.internal.metals.ScalafixProvider.$anonfun$runScalafixRules$1(ScalafixProvider.scala:159)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)
	... 3 more
23/12/04 10:47:31 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 10:47:31 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 10:47:31 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 10:47:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 10:47:31 INFO MemoryStore: MemoryStore cleared
23/12/04 10:47:31 INFO BlockManager: BlockManager stopped
23/12/04 10:47:31 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 10:47:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 10:47:31 INFO SparkContext: Successfully stopped SparkContext
23/12/04 10:47:31 INFO ShutdownHookManager: Shutdown hook called
23/12/04 10:47:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-411257ab-f6e1-4e07-85a4-4f156204e3c9
23/12/04 10:51:12 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 10:51:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 10:51:12 INFO SparkContext: Running Spark version 3.5.0
23/12/04 10:51:12 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 10:51:12 INFO SparkContext: Java version 11.0.20
23/12/04 10:51:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 10:51:12 INFO ResourceUtils: ==============================================================
23/12/04 10:51:12 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 10:51:12 INFO ResourceUtils: ==============================================================
23/12/04 10:51:12 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 10:51:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 10:51:12 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 10:51:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 10:51:12 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 10:51:12 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 10:51:12 INFO SecurityManager: Changing view acls groups to: 
23/12/04 10:51:12 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 10:51:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 10:51:12 INFO Utils: Successfully started service 'sparkDriver' on port 43943.
23/12/04 10:51:12 INFO SparkEnv: Registering MapOutputTracker
23/12/04 10:51:12 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 10:51:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 10:51:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 10:51:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 10:51:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0d458197-1d07-4a74-91cd-131d0719563e
23/12/04 10:51:12 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/12/04 10:51:12 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 10:51:12 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 10:51:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 10:51:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 10:51:13 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 10:51:13 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 10:51:13 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 10:51:13 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 10:51:13 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 10:51:13 INFO Executor: Java version 11.0.20
23/12/04 10:51:13 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 10:51:13 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@118775d for default.
23/12/04 10:51:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45157.
23/12/04 10:51:13 INFO NettyBlockTransferService: Server created on 10.25.86.80:45157
23/12/04 10:51:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 10:51:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 45157, None)
23/12/04 10:51:13 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:45157 with 434.4 MiB RAM, BlockManagerId(driver, 10.25.86.80, 45157, None)
23/12/04 10:51:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 45157, None)
23/12/04 10:51:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 45157, None)
23/12/04 10:51:13 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 10:51:13 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 10:51:13 INFO xnio: XNIO version 3.8.7.Final
23/12/04 10:51:13 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 10:51:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 10:51:23 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 10:51:24 INFO CodeGenerator: Code generated in 123.356817 ms
23/12/04 10:51:24 INFO SparkContext: Starting job: collect at SparkJob.scala:13
23/12/04 10:51:24 INFO DAGScheduler: Got job 0 (collect at SparkJob.scala:13) with 1 output partitions
23/12/04 10:51:24 INFO DAGScheduler: Final stage: ResultStage 0 (collect at SparkJob.scala:13)
23/12/04 10:51:24 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:51:24 INFO DAGScheduler: Missing parents: List()
23/12/04 10:51:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13), which has no missing parents
23/12/04 10:51:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 14.8 KiB, free 434.4 MiB)
23/12/04 10:51:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.4 MiB)
23/12/04 10:51:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:45157 (size: 6.1 KiB, free: 434.4 MiB)
23/12/04 10:51:24 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/04 10:51:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at collect at SparkJob.scala:13) (first 15 tasks are for partitions Vector(0))
23/12/04 10:51:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/04 10:51:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 10:51:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/04 10:51:25 INFO CodeGenerator: Code generated in 8.657464 ms
23/12/04 10:51:25 INFO CodeGenerator: Code generated in 26.585459 ms
23/12/04 10:51:25 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1700 bytes result sent to driver
23/12/04 10:51:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 813 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:51:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/04 10:51:25 INFO DAGScheduler: ResultStage 0 (collect at SparkJob.scala:13) finished in 0.913 s
23/12/04 10:51:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:51:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/04 10:51:25 INFO DAGScheduler: Job 0 finished: collect at SparkJob.scala:13, took 0.938364 s
23/12/04 10:51:25 INFO CodeGenerator: Code generated in 10.339321 ms
23/12/04 10:51:52 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 10:51:52 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 10:51:52 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 10:51:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 10:51:52 INFO MemoryStore: MemoryStore cleared
23/12/04 10:51:52 INFO BlockManager: BlockManager stopped
23/12/04 10:51:52 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 10:51:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 10:51:52 INFO SparkContext: Successfully stopped SparkContext
23/12/04 10:51:52 INFO ShutdownHookManager: Shutdown hook called
23/12/04 10:51:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-16bd182d-8f4c-4dce-9e4e-0dbf5f8b5d19
23/12/04 10:51:52 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_7f15e44d/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/04 10:53:49 WARN RemoteEndpoint: Unmatched cancel notification for request id 22887
23/12/04 10:54:39 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 10:54:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 10:54:39 INFO SparkContext: Running Spark version 3.5.0
23/12/04 10:54:39 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 10:54:39 INFO SparkContext: Java version 11.0.20
23/12/04 10:54:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 10:54:39 INFO ResourceUtils: ==============================================================
23/12/04 10:54:39 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 10:54:39 INFO ResourceUtils: ==============================================================
23/12/04 10:54:39 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 10:54:39 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 10:54:39 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 10:54:39 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 10:54:39 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 10:54:39 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 10:54:39 INFO SecurityManager: Changing view acls groups to: 
23/12/04 10:54:39 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 10:54:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 10:54:39 INFO Utils: Successfully started service 'sparkDriver' on port 45845.
23/12/04 10:54:39 INFO SparkEnv: Registering MapOutputTracker
23/12/04 10:54:39 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 10:54:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 10:54:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 10:54:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 10:54:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ae2c9309-c2ef-4ff6-a5b8-dd7e674e851a
23/12/04 10:54:39 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 10:54:39 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 10:54:39 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 10:54:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 10:54:39 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 10:54:39 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 10:54:39 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 10:54:39 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 10:54:39 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 10:54:39 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 10:54:39 INFO Executor: Java version 11.0.20
23/12/04 10:54:39 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 10:54:39 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5cac6a59 for default.
23/12/04 10:54:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40101.
23/12/04 10:54:39 INFO NettyBlockTransferService: Server created on 10.25.86.80:40101
23/12/04 10:54:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 10:54:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 40101, None)
23/12/04 10:54:39 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:40101 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 40101, None)
23/12/04 10:54:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 40101, None)
23/12/04 10:54:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 40101, None)
23/12/04 10:54:40 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 10:54:40 INFO xnio: XNIO version 3.8.7.Final
23/12/04 10:54:40 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 10:54:40 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 10:54:40 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 10:54:40 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 10:54:40 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 10:54:40 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 10:54:40 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 10:54:40 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 10:54:40 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 10:54:40 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 10:54:40 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 10:54:40 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 10:54:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 10:54:40 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 10:54:42 INFO CodeGenerator: Code generated in 123.434809 ms
23/12/04 10:54:42 INFO CodeGenerator: Code generated in 13.351105 ms
23/12/04 10:54:42 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 10:54:42 INFO DAGScheduler: Got job 0 (collect at Ast.scala:253) with 1 output partitions
23/12/04 10:54:42 INFO DAGScheduler: Final stage: ResultStage 0 (collect at Ast.scala:253)
23/12/04 10:54:42 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:42 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:253), which has no missing parents
23/12/04 10:54:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 10:54:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 10:54:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:40101 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 10:54:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/04 10:54:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 10:54:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/04 10:54:42 INFO CodeGenerator: Code generated in 6.608134 ms
23/12/04 10:54:42 INFO CodeGenerator: Code generated in 7.128984 ms
23/12/04 10:54:42 INFO CodeGenerator: Code generated in 22.529754 ms
23/12/04 10:54:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1869 bytes result sent to driver
23/12/04 10:54:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 645 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/04 10:54:43 INFO DAGScheduler: ResultStage 0 (collect at Ast.scala:253) finished in 0.736 s
23/12/04 10:54:43 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/04 10:54:43 INFO DAGScheduler: Job 0 finished: collect at Ast.scala:253, took 0.761293 s
23/12/04 10:54:43 INFO DAGScheduler: Registering RDD 8 (collect at Ast.scala:253) as input to shuffle 0
23/12/04 10:54:43 INFO DAGScheduler: Got map stage job 1 (collect at Ast.scala:253) with 1 output partitions
23/12/04 10:54:43 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at Ast.scala:253)
23/12/04 10:54:43 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:43 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:43 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:253), which has no missing parents
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.25.86.80:40101 (size: 8.2 KiB, free: 9.1 GiB)
23/12/04 10:54:43 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:43 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:43 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/12/04 10:54:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 10:54:43 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/12/04 10:54:43 INFO CodeGenerator: Code generated in 6.072936 ms
23/12/04 10:54:43 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1793 bytes result sent to driver
23/12/04 10:54:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 54 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/04 10:54:43 INFO DAGScheduler: ShuffleMapStage 1 (collect at Ast.scala:253) finished in 0.068 s
23/12/04 10:54:43 INFO DAGScheduler: looking for newly runnable stages
23/12/04 10:54:43 INFO DAGScheduler: running: HashSet()
23/12/04 10:54:43 INFO DAGScheduler: waiting: HashSet()
23/12/04 10:54:43 INFO DAGScheduler: failed: HashSet()
23/12/04 10:54:43 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 10:54:43 INFO CodeGenerator: Code generated in 7.803826 ms
23/12/04 10:54:43 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 10:54:43 INFO DAGScheduler: Got job 2 (collect at Ast.scala:253) with 1 output partitions
23/12/04 10:54:43 INFO DAGScheduler: Final stage: ResultStage 3 (collect at Ast.scala:253)
23/12/04 10:54:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
23/12/04 10:54:43 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:43 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:253), which has no missing parents
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.25.86.80:40101 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 10:54:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:43 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
23/12/04 10:54:43 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 10:54:43 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
23/12/04 10:54:43 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 10:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
23/12/04 10:54:43 INFO CodeGenerator: Code generated in 9.288953 ms
23/12/04 10:54:43 INFO CodeGenerator: Code generated in 6.24916 ms
23/12/04 10:54:43 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4537 bytes result sent to driver
23/12/04 10:54:43 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 88 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:43 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
23/12/04 10:54:43 INFO DAGScheduler: ResultStage 3 (collect at Ast.scala:253) finished in 0.097 s
23/12/04 10:54:43 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
23/12/04 10:54:43 INFO DAGScheduler: Job 2 finished: collect at Ast.scala:253, took 0.104207 s
23/12/04 10:54:43 INFO CodeGenerator: Code generated in 7.330523 ms
23/12/04 10:54:43 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 10:54:43 INFO DAGScheduler: Got job 3 (collect at Ast.scala:253) with 1 output partitions
23/12/04 10:54:43 INFO DAGScheduler: Final stage: ResultStage 4 (collect at Ast.scala:253)
23/12/04 10:54:43 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:43 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:43 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:253), which has no missing parents
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.25.86.80:40101 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 10:54:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:43 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/12/04 10:54:43 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/04 10:54:43 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
23/12/04 10:54:43 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 1826 bytes result sent to driver
23/12/04 10:54:43 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:43 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/12/04 10:54:43 INFO DAGScheduler: ResultStage 4 (collect at Ast.scala:253) finished in 0.024 s
23/12/04 10:54:43 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/12/04 10:54:43 INFO DAGScheduler: Job 3 finished: collect at Ast.scala:253, took 0.027692 s
23/12/04 10:54:43 INFO DAGScheduler: Registering RDD 16 (collect at Ast.scala:253) as input to shuffle 1
23/12/04 10:54:43 INFO DAGScheduler: Got map stage job 4 (collect at Ast.scala:253) with 1 output partitions
23/12/04 10:54:43 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (collect at Ast.scala:253)
23/12/04 10:54:43 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:43 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:43 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:253), which has no missing parents
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.25.86.80:40101 (size: 8.2 KiB, free: 9.1 GiB)
23/12/04 10:54:43 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:43 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:43 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
23/12/04 10:54:43 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 10:54:43 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
23/12/04 10:54:43 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1793 bytes result sent to driver
23/12/04 10:54:43 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:43 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
23/12/04 10:54:43 INFO DAGScheduler: ShuffleMapStage 5 (collect at Ast.scala:253) finished in 0.021 s
23/12/04 10:54:43 INFO DAGScheduler: looking for newly runnable stages
23/12/04 10:54:43 INFO DAGScheduler: running: HashSet()
23/12/04 10:54:43 INFO DAGScheduler: waiting: HashSet()
23/12/04 10:54:43 INFO DAGScheduler: failed: HashSet()
23/12/04 10:54:43 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 10:54:43 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 10:54:43 INFO DAGScheduler: Got job 5 (collect at Ast.scala:253) with 1 output partitions
23/12/04 10:54:43 INFO DAGScheduler: Final stage: ResultStage 7 (collect at Ast.scala:253)
23/12/04 10:54:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
23/12/04 10:54:43 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:43 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:253), which has no missing parents
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.25.86.80:40101 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 10:54:43 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:43 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
23/12/04 10:54:43 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 10:54:43 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)
23/12/04 10:54:43 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 10:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 10:54:43 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 4494 bytes result sent to driver
23/12/04 10:54:43 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:43 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
23/12/04 10:54:43 INFO DAGScheduler: ResultStage 7 (collect at Ast.scala:253) finished in 0.017 s
23/12/04 10:54:43 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
23/12/04 10:54:43 INFO DAGScheduler: Job 5 finished: collect at Ast.scala:253, took 0.021055 s
23/12/04 10:54:43 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 10:54:43 INFO DAGScheduler: Got job 6 (collect at Ast.scala:253) with 1 output partitions
23/12/04 10:54:43 INFO DAGScheduler: Final stage: ResultStage 8 (collect at Ast.scala:253)
23/12/04 10:54:43 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:43 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:43 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:253), which has no missing parents
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.25.86.80:40101 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 10:54:43 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:43 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
23/12/04 10:54:43 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 10:54:43 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)
23/12/04 10:54:43 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 1826 bytes result sent to driver
23/12/04 10:54:43 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:43 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
23/12/04 10:54:43 INFO DAGScheduler: ResultStage 8 (collect at Ast.scala:253) finished in 0.020 s
23/12/04 10:54:43 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
23/12/04 10:54:43 INFO DAGScheduler: Job 6 finished: collect at Ast.scala:253, took 0.022230 s
23/12/04 10:54:43 INFO DAGScheduler: Registering RDD 24 (collect at Ast.scala:253) as input to shuffle 2
23/12/04 10:54:43 INFO DAGScheduler: Got map stage job 7 (collect at Ast.scala:253) with 1 output partitions
23/12/04 10:54:43 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (collect at Ast.scala:253)
23/12/04 10:54:43 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:43 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:43 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:253), which has no missing parents
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.25.86.80:40101 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 10:54:43 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:43 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:43 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
23/12/04 10:54:43 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/04 10:54:43 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)
23/12/04 10:54:43 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 1793 bytes result sent to driver
23/12/04 10:54:43 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 15 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:43 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
23/12/04 10:54:43 INFO DAGScheduler: ShuffleMapStage 9 (collect at Ast.scala:253) finished in 0.021 s
23/12/04 10:54:43 INFO DAGScheduler: looking for newly runnable stages
23/12/04 10:54:43 INFO DAGScheduler: running: HashSet()
23/12/04 10:54:43 INFO DAGScheduler: waiting: HashSet()
23/12/04 10:54:43 INFO DAGScheduler: failed: HashSet()
23/12/04 10:54:43 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 10:54:43 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 10:54:43 INFO DAGScheduler: Got job 8 (collect at Ast.scala:253) with 1 output partitions
23/12/04 10:54:43 INFO DAGScheduler: Final stage: ResultStage 11 (collect at Ast.scala:253)
23/12/04 10:54:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
23/12/04 10:54:43 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:43 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:253), which has no missing parents
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.25.86.80:40101 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 10:54:43 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:43 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
23/12/04 10:54:43 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 10:54:43 INFO Executor: Running task 0.0 in stage 11.0 (TID 8)
23/12/04 10:54:43 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 10:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 10:54:43 INFO Executor: Finished task 0.0 in stage 11.0 (TID 8). 4494 bytes result sent to driver
23/12/04 10:54:43 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:43 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
23/12/04 10:54:43 INFO DAGScheduler: ResultStage 11 (collect at Ast.scala:253) finished in 0.022 s
23/12/04 10:54:43 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
23/12/04 10:54:43 INFO DAGScheduler: Job 8 finished: collect at Ast.scala:253, took 0.028011 s
23/12/04 10:54:43 INFO InMemoryFileIndex: It took 18 ms to list leaf files for 1 paths.
23/12/04 10:54:43 INFO SparkContext: Starting job: parquet at Ast.scala:279
23/12/04 10:54:43 INFO DAGScheduler: Got job 9 (parquet at Ast.scala:279) with 1 output partitions
23/12/04 10:54:43 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at Ast.scala:279)
23/12/04 10:54:43 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:43 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:43 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:279), which has no missing parents
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 10:54:43 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.25.86.80:40101 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 10:54:43 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:279) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:43 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
23/12/04 10:54:43 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 10:54:43 INFO Executor: Running task 0.0 in stage 12.0 (TID 9)
23/12/04 10:54:43 INFO Executor: Finished task 0.0 in stage 12.0 (TID 9). 2085 bytes result sent to driver
23/12/04 10:54:43 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 232 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:43 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
23/12/04 10:54:43 INFO DAGScheduler: ResultStage 12 (parquet at Ast.scala:279) finished in 0.247 s
23/12/04 10:54:43 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
23/12/04 10:54:43 INFO DAGScheduler: Job 9 finished: parquet at Ast.scala:279, took 0.250029 s
23/12/04 10:54:44 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/12/04 10:54:44 INFO SparkContext: Starting job: parquet at Ast.scala:279
23/12/04 10:54:44 INFO DAGScheduler: Got job 10 (parquet at Ast.scala:279) with 1 output partitions
23/12/04 10:54:44 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at Ast.scala:279)
23/12/04 10:54:44 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:44 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:44 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[31] at parquet at Ast.scala:279), which has no missing parents
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.25.86.80:40101 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[31] at parquet at Ast.scala:279) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:44 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
23/12/04 10:54:44 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 10:54:44 INFO Executor: Running task 0.0 in stage 13.0 (TID 10)
23/12/04 10:54:44 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.25.86.80:40101 in memory (size: 8.2 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO Executor: Finished task 0.0 in stage 13.0 (TID 10). 2042 bytes result sent to driver
23/12/04 10:54:44 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 14 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:44 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
23/12/04 10:54:44 INFO DAGScheduler: ResultStage 13 (parquet at Ast.scala:279) finished in 0.027 s
23/12/04 10:54:44 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.25.86.80:40101 in memory (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
23/12/04 10:54:44 INFO DAGScheduler: Job 10 finished: parquet at Ast.scala:279, took 0.030383 s
23/12/04 10:54:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.25.86.80:40101 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/12/04 10:54:44 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.25.86.80:40101 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.25.86.80:40101 in memory (size: 8.2 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.25.86.80:40101 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.25.86.80:40101 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.25.86.80:40101 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.25.86.80:40101 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.25.86.80:40101 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO SparkContext: Starting job: parquet at Ast.scala:279
23/12/04 10:54:44 INFO DAGScheduler: Got job 11 (parquet at Ast.scala:279) with 1 output partitions
23/12/04 10:54:44 INFO DAGScheduler: Final stage: ResultStage 14 (parquet at Ast.scala:279)
23/12/04 10:54:44 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:44 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:44 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[33] at parquet at Ast.scala:279), which has no missing parents
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.25.86.80:40101 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[33] at parquet at Ast.scala:279) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:44 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
23/12/04 10:54:44 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 10:54:44 INFO Executor: Running task 0.0 in stage 14.0 (TID 11)
23/12/04 10:54:44 INFO Executor: Finished task 0.0 in stage 14.0 (TID 11). 2042 bytes result sent to driver
23/12/04 10:54:44 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:44 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
23/12/04 10:54:44 INFO DAGScheduler: ResultStage 14 (parquet at Ast.scala:279) finished in 0.020 s
23/12/04 10:54:44 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
23/12/04 10:54:44 INFO DAGScheduler: Job 11 finished: parquet at Ast.scala:279, took 0.021624 s
23/12/04 10:54:44 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 10:54:44 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 10:54:44 INFO CodeGenerator: Code generated in 15.775579 ms
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.25.86.80:40101 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO SparkContext: Created broadcast 12 from show at Ast.scala:95
23/12/04 10:54:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 10:54:44 INFO SparkContext: Starting job: show at Ast.scala:95
23/12/04 10:54:44 INFO DAGScheduler: Got job 12 (show at Ast.scala:95) with 1 output partitions
23/12/04 10:54:44 INFO DAGScheduler: Final stage: ResultStage 15 (show at Ast.scala:95)
23/12/04 10:54:44 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:44 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:44 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[37] at show at Ast.scala:95), which has no missing parents
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.25.86.80:40101 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[37] at show at Ast.scala:95) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:44 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
23/12/04 10:54:44 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 10:54:44 INFO Executor: Running task 0.0 in stage 15.0 (TID 12)
23/12/04 10:54:44 INFO CodeGenerator: Code generated in 11.0006 ms
23/12/04 10:54:44 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 10:54:44 INFO CodecPool: Got brand-new decompressor [.snappy]
23/12/04 10:54:44 INFO Executor: Finished task 0.0 in stage 15.0 (TID 12). 1928 bytes result sent to driver
23/12/04 10:54:44 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 246 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:44 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
23/12/04 10:54:44 INFO DAGScheduler: ResultStage 15 (show at Ast.scala:95) finished in 0.261 s
23/12/04 10:54:44 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
23/12/04 10:54:44 INFO DAGScheduler: Job 12 finished: show at Ast.scala:95, took 0.265201 s
23/12/04 10:54:44 INFO CodeGenerator: Code generated in 5.967299 ms
23/12/04 10:54:44 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 10:54:44 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.25.86.80:40101 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO SparkContext: Created broadcast 14 from show at Ast.scala:96
23/12/04 10:54:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 10:54:44 INFO SparkContext: Starting job: show at Ast.scala:96
23/12/04 10:54:44 INFO DAGScheduler: Got job 13 (show at Ast.scala:96) with 1 output partitions
23/12/04 10:54:44 INFO DAGScheduler: Final stage: ResultStage 16 (show at Ast.scala:96)
23/12/04 10:54:44 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:44 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:44 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[41] at show at Ast.scala:96), which has no missing parents
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.25.86.80:40101 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[41] at show at Ast.scala:96) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:44 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
23/12/04 10:54:44 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 13) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 10:54:44 INFO Executor: Running task 0.0 in stage 16.0 (TID 13)
23/12/04 10:54:44 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 10:54:44 INFO Executor: Finished task 0.0 in stage 16.0 (TID 13). 1928 bytes result sent to driver
23/12/04 10:54:44 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 13) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:44 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
23/12/04 10:54:44 INFO DAGScheduler: ResultStage 16 (show at Ast.scala:96) finished in 0.017 s
23/12/04 10:54:44 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
23/12/04 10:54:44 INFO DAGScheduler: Job 13 finished: show at Ast.scala:96, took 0.019814 s
23/12/04 10:54:44 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 10:54:44 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 10:54:44 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 10:54:44 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 10:54:44 INFO CodeGenerator: Code generated in 6.881575 ms
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.25.86.80:40101 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO SparkContext: Created broadcast 16 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 10:54:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 10:54:44 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 10:54:44 INFO DAGScheduler: Got job 14 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 10:54:44 INFO DAGScheduler: Final stage: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 10:54:44 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:44 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:44 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[45] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.25.86.80:40101 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 10:54:44 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[45] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:44 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
23/12/04 10:54:44 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 14) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 10:54:44 INFO Executor: Running task 0.0 in stage 17.0 (TID 14)
23/12/04 10:54:44 INFO CodeGenerator: Code generated in 9.540665 ms
23/12/04 10:54:44 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 10:54:44 INFO Executor: Finished task 0.0 in stage 17.0 (TID 14). 2007 bytes result sent to driver
23/12/04 10:54:44 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 14) in 81 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:44 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
23/12/04 10:54:44 INFO DAGScheduler: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.084 s
23/12/04 10:54:44 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
23/12/04 10:54:44 INFO DAGScheduler: Job 14 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.086305 s
23/12/04 10:54:44 INFO CodeGenerator: Code generated in 3.998101 ms
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 10:54:44 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 10:54:44 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.25.86.80:40101 (size: 318.0 B, free: 9.1 GiB)
23/12/04 10:54:44 INFO SparkContext: Created broadcast 18 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 10:54:44 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 10:54:44 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 12.532792 ms
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.25.86.80:40101 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 19 from show at Ast.scala:108
23/12/04 10:54:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 10:54:45 INFO SparkContext: Starting job: show at Ast.scala:108
23/12/04 10:54:45 INFO DAGScheduler: Got job 15 (show at Ast.scala:108) with 1 output partitions
23/12/04 10:54:45 INFO DAGScheduler: Final stage: ResultStage 18 (show at Ast.scala:108)
23/12/04 10:54:45 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:45 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:45 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[49] at show at Ast.scala:108), which has no missing parents
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 19.6 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.25.86.80:40101 (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[49] at show at Ast.scala:108) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:45 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
23/12/04 10:54:45 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 15) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 10:54:45 INFO Executor: Running task 0.0 in stage 18.0 (TID 15)
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 10.469807 ms
23/12/04 10:54:45 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 10:54:45 INFO Executor: Finished task 0.0 in stage 18.0 (TID 15). 2118 bytes result sent to driver
23/12/04 10:54:45 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 15) in 25 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:45 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
23/12/04 10:54:45 INFO DAGScheduler: ResultStage 18 (show at Ast.scala:108) finished in 0.032 s
23/12/04 10:54:45 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
23/12/04 10:54:45 INFO DAGScheduler: Job 15 finished: show at Ast.scala:108, took 0.033825 s
23/12/04 10:54:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 10:54:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 10:54:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 10:54:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.25.86.80:40101 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 21 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 10:54:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 10:54:45 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 10:54:45 INFO DAGScheduler: Got job 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 10:54:45 INFO DAGScheduler: Final stage: ResultStage 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 10:54:45 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:45 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:45 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[53] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.25.86.80:40101 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[53] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:45 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
23/12/04 10:54:45 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 16) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 10:54:45 INFO Executor: Running task 0.0 in stage 19.0 (TID 16)
23/12/04 10:54:45 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 10:54:45 INFO Executor: Finished task 0.0 in stage 19.0 (TID 16). 2007 bytes result sent to driver
23/12/04 10:54:45 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 16) in 8 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:45 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
23/12/04 10:54:45 INFO DAGScheduler: ResultStage 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.014 s
23/12/04 10:54:45 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
23/12/04 10:54:45 INFO DAGScheduler: Job 16 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.015707 s
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.25.86.80:40101 (size: 318.0 B, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 23 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 10:54:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 10:54:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.25.86.80:40101 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 24 from show at Ast.scala:112
23/12/04 10:54:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 10:54:45 INFO SparkContext: Starting job: show at Ast.scala:112
23/12/04 10:54:45 INFO DAGScheduler: Got job 17 (show at Ast.scala:112) with 1 output partitions
23/12/04 10:54:45 INFO DAGScheduler: Final stage: ResultStage 20 (show at Ast.scala:112)
23/12/04 10:54:45 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:45 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:45 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[57] at show at Ast.scala:112), which has no missing parents
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 19.6 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.25.86.80:40101 (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[57] at show at Ast.scala:112) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:45 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
23/12/04 10:54:45 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 17) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 10:54:45 INFO Executor: Running task 0.0 in stage 20.0 (TID 17)
23/12/04 10:54:45 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 10:54:45 INFO Executor: Finished task 0.0 in stage 20.0 (TID 17). 2118 bytes result sent to driver
23/12/04 10:54:45 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 17) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:45 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
23/12/04 10:54:45 INFO DAGScheduler: ResultStage 20 (show at Ast.scala:112) finished in 0.019 s
23/12/04 10:54:45 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
23/12/04 10:54:45 INFO DAGScheduler: Job 17 finished: show at Ast.scala:112, took 0.021043 s
23/12/04 10:54:45 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 10:54:45 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.25.86.80:40101 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 26 from show at Ast.scala:113
23/12/04 10:54:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 10:54:45 INFO SparkContext: Starting job: show at Ast.scala:113
23/12/04 10:54:45 INFO DAGScheduler: Got job 18 (show at Ast.scala:113) with 1 output partitions
23/12/04 10:54:45 INFO DAGScheduler: Final stage: ResultStage 21 (show at Ast.scala:113)
23/12/04 10:54:45 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:45 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:45 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[61] at show at Ast.scala:113), which has no missing parents
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.25.86.80:40101 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[61] at show at Ast.scala:113) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:45 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
23/12/04 10:54:45 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 18) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 10:54:45 INFO Executor: Running task 0.0 in stage 21.0 (TID 18)
23/12/04 10:54:45 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 10:54:45 INFO Executor: Finished task 0.0 in stage 21.0 (TID 18). 1928 bytes result sent to driver
23/12/04 10:54:45 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 18) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:45 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
23/12/04 10:54:45 INFO DAGScheduler: ResultStage 21 (show at Ast.scala:113) finished in 0.014 s
23/12/04 10:54:45 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
23/12/04 10:54:45 INFO DAGScheduler: Job 18 finished: show at Ast.scala:113, took 0.015986 s
23/12/04 10:54:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 10:54:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 10:54:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 10:54:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 10:54:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 10:54:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#55)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.25.86.80:40101 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 28 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 10:54:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 10:54:45 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 10:54:45 INFO DAGScheduler: Got job 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 10:54:45 INFO DAGScheduler: Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 10:54:45 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:45 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:45 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[65] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.25.86.80:40101 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[65] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:45 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
23/12/04 10:54:45 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 19) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 10:54:45 INFO Executor: Running task 0.0 in stage 22.0 (TID 19)
23/12/04 10:54:45 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 10:54:45 INFO Executor: Finished task 0.0 in stage 22.0 (TID 19). 2007 bytes result sent to driver
23/12/04 10:54:45 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 19) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:45 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
23/12/04 10:54:45 INFO DAGScheduler: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.014 s
23/12/04 10:54:45 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
23/12/04 10:54:45 INFO DAGScheduler: Job 19 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.016355 s
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.25.86.80:40101 (size: 318.0 B, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 30 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 10:54:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 10:54:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 11.655144 ms
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.25.86.80:40101 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 31 from show at Ast.scala:126
23/12/04 10:54:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 10:54:45 INFO SparkContext: Starting job: show at Ast.scala:126
23/12/04 10:54:45 INFO DAGScheduler: Got job 20 (show at Ast.scala:126) with 1 output partitions
23/12/04 10:54:45 INFO DAGScheduler: Final stage: ResultStage 23 (show at Ast.scala:126)
23/12/04 10:54:45 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:45 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:45 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[69] at show at Ast.scala:126), which has no missing parents
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 22.4 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.25.86.80:40101 (size: 8.6 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[69] at show at Ast.scala:126) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:45 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
23/12/04 10:54:45 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 20) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 10:54:45 INFO Executor: Running task 0.0 in stage 23.0 (TID 20)
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 11.677449 ms
23/12/04 10:54:45 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 10:54:45 INFO Executor: Finished task 0.0 in stage 23.0 (TID 20). 2172 bytes result sent to driver
23/12/04 10:54:45 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 20) in 21 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:45 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
23/12/04 10:54:45 INFO DAGScheduler: ResultStage 23 (show at Ast.scala:126) finished in 0.026 s
23/12/04 10:54:45 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
23/12/04 10:54:45 INFO DAGScheduler: Job 20 finished: show at Ast.scala:126, took 0.028170 s
23/12/04 10:54:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 10:54:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 10:54:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 10:54:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 10:54:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 10:54:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#55)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.25.86.80:40101 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 33 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 10:54:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 10:54:45 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 10:54:45 INFO DAGScheduler: Got job 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 10:54:45 INFO DAGScheduler: Final stage: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 10:54:45 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:45 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:45 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[73] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.25.86.80:40101 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[73] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:45 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
23/12/04 10:54:45 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 21) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 10:54:45 INFO Executor: Running task 0.0 in stage 24.0 (TID 21)
23/12/04 10:54:45 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 10:54:45 INFO Executor: Finished task 0.0 in stage 24.0 (TID 21). 2007 bytes result sent to driver
23/12/04 10:54:45 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 21) in 8 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:45 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
23/12/04 10:54:45 INFO DAGScheduler: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.012 s
23/12/04 10:54:45 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
23/12/04 10:54:45 INFO DAGScheduler: Job 21 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.013447 s
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.25.86.80:40101 (size: 318.0 B, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 35 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 10:54:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 10:54:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 9.941665 ms
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.25.86.80:40101 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 36 from collect at Ast.scala:253
23/12/04 10:54:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 3.576555 ms
23/12/04 10:54:45 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 10:54:45 INFO DAGScheduler: Got job 22 (collect at Ast.scala:253) with 1 output partitions
23/12/04 10:54:45 INFO DAGScheduler: Final stage: ResultStage 25 (collect at Ast.scala:253)
23/12/04 10:54:45 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:45 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:45 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[79] at collect at Ast.scala:253), which has no missing parents
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 25.1 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.25.86.80:40101 (size: 9.8 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[79] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:45 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
23/12/04 10:54:45 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 22) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 10:54:45 INFO Executor: Running task 0.0 in stage 25.0 (TID 22)
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 8.467256 ms
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 2.903588 ms
23/12/04 10:54:45 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 10:54:45 INFO Executor: Finished task 0.0 in stage 25.0 (TID 22). 2331 bytes result sent to driver
23/12/04 10:54:45 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 22) in 21 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:45 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
23/12/04 10:54:45 INFO DAGScheduler: ResultStage 25 (collect at Ast.scala:253) finished in 0.025 s
23/12/04 10:54:45 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
23/12/04 10:54:45 INFO DAGScheduler: Job 22 finished: collect at Ast.scala:253, took 0.026553 s
23/12/04 10:54:45 INFO DAGScheduler: Registering RDD 80 (collect at Ast.scala:253) as input to shuffle 3
23/12/04 10:54:45 INFO DAGScheduler: Got map stage job 23 (collect at Ast.scala:253) with 1 output partitions
23/12/04 10:54:45 INFO DAGScheduler: Final stage: ShuffleMapStage 26 (collect at Ast.scala:253)
23/12/04 10:54:45 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:45 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:45 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[80] at collect at Ast.scala:253), which has no missing parents
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 27.2 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.25.86.80:40101 (size: 10.9 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[80] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:45 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
23/12/04 10:54:45 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 23) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8468 bytes) 
23/12/04 10:54:45 INFO Executor: Running task 0.0 in stage 26.0 (TID 23)
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 3.674691 ms
23/12/04 10:54:45 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 10:54:45 INFO Executor: Finished task 0.0 in stage 26.0 (TID 23). 2298 bytes result sent to driver
23/12/04 10:54:45 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 23) in 15 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:45 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
23/12/04 10:54:45 INFO DAGScheduler: ShuffleMapStage 26 (collect at Ast.scala:253) finished in 0.020 s
23/12/04 10:54:45 INFO DAGScheduler: looking for newly runnable stages
23/12/04 10:54:45 INFO DAGScheduler: running: HashSet()
23/12/04 10:54:45 INFO DAGScheduler: waiting: HashSet()
23/12/04 10:54:45 INFO DAGScheduler: failed: HashSet()
23/12/04 10:54:45 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 4.708178 ms
23/12/04 10:54:45 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 10:54:45 INFO DAGScheduler: Got job 24 (collect at Ast.scala:253) with 1 output partitions
23/12/04 10:54:45 INFO DAGScheduler: Final stage: ResultStage 28 (collect at Ast.scala:253)
23/12/04 10:54:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
23/12/04 10:54:45 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:45 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[83] at collect at Ast.scala:253), which has no missing parents
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 38.1 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 16.5 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.25.86.80:40101 (size: 16.5 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[83] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:45 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
23/12/04 10:54:45 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 24) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 10:54:45 INFO Executor: Running task 0.0 in stage 28.0 (TID 24)
23/12/04 10:54:45 INFO ShuffleBlockFetcherIterator: Getting 1 (432.0 B) non-empty blocks including 1 (432.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 10:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 5.266729 ms
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 3.922957 ms
23/12/04 10:54:45 INFO Executor: Finished task 0.0 in stage 28.0 (TID 24). 5956 bytes result sent to driver
23/12/04 10:54:45 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 24) in 19 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:45 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
23/12/04 10:54:45 INFO DAGScheduler: ResultStage 28 (collect at Ast.scala:253) finished in 0.024 s
23/12/04 10:54:45 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
23/12/04 10:54:45 INFO DAGScheduler: Job 24 finished: collect at Ast.scala:253, took 0.026228 s
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 4.537219 ms
23/12/04 10:54:45 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
23/12/04 10:54:45 INFO SparkContext: Starting job: parquet at Ast.scala:279
23/12/04 10:54:45 INFO DAGScheduler: Got job 25 (parquet at Ast.scala:279) with 1 output partitions
23/12/04 10:54:45 INFO DAGScheduler: Final stage: ResultStage 29 (parquet at Ast.scala:279)
23/12/04 10:54:45 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:45 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:45 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[85] at parquet at Ast.scala:279), which has no missing parents
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.25.86.80:40101 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[85] at parquet at Ast.scala:279) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:45 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
23/12/04 10:54:45 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 25) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 10:54:45 INFO Executor: Running task 0.0 in stage 29.0 (TID 25)
23/12/04 10:54:45 INFO Executor: Finished task 0.0 in stage 29.0 (TID 25). 2042 bytes result sent to driver
23/12/04 10:54:45 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 25) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:45 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
23/12/04 10:54:45 INFO DAGScheduler: ResultStage 29 (parquet at Ast.scala:279) finished in 0.017 s
23/12/04 10:54:45 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
23/12/04 10:54:45 INFO DAGScheduler: Job 25 finished: parquet at Ast.scala:279, took 0.018277 s
23/12/04 10:54:45 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 10:54:45 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.25.86.80:40101 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 41 from show at IntegrationSuite.scala:176
23/12/04 10:54:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 10:54:45 INFO SparkContext: Starting job: show at IntegrationSuite.scala:176
23/12/04 10:54:45 INFO DAGScheduler: Got job 26 (show at IntegrationSuite.scala:176) with 1 output partitions
23/12/04 10:54:45 INFO DAGScheduler: Final stage: ResultStage 30 (show at IntegrationSuite.scala:176)
23/12/04 10:54:45 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:45 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:45 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[89] at show at IntegrationSuite.scala:176), which has no missing parents
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.25.86.80:40101 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[89] at show at IntegrationSuite.scala:176) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:45 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
23/12/04 10:54:45 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 26) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 10:54:45 INFO Executor: Running task 0.0 in stage 30.0 (TID 26)
23/12/04 10:54:45 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/6ed6590ed5b0d3977c42a362e765ef102e2b3acec91e0482e0ab60862c44369d.parquet/part-00000-26ada9c0-e8ca-473c-bb64-4a3a5c7c5ea8-c000.snappy.parquet, range: 0-1302, partition values: [empty row]
23/12/04 10:54:45 INFO Executor: Finished task 0.0 in stage 30.0 (TID 26). 1961 bytes result sent to driver
23/12/04 10:54:45 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 26) in 6 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:45 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
23/12/04 10:54:45 INFO DAGScheduler: ResultStage 30 (show at IntegrationSuite.scala:176) finished in 0.013 s
23/12/04 10:54:45 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
23/12/04 10:54:45 INFO DAGScheduler: Job 26 finished: show at IntegrationSuite.scala:176, took 0.014576 s
23/12/04 10:54:45 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 10:54:45 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 4.683836 ms
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 200.4 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.25.86.80:40101 (size: 34.8 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 43 from count at IntegrationSuite.scala:178
23/12/04 10:54:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 10:54:45 INFO DAGScheduler: Registering RDD 93 (count at IntegrationSuite.scala:178) as input to shuffle 4
23/12/04 10:54:45 INFO DAGScheduler: Got map stage job 27 (count at IntegrationSuite.scala:178) with 1 output partitions
23/12/04 10:54:45 INFO DAGScheduler: Final stage: ShuffleMapStage 31 (count at IntegrationSuite.scala:178)
23/12/04 10:54:45 INFO DAGScheduler: Parents of final stage: List()
23/12/04 10:54:45 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:45 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[93] at count at IntegrationSuite.scala:178), which has no missing parents
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 17.5 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.25.86.80:40101 (size: 7.9 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[93] at count at IntegrationSuite.scala:178) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:45 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
23/12/04 10:54:45 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 27) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8468 bytes) 
23/12/04 10:54:45 INFO Executor: Running task 0.0 in stage 31.0 (TID 27)
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 5.113609 ms
23/12/04 10:54:45 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/6ed6590ed5b0d3977c42a362e765ef102e2b3acec91e0482e0ab60862c44369d.parquet/part-00000-26ada9c0-e8ca-473c-bb64-4a3a5c7c5ea8-c000.snappy.parquet, range: 0-1302, partition values: [empty row]
23/12/04 10:54:45 INFO Executor: Finished task 0.0 in stage 31.0 (TID 27). 2224 bytes result sent to driver
23/12/04 10:54:45 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 27) in 14 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:45 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
23/12/04 10:54:45 INFO DAGScheduler: ShuffleMapStage 31 (count at IntegrationSuite.scala:178) finished in 0.018 s
23/12/04 10:54:45 INFO DAGScheduler: looking for newly runnable stages
23/12/04 10:54:45 INFO DAGScheduler: running: HashSet()
23/12/04 10:54:45 INFO DAGScheduler: waiting: HashSet()
23/12/04 10:54:45 INFO DAGScheduler: failed: HashSet()
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 3.972392 ms
23/12/04 10:54:45 INFO SparkContext: Starting job: count at IntegrationSuite.scala:178
23/12/04 10:54:45 INFO DAGScheduler: Got job 28 (count at IntegrationSuite.scala:178) with 1 output partitions
23/12/04 10:54:45 INFO DAGScheduler: Final stage: ResultStage 33 (count at IntegrationSuite.scala:178)
23/12/04 10:54:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
23/12/04 10:54:45 INFO DAGScheduler: Missing parents: List()
23/12/04 10:54:45 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[96] at count at IntegrationSuite.scala:178), which has no missing parents
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 13.1 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 9.1 GiB)
23/12/04 10:54:45 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.25.86.80:40101 (size: 6.1 KiB, free: 9.1 GiB)
23/12/04 10:54:45 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1580
23/12/04 10:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[96] at count at IntegrationSuite.scala:178) (first 15 tasks are for partitions Vector(0))
23/12/04 10:54:45 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
23/12/04 10:54:45 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 28) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 10:54:45 INFO Executor: Running task 0.0 in stage 33.0 (TID 28)
23/12/04 10:54:45 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 10:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 10:54:45 INFO CodeGenerator: Code generated in 4.14237 ms
23/12/04 10:54:45 INFO Executor: Finished task 0.0 in stage 33.0 (TID 28). 4084 bytes result sent to driver
23/12/04 10:54:45 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 28) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 10:54:45 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
23/12/04 10:54:45 INFO DAGScheduler: ResultStage 33 (count at IntegrationSuite.scala:178) finished in 0.014 s
23/12/04 10:54:45 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 10:54:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
23/12/04 10:54:45 INFO DAGScheduler: Job 28 finished: count at IntegrationSuite.scala:178, took 0.015607 s
23/12/04 10:54:45 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 10:54:45 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 10:54:45 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 10:54:45 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 10:54:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 10:54:45 INFO MemoryStore: MemoryStore cleared
23/12/04 10:54:45 INFO BlockManager: BlockManager stopped
23/12/04 10:54:45 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 10:54:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 10:54:45 INFO SparkContext: Successfully stopped SparkContext
23/12/04 10:54:45 INFO ShutdownHookManager: Shutdown hook called
23/12/04 10:54:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-8d6eb9ba-5bd4-44aa-890d-021457327ad7
23/12/04 11:10:12 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 11:10:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 11:10:12 INFO SparkContext: Running Spark version 3.5.0
23/12/04 11:10:12 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:10:12 INFO SparkContext: Java version 11.0.20
23/12/04 11:10:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 11:10:12 INFO ResourceUtils: ==============================================================
23/12/04 11:10:12 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 11:10:12 INFO ResourceUtils: ==============================================================
23/12/04 11:10:12 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 11:10:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 11:10:12 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 11:10:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 11:10:12 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 11:10:12 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 11:10:12 INFO SecurityManager: Changing view acls groups to: 
23/12/04 11:10:12 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 11:10:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 11:10:12 INFO Utils: Successfully started service 'sparkDriver' on port 38043.
23/12/04 11:10:12 INFO SparkEnv: Registering MapOutputTracker
23/12/04 11:10:12 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 11:10:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 11:10:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 11:10:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 11:10:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cdc6cc97-46ef-49d0-9e02-5b35bbcbf02d
23/12/04 11:10:12 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 11:10:12 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 11:10:12 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 11:10:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 11:10:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 11:10:13 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 11:10:13 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 11:10:13 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 11:10:13 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 11:10:13 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:10:13 INFO Executor: Java version 11.0.20
23/12/04 11:10:13 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 11:10:13 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@32ae10bb for default.
23/12/04 11:10:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43203.
23/12/04 11:10:13 INFO NettyBlockTransferService: Server created on 10.25.86.80:43203
23/12/04 11:10:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 11:10:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 43203, None)
23/12/04 11:10:13 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:43203 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 43203, None)
23/12/04 11:10:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 43203, None)
23/12/04 11:10:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 43203, None)
23/12/04 11:10:13 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:10:13 INFO xnio: XNIO version 3.8.7.Final
23/12/04 11:10:13 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 11:10:13 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 11:10:13 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:10:13 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:10:13 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:10:13 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:10:13 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:10:13 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:10:13 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:10:13 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:10:13 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:10:13 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:10:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 11:10:13 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 11:10:15 INFO CodeGenerator: Code generated in 143.793252 ms
23/12/04 11:10:15 INFO CodeGenerator: Code generated in 13.807274 ms
23/12/04 11:10:15 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:10:15 INFO DAGScheduler: Got job 0 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:10:15 INFO DAGScheduler: Final stage: ResultStage 0 (collect at Ast.scala:253)
23/12/04 11:10:15 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:10:15 INFO DAGScheduler: Missing parents: List()
23/12/04 11:10:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:10:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:10:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:10:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:43203 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:10:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/04 11:10:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:10:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/04 11:10:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 11:10:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/04 11:10:15 INFO CodeGenerator: Code generated in 6.756287 ms
23/12/04 11:10:15 INFO CodeGenerator: Code generated in 7.185063 ms
23/12/04 11:10:16 INFO CodeGenerator: Code generated in 23.992721 ms
23/12/04 11:10:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1869 bytes result sent to driver
23/12/04 11:10:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 675 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:10:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/04 11:10:16 INFO DAGScheduler: ResultStage 0 (collect at Ast.scala:253) finished in 0.768 s
23/12/04 11:10:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:10:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/04 11:10:16 INFO DAGScheduler: Job 0 finished: collect at Ast.scala:253, took 0.793935 s
23/12/04 11:10:16 INFO DAGScheduler: Registering RDD 8 (collect at Ast.scala:253) as input to shuffle 0
23/12/04 11:10:16 INFO DAGScheduler: Got map stage job 1 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:10:16 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at Ast.scala:253)
23/12/04 11:10:16 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:10:16 INFO DAGScheduler: Missing parents: List()
23/12/04 11:10:16 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.25.86.80:43203 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:10:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/12/04 11:10:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:10:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/12/04 11:10:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 11:10:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/12/04 11:10:16 INFO CodeGenerator: Code generated in 5.297726 ms
23/12/04 11:10:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1836 bytes result sent to driver
23/12/04 11:10:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 51 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:10:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/04 11:10:16 INFO DAGScheduler: ShuffleMapStage 1 (collect at Ast.scala:253) finished in 0.066 s
23/12/04 11:10:16 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:10:16 INFO DAGScheduler: running: HashSet()
23/12/04 11:10:16 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:10:16 INFO DAGScheduler: failed: HashSet()
23/12/04 11:10:16 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:10:16 INFO CodeGenerator: Code generated in 12.413317 ms
23/12/04 11:10:16 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:10:16 INFO DAGScheduler: Got job 2 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:10:16 INFO DAGScheduler: Final stage: ResultStage 3 (collect at Ast.scala:253)
23/12/04 11:10:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
23/12/04 11:10:16 INFO DAGScheduler: Missing parents: List()
23/12/04 11:10:16 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.25.86.80:43203 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:10:16 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
23/12/04 11:10:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:10:16 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
23/12/04 11:10:16 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:10:16 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
23/12/04 11:10:16 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:10:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
23/12/04 11:10:16 INFO CodeGenerator: Code generated in 7.973796 ms
23/12/04 11:10:16 INFO CodeGenerator: Code generated in 5.576124 ms
23/12/04 11:10:16 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4537 bytes result sent to driver
23/12/04 11:10:16 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 88 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:10:16 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
23/12/04 11:10:16 INFO DAGScheduler: ResultStage 3 (collect at Ast.scala:253) finished in 0.099 s
23/12/04 11:10:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:10:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
23/12/04 11:10:16 INFO DAGScheduler: Job 2 finished: collect at Ast.scala:253, took 0.105705 s
23/12/04 11:10:16 INFO CodeGenerator: Code generated in 6.703018 ms
23/12/04 11:10:16 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:10:16 INFO DAGScheduler: Got job 3 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:10:16 INFO DAGScheduler: Final stage: ResultStage 4 (collect at Ast.scala:253)
23/12/04 11:10:16 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:10:16 INFO DAGScheduler: Missing parents: List()
23/12/04 11:10:16 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.25.86.80:43203 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:10:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
23/12/04 11:10:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:10:16 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/12/04 11:10:16 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/04 11:10:16 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
23/12/04 11:10:16 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 1826 bytes result sent to driver
23/12/04 11:10:16 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:10:16 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/12/04 11:10:16 INFO DAGScheduler: ResultStage 4 (collect at Ast.scala:253) finished in 0.020 s
23/12/04 11:10:16 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:10:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/12/04 11:10:16 INFO DAGScheduler: Job 3 finished: collect at Ast.scala:253, took 0.021977 s
23/12/04 11:10:16 INFO DAGScheduler: Registering RDD 16 (collect at Ast.scala:253) as input to shuffle 1
23/12/04 11:10:16 INFO DAGScheduler: Got map stage job 4 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:10:16 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (collect at Ast.scala:253)
23/12/04 11:10:16 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:10:16 INFO DAGScheduler: Missing parents: List()
23/12/04 11:10:16 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.25.86.80:43203 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:10:16 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
23/12/04 11:10:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:10:16 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
23/12/04 11:10:16 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 11:10:16 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
23/12/04 11:10:16 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1793 bytes result sent to driver
23/12/04 11:10:16 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 16 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:10:16 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
23/12/04 11:10:16 INFO DAGScheduler: ShuffleMapStage 5 (collect at Ast.scala:253) finished in 0.024 s
23/12/04 11:10:16 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:10:16 INFO DAGScheduler: running: HashSet()
23/12/04 11:10:16 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:10:16 INFO DAGScheduler: failed: HashSet()
23/12/04 11:10:16 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:10:16 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:10:16 INFO DAGScheduler: Got job 5 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:10:16 INFO DAGScheduler: Final stage: ResultStage 7 (collect at Ast.scala:253)
23/12/04 11:10:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
23/12/04 11:10:16 INFO DAGScheduler: Missing parents: List()
23/12/04 11:10:16 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.25.86.80:43203 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:10:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
23/12/04 11:10:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:10:16 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
23/12/04 11:10:16 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:10:16 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)
23/12/04 11:10:16 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:10:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 11:10:16 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 4494 bytes result sent to driver
23/12/04 11:10:16 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:10:16 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
23/12/04 11:10:16 INFO DAGScheduler: ResultStage 7 (collect at Ast.scala:253) finished in 0.019 s
23/12/04 11:10:16 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:10:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
23/12/04 11:10:16 INFO DAGScheduler: Job 5 finished: collect at Ast.scala:253, took 0.023029 s
23/12/04 11:10:16 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:10:16 INFO DAGScheduler: Got job 6 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:10:16 INFO DAGScheduler: Final stage: ResultStage 8 (collect at Ast.scala:253)
23/12/04 11:10:16 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:10:16 INFO DAGScheduler: Missing parents: List()
23/12/04 11:10:16 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.25.86.80:43203 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:10:16 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
23/12/04 11:10:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:10:16 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
23/12/04 11:10:16 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 11:10:16 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)
23/12/04 11:10:16 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 1826 bytes result sent to driver
23/12/04 11:10:16 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:10:16 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
23/12/04 11:10:16 INFO DAGScheduler: ResultStage 8 (collect at Ast.scala:253) finished in 0.017 s
23/12/04 11:10:16 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:10:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
23/12/04 11:10:16 INFO DAGScheduler: Job 6 finished: collect at Ast.scala:253, took 0.018765 s
23/12/04 11:10:16 INFO DAGScheduler: Registering RDD 24 (collect at Ast.scala:253) as input to shuffle 2
23/12/04 11:10:16 INFO DAGScheduler: Got map stage job 7 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:10:16 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (collect at Ast.scala:253)
23/12/04 11:10:16 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:10:16 INFO DAGScheduler: Missing parents: List()
23/12/04 11:10:16 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.25.86.80:43203 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:10:16 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
23/12/04 11:10:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:10:16 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
23/12/04 11:10:16 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/04 11:10:16 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)
23/12/04 11:10:16 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 1793 bytes result sent to driver
23/12/04 11:10:16 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:10:16 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
23/12/04 11:10:16 INFO DAGScheduler: ShuffleMapStage 9 (collect at Ast.scala:253) finished in 0.018 s
23/12/04 11:10:16 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:10:16 INFO DAGScheduler: running: HashSet()
23/12/04 11:10:16 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:10:16 INFO DAGScheduler: failed: HashSet()
23/12/04 11:10:16 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:10:16 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:10:16 INFO DAGScheduler: Got job 8 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:10:16 INFO DAGScheduler: Final stage: ResultStage 11 (collect at Ast.scala:253)
23/12/04 11:10:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
23/12/04 11:10:16 INFO DAGScheduler: Missing parents: List()
23/12/04 11:10:16 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.25.86.80:43203 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:10:16 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
23/12/04 11:10:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:10:16 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
23/12/04 11:10:16 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:10:16 INFO Executor: Running task 0.0 in stage 11.0 (TID 8)
23/12/04 11:10:16 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:10:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 11:10:16 INFO Executor: Finished task 0.0 in stage 11.0 (TID 8). 4494 bytes result sent to driver
23/12/04 11:10:16 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:10:16 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
23/12/04 11:10:16 INFO DAGScheduler: ResultStage 11 (collect at Ast.scala:253) finished in 0.015 s
23/12/04 11:10:16 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:10:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
23/12/04 11:10:16 INFO DAGScheduler: Job 8 finished: collect at Ast.scala:253, took 0.019161 s
23/12/04 11:10:16 INFO InMemoryFileIndex: It took 16 ms to list leaf files for 1 paths.
23/12/04 11:10:16 INFO SparkContext: Starting job: parquet at Ast.scala:279
23/12/04 11:10:16 INFO DAGScheduler: Got job 9 (parquet at Ast.scala:279) with 1 output partitions
23/12/04 11:10:16 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at Ast.scala:279)
23/12/04 11:10:16 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:10:16 INFO DAGScheduler: Missing parents: List()
23/12/04 11:10:16 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:279), which has no missing parents
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 11:10:16 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.25.86.80:43203 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 11:10:16 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
23/12/04 11:10:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:279) (first 15 tasks are for partitions Vector(0))
23/12/04 11:10:16 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
23/12/04 11:10:16 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 11:10:16 INFO Executor: Running task 0.0 in stage 12.0 (TID 9)
23/12/04 11:11:30 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 11:11:30 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 11:11:30 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 11:11:30 INFO DAGScheduler: Job 9 failed: parquet at Ast.scala:279, took 73.749913 s
23/12/04 11:11:30 INFO DAGScheduler: ResultStage 12 (parquet at Ast.scala:279) failed in 73.748 s due to Stage cancelled because SparkContext was shut down
23/12/04 11:11:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 11:11:30 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:11:30 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:11:30 INFO MemoryStore: MemoryStore cleared
23/12/04 11:11:30 INFO BlockManager: BlockManager stopped
23/12/04 11:11:30 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 11:11:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 11:11:30 INFO SparkContext: Successfully stopped SparkContext
23/12/04 11:11:30 INFO ShutdownHookManager: Shutdown hook called
23/12/04 11:11:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-8e9ef430-7592-4a60-a1b4-a190ef5ae64e
23/12/04 11:11:30 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:19:33 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 11:19:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 11:19:33 INFO SparkContext: Running Spark version 3.5.0
23/12/04 11:19:33 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:19:33 INFO SparkContext: Java version 11.0.20
23/12/04 11:19:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 11:19:34 INFO ResourceUtils: ==============================================================
23/12/04 11:19:34 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 11:19:34 INFO ResourceUtils: ==============================================================
23/12/04 11:19:34 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 11:19:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 11:19:34 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 11:19:34 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 11:19:34 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 11:19:34 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 11:19:34 INFO SecurityManager: Changing view acls groups to: 
23/12/04 11:19:34 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 11:19:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 11:19:34 INFO Utils: Successfully started service 'sparkDriver' on port 33221.
23/12/04 11:19:34 INFO SparkEnv: Registering MapOutputTracker
23/12/04 11:19:34 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 11:19:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 11:19:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 11:19:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 11:19:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-36d44ea8-a355-40fe-a3f0-1192ba5d3b39
23/12/04 11:19:34 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 11:19:34 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 11:19:34 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 11:19:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 11:19:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 11:19:34 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 11:19:34 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 11:19:34 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 11:19:34 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 11:19:34 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:19:34 INFO Executor: Java version 11.0.20
23/12/04 11:19:34 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 11:19:34 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5d6f67e5 for default.
23/12/04 11:19:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37159.
23/12/04 11:19:34 INFO NettyBlockTransferService: Server created on 10.25.86.80:37159
23/12/04 11:19:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 11:19:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 37159, None)
23/12/04 11:19:34 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:37159 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 37159, None)
23/12/04 11:19:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 37159, None)
23/12/04 11:19:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 37159, None)
23/12/04 11:19:34 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:19:34 INFO xnio: XNIO version 3.8.7.Final
23/12/04 11:19:34 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 11:19:34 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 11:19:35 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:19:35 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:19:35 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:19:35 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:19:35 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:19:35 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:19:35 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:19:35 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:19:35 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:19:35 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:19:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 11:19:35 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 11:19:36 INFO CodeGenerator: Code generated in 130.377191 ms
23/12/04 11:19:36 INFO CodeGenerator: Code generated in 13.491861 ms
23/12/04 11:19:36 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:19:36 INFO DAGScheduler: Got job 0 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:19:36 INFO DAGScheduler: Final stage: ResultStage 0 (collect at Ast.scala:253)
23/12/04 11:19:36 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:19:36 INFO DAGScheduler: Missing parents: List()
23/12/04 11:19:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:19:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:19:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:19:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:37159 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:19:36 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/04 11:19:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:19:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/04 11:19:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 11:19:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/04 11:19:37 INFO CodeGenerator: Code generated in 7.989783 ms
23/12/04 11:19:37 INFO CodeGenerator: Code generated in 6.965519 ms
23/12/04 11:19:37 INFO CodeGenerator: Code generated in 26.602361 ms
23/12/04 11:19:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1869 bytes result sent to driver
23/12/04 11:19:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 669 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:19:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/04 11:19:37 INFO DAGScheduler: ResultStage 0 (collect at Ast.scala:253) finished in 0.762 s
23/12/04 11:19:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:19:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/04 11:19:37 INFO DAGScheduler: Job 0 finished: collect at Ast.scala:253, took 0.791836 s
23/12/04 11:19:37 INFO DAGScheduler: Registering RDD 8 (collect at Ast.scala:253) as input to shuffle 0
23/12/04 11:19:37 INFO DAGScheduler: Got map stage job 1 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:19:37 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at Ast.scala:253)
23/12/04 11:19:37 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:19:37 INFO DAGScheduler: Missing parents: List()
23/12/04 11:19:37 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:19:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:19:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:19:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.25.86.80:37159 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:19:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/12/04 11:19:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:19:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/12/04 11:19:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 11:19:37 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/12/04 11:19:37 INFO CodeGenerator: Code generated in 5.61766 ms
23/12/04 11:19:37 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1793 bytes result sent to driver
23/12/04 11:19:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 46 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:19:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/04 11:19:37 INFO DAGScheduler: ShuffleMapStage 1 (collect at Ast.scala:253) finished in 0.061 s
23/12/04 11:19:37 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:19:37 INFO DAGScheduler: running: HashSet()
23/12/04 11:19:37 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:19:37 INFO DAGScheduler: failed: HashSet()
23/12/04 11:19:37 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:19:37 INFO CodeGenerator: Code generated in 11.343412 ms
23/12/04 11:19:37 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:19:37 INFO DAGScheduler: Got job 2 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:19:37 INFO DAGScheduler: Final stage: ResultStage 3 (collect at Ast.scala:253)
23/12/04 11:19:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
23/12/04 11:19:37 INFO DAGScheduler: Missing parents: List()
23/12/04 11:19:37 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:19:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:19:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:19:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.25.86.80:37159 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:19:37 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
23/12/04 11:19:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:19:37 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
23/12/04 11:19:37 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:19:37 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
23/12/04 11:19:37 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
23/12/04 11:19:37 INFO CodeGenerator: Code generated in 8.018051 ms
23/12/04 11:19:37 INFO CodeGenerator: Code generated in 4.312241 ms
23/12/04 11:19:37 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4537 bytes result sent to driver
23/12/04 11:19:37 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 86 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:19:37 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
23/12/04 11:19:37 INFO DAGScheduler: ResultStage 3 (collect at Ast.scala:253) finished in 0.095 s
23/12/04 11:19:37 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:19:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
23/12/04 11:19:37 INFO DAGScheduler: Job 2 finished: collect at Ast.scala:253, took 0.102444 s
23/12/04 11:19:38 INFO CodeGenerator: Code generated in 7.470016 ms
23/12/04 11:19:38 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:19:38 INFO DAGScheduler: Got job 3 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:19:38 INFO DAGScheduler: Final stage: ResultStage 4 (collect at Ast.scala:253)
23/12/04 11:19:38 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:19:38 INFO DAGScheduler: Missing parents: List()
23/12/04 11:19:38 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.25.86.80:37159 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
23/12/04 11:19:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:19:38 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/12/04 11:19:38 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/04 11:19:38 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
23/12/04 11:19:38 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 1826 bytes result sent to driver
23/12/04 11:19:38 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 14 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:19:38 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/12/04 11:19:38 INFO DAGScheduler: ResultStage 4 (collect at Ast.scala:253) finished in 0.020 s
23/12/04 11:19:38 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:19:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/12/04 11:19:38 INFO DAGScheduler: Job 3 finished: collect at Ast.scala:253, took 0.023125 s
23/12/04 11:19:38 INFO DAGScheduler: Registering RDD 16 (collect at Ast.scala:253) as input to shuffle 1
23/12/04 11:19:38 INFO DAGScheduler: Got map stage job 4 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:19:38 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (collect at Ast.scala:253)
23/12/04 11:19:38 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:19:38 INFO DAGScheduler: Missing parents: List()
23/12/04 11:19:38 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.25.86.80:37159 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
23/12/04 11:19:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:19:38 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
23/12/04 11:19:38 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 11:19:38 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
23/12/04 11:19:38 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1793 bytes result sent to driver
23/12/04 11:19:38 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 17 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:19:38 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
23/12/04 11:19:38 INFO DAGScheduler: ShuffleMapStage 5 (collect at Ast.scala:253) finished in 0.027 s
23/12/04 11:19:38 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:19:38 INFO DAGScheduler: running: HashSet()
23/12/04 11:19:38 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:19:38 INFO DAGScheduler: failed: HashSet()
23/12/04 11:19:38 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:19:38 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:19:38 INFO DAGScheduler: Got job 5 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:19:38 INFO DAGScheduler: Final stage: ResultStage 7 (collect at Ast.scala:253)
23/12/04 11:19:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
23/12/04 11:19:38 INFO DAGScheduler: Missing parents: List()
23/12/04 11:19:38 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.25.86.80:37159 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
23/12/04 11:19:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:19:38 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
23/12/04 11:19:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:19:38 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)
23/12/04 11:19:38 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:19:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 11:19:38 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 4494 bytes result sent to driver
23/12/04 11:19:38 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 16 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:19:38 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
23/12/04 11:19:38 INFO DAGScheduler: ResultStage 7 (collect at Ast.scala:253) finished in 0.025 s
23/12/04 11:19:38 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:19:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
23/12/04 11:19:38 INFO DAGScheduler: Job 5 finished: collect at Ast.scala:253, took 0.028364 s
23/12/04 11:19:38 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:19:38 INFO DAGScheduler: Got job 6 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:19:38 INFO DAGScheduler: Final stage: ResultStage 8 (collect at Ast.scala:253)
23/12/04 11:19:38 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:19:38 INFO DAGScheduler: Missing parents: List()
23/12/04 11:19:38 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.25.86.80:37159 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
23/12/04 11:19:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:19:38 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
23/12/04 11:19:38 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 11:19:38 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)
23/12/04 11:19:38 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 1826 bytes result sent to driver
23/12/04 11:19:38 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:19:38 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
23/12/04 11:19:38 INFO DAGScheduler: ResultStage 8 (collect at Ast.scala:253) finished in 0.018 s
23/12/04 11:19:38 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:19:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
23/12/04 11:19:38 INFO DAGScheduler: Job 6 finished: collect at Ast.scala:253, took 0.022877 s
23/12/04 11:19:38 INFO DAGScheduler: Registering RDD 24 (collect at Ast.scala:253) as input to shuffle 2
23/12/04 11:19:38 INFO DAGScheduler: Got map stage job 7 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:19:38 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (collect at Ast.scala:253)
23/12/04 11:19:38 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:19:38 INFO DAGScheduler: Missing parents: List()
23/12/04 11:19:38 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.25.86.80:37159 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
23/12/04 11:19:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:19:38 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
23/12/04 11:19:38 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/04 11:19:38 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)
23/12/04 11:19:38 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 1793 bytes result sent to driver
23/12/04 11:19:38 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 16 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:19:38 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
23/12/04 11:19:38 INFO DAGScheduler: ShuffleMapStage 9 (collect at Ast.scala:253) finished in 0.022 s
23/12/04 11:19:38 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:19:38 INFO DAGScheduler: running: HashSet()
23/12/04 11:19:38 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:19:38 INFO DAGScheduler: failed: HashSet()
23/12/04 11:19:38 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:19:38 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:19:38 INFO DAGScheduler: Got job 8 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:19:38 INFO DAGScheduler: Final stage: ResultStage 11 (collect at Ast.scala:253)
23/12/04 11:19:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
23/12/04 11:19:38 INFO DAGScheduler: Missing parents: List()
23/12/04 11:19:38 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.25.86.80:37159 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
23/12/04 11:19:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:19:38 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
23/12/04 11:19:38 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:19:38 INFO Executor: Running task 0.0 in stage 11.0 (TID 8)
23/12/04 11:19:38 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:19:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 11:19:38 INFO Executor: Finished task 0.0 in stage 11.0 (TID 8). 4494 bytes result sent to driver
23/12/04 11:19:38 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:19:38 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
23/12/04 11:19:38 INFO DAGScheduler: ResultStage 11 (collect at Ast.scala:253) finished in 0.021 s
23/12/04 11:19:38 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:19:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
23/12/04 11:19:38 INFO DAGScheduler: Job 8 finished: collect at Ast.scala:253, took 0.024780 s
23/12/04 11:19:38 INFO InMemoryFileIndex: It took 17 ms to list leaf files for 1 paths.
23/12/04 11:19:38 INFO SparkContext: Starting job: parquet at Ast.scala:279
23/12/04 11:19:38 INFO DAGScheduler: Got job 9 (parquet at Ast.scala:279) with 1 output partitions
23/12/04 11:19:38 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at Ast.scala:279)
23/12/04 11:19:38 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:19:38 INFO DAGScheduler: Missing parents: List()
23/12/04 11:19:38 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:279), which has no missing parents
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.25.86.80:37159 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
23/12/04 11:19:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:279) (first 15 tasks are for partitions Vector(0))
23/12/04 11:19:38 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
23/12/04 11:19:38 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 11:19:38 INFO Executor: Running task 0.0 in stage 12.0 (TID 9)
23/12/04 11:19:38 INFO Executor: Finished task 0.0 in stage 12.0 (TID 9). 2085 bytes result sent to driver
23/12/04 11:19:38 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 220 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:19:38 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
23/12/04 11:19:38 INFO DAGScheduler: ResultStage 12 (parquet at Ast.scala:279) finished in 0.233 s
23/12/04 11:19:38 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:19:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
23/12/04 11:19:38 INFO DAGScheduler: Job 9 finished: parquet at Ast.scala:279, took 0.239890 s
23/12/04 11:19:38 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/12/04 11:19:38 INFO SparkContext: Starting job: parquet at Ast.scala:279
23/12/04 11:19:38 INFO DAGScheduler: Got job 10 (parquet at Ast.scala:279) with 1 output partitions
23/12/04 11:19:38 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at Ast.scala:279)
23/12/04 11:19:38 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:19:38 INFO DAGScheduler: Missing parents: List()
23/12/04 11:19:38 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[31] at parquet at Ast.scala:279), which has no missing parents
23/12/04 11:19:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.25.86.80:37159 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.25.86.80:37159 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.25.86.80:37159 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580
23/12/04 11:19:38 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.25.86.80:37159 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[31] at parquet at Ast.scala:279) (first 15 tasks are for partitions Vector(0))
23/12/04 11:19:38 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
23/12/04 11:19:38 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 11:19:38 INFO Executor: Running task 0.0 in stage 13.0 (TID 10)
23/12/04 11:19:38 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.25.86.80:37159 in memory (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.25.86.80:37159 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO Executor: Finished task 0.0 in stage 13.0 (TID 10). 2042 bytes result sent to driver
23/12/04 11:19:38 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:19:38 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
23/12/04 11:19:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.25.86.80:37159 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO DAGScheduler: ResultStage 13 (parquet at Ast.scala:279) finished in 0.022 s
23/12/04 11:19:38 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:19:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
23/12/04 11:19:38 INFO DAGScheduler: Job 10 finished: parquet at Ast.scala:279, took 0.023166 s
23/12/04 11:19:38 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.25.86.80:37159 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.25.86.80:37159 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.25.86.80:37159 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.25.86.80:37159 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
23/12/04 11:19:38 INFO SparkContext: Starting job: parquet at Ast.scala:279
23/12/04 11:19:38 INFO DAGScheduler: Got job 11 (parquet at Ast.scala:279) with 1 output partitions
23/12/04 11:19:38 INFO DAGScheduler: Final stage: ResultStage 14 (parquet at Ast.scala:279)
23/12/04 11:19:38 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:19:38 INFO DAGScheduler: Missing parents: List()
23/12/04 11:19:38 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[33] at parquet at Ast.scala:279), which has no missing parents
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 11:19:38 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.25.86.80:37159 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 11:19:38 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580
23/12/04 11:19:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[33] at parquet at Ast.scala:279) (first 15 tasks are for partitions Vector(0))
23/12/04 11:19:38 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
23/12/04 11:19:38 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 11:19:38 INFO Executor: Running task 0.0 in stage 14.0 (TID 11)
23/12/04 11:19:38 INFO Executor: Finished task 0.0 in stage 14.0 (TID 11). 2042 bytes result sent to driver
23/12/04 11:19:38 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:19:38 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
23/12/04 11:19:38 INFO DAGScheduler: ResultStage 14 (parquet at Ast.scala:279) finished in 0.023 s
23/12/04 11:19:38 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:19:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
23/12/04 11:19:38 INFO DAGScheduler: Job 11 finished: parquet at Ast.scala:279, took 0.025682 s
23/12/04 11:19:38 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 11:19:38 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 11:19:39 INFO CodeGenerator: Code generated in 17.436359 ms
23/12/04 11:19:39 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 11:19:39 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 11:19:39 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.25.86.80:37159 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:19:39 INFO SparkContext: Created broadcast 12 from show at Ast.scala:95
23/12/04 11:19:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:19:39 INFO SparkContext: Starting job: show at Ast.scala:95
23/12/04 11:19:39 INFO DAGScheduler: Got job 12 (show at Ast.scala:95) with 1 output partitions
23/12/04 11:19:39 INFO DAGScheduler: Final stage: ResultStage 15 (show at Ast.scala:95)
23/12/04 11:19:39 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:19:39 INFO DAGScheduler: Missing parents: List()
23/12/04 11:19:39 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[37] at show at Ast.scala:95), which has no missing parents
23/12/04 11:19:39 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 11:19:39 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 11:19:39 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.25.86.80:37159 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:19:39 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580
23/12/04 11:19:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[37] at show at Ast.scala:95) (first 15 tasks are for partitions Vector(0))
23/12/04 11:19:39 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
23/12/04 11:19:39 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 11:19:39 INFO Executor: Running task 0.0 in stage 15.0 (TID 12)
23/12/04 11:19:39 INFO CodeGenerator: Code generated in 14.476412 ms
23/12/04 11:19:39 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 11:19:39 INFO CodecPool: Got brand-new decompressor [.snappy]
23/12/04 11:19:39 INFO Executor: Finished task 0.0 in stage 15.0 (TID 12). 1928 bytes result sent to driver
23/12/04 11:19:39 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 275 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:19:39 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
23/12/04 11:19:39 INFO DAGScheduler: ResultStage 15 (show at Ast.scala:95) finished in 0.289 s
23/12/04 11:19:39 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:19:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
23/12/04 11:19:39 INFO DAGScheduler: Job 12 finished: show at Ast.scala:95, took 0.291137 s
23/12/04 11:19:39 INFO CodeGenerator: Code generated in 7.647673 ms
23/12/04 11:19:39 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 11:19:39 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 11:19:39 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 11:19:39 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 11:19:39 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.25.86.80:37159 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:19:39 INFO SparkContext: Created broadcast 14 from show at Ast.scala:96
23/12/04 11:19:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:19:39 INFO SparkContext: Starting job: show at Ast.scala:96
23/12/04 11:19:39 INFO DAGScheduler: Got job 13 (show at Ast.scala:96) with 1 output partitions
23/12/04 11:19:39 INFO DAGScheduler: Final stage: ResultStage 16 (show at Ast.scala:96)
23/12/04 11:19:39 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:19:39 INFO DAGScheduler: Missing parents: List()
23/12/04 11:19:39 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[41] at show at Ast.scala:96), which has no missing parents
23/12/04 11:19:39 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 11:19:39 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 11:19:39 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.25.86.80:37159 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:19:39 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580
23/12/04 11:19:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[41] at show at Ast.scala:96) (first 15 tasks are for partitions Vector(0))
23/12/04 11:19:39 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
23/12/04 11:19:39 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 13) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 11:19:39 INFO Executor: Running task 0.0 in stage 16.0 (TID 13)
23/12/04 11:19:39 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 11:19:39 INFO Executor: Finished task 0.0 in stage 16.0 (TID 13). 1928 bytes result sent to driver
23/12/04 11:19:39 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 13) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:19:39 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
23/12/04 11:19:39 INFO DAGScheduler: ResultStage 16 (show at Ast.scala:96) finished in 0.014 s
23/12/04 11:19:39 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:19:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
23/12/04 11:19:39 INFO DAGScheduler: Job 13 finished: show at Ast.scala:96, took 0.016782 s
23/12/04 11:19:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:19:39 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 11:19:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:19:39 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 11:19:39 INFO CodeGenerator: Code generated in 9.436307 ms
23/12/04 11:19:39 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 11:19:39 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 11:19:39 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.25.86.80:37159 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:19:39 INFO SparkContext: Created broadcast 16 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 11:19:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:19:39 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 11:19:39 INFO DAGScheduler: Got job 14 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 11:19:39 INFO DAGScheduler: Final stage: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 11:19:39 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:19:39 INFO DAGScheduler: Missing parents: List()
23/12/04 11:19:39 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[45] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 11:19:39 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 11:19:39 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 11:19:39 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.25.86.80:37159 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:19:39 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580
23/12/04 11:19:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[45] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 11:19:39 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
23/12/04 11:19:39 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 14) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 11:19:39 INFO Executor: Running task 0.0 in stage 17.0 (TID 14)
23/12/04 11:19:39 INFO CodeGenerator: Code generated in 6.077176 ms
23/12/04 11:19:39 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 11:19:39 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 11:19:39 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 11:19:39 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 11:19:39 INFO DAGScheduler: Job 14 failed: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.047792 s
23/12/04 11:19:39 INFO DAGScheduler: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 0.046 s due to Stage cancelled because SparkContext was shut down
23/12/04 11:19:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 11:19:39 INFO MemoryStore: MemoryStore cleared
23/12/04 11:19:39 INFO BlockManager: BlockManager stopped
23/12/04 11:19:39 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 11:19:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 11:19:39 INFO SparkContext: Successfully stopped SparkContext
23/12/04 11:19:39 INFO ShutdownHookManager: Shutdown hook called
23/12/04 11:19:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-451ba4a9-86f2-4d44-92f2-67dbe0b79511
23/12/04 11:19:53 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 11:19:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 11:19:53 INFO SparkContext: Running Spark version 3.5.0
23/12/04 11:19:53 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:19:53 INFO SparkContext: Java version 11.0.20
23/12/04 11:19:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 11:19:53 INFO ResourceUtils: ==============================================================
23/12/04 11:19:53 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 11:19:53 INFO ResourceUtils: ==============================================================
23/12/04 11:19:53 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 11:19:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 11:19:53 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 11:19:53 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 11:19:53 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 11:19:53 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 11:19:53 INFO SecurityManager: Changing view acls groups to: 
23/12/04 11:19:53 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 11:19:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 11:19:54 INFO Utils: Successfully started service 'sparkDriver' on port 43751.
23/12/04 11:19:54 INFO SparkEnv: Registering MapOutputTracker
23/12/04 11:19:54 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 11:19:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 11:19:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 11:19:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 11:19:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cb848ab3-2959-404b-8e37-801f141fec0d
23/12/04 11:19:54 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 11:19:54 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 11:19:54 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 11:19:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 11:19:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 11:19:54 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 11:19:54 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 11:19:54 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 11:19:54 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 11:19:54 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:19:54 INFO Executor: Java version 11.0.20
23/12/04 11:19:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 11:19:54 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@365a67d5 for default.
23/12/04 11:19:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46591.
23/12/04 11:19:54 INFO NettyBlockTransferService: Server created on 10.25.86.80:46591
23/12/04 11:19:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 11:19:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 46591, None)
23/12/04 11:19:54 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:46591 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 46591, None)
23/12/04 11:19:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 46591, None)
23/12/04 11:19:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 46591, None)
23/12/04 11:19:54 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:19:54 INFO xnio: XNIO version 3.8.7.Final
23/12/04 11:19:54 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 11:19:54 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 11:19:54 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:19:54 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:19:54 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:19:54 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:19:54 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:19:54 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:19:54 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:19:54 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:19:54 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:19:54 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:19:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 11:19:55 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 11:19:55 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 11:19:55 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 11:19:55 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 11:19:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 11:19:55 INFO MemoryStore: MemoryStore cleared
23/12/04 11:19:55 INFO BlockManager: BlockManager stopped
23/12/04 11:19:55 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 11:19:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 11:19:55 INFO SparkContext: Successfully stopped SparkContext
23/12/04 11:19:55 INFO ShutdownHookManager: Shutdown hook called
23/12/04 11:19:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-43307235-f827-4e9f-92d7-7207ed4489e0
23/12/04 11:20:16 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 11:20:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 11:20:16 INFO SparkContext: Running Spark version 3.5.0
23/12/04 11:20:16 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:20:16 INFO SparkContext: Java version 11.0.20
23/12/04 11:20:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 11:20:16 INFO ResourceUtils: ==============================================================
23/12/04 11:20:16 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 11:20:16 INFO ResourceUtils: ==============================================================
23/12/04 11:20:16 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 11:20:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 11:20:16 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 11:20:16 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 11:20:16 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 11:20:16 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 11:20:16 INFO SecurityManager: Changing view acls groups to: 
23/12/04 11:20:16 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 11:20:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 11:20:17 INFO Utils: Successfully started service 'sparkDriver' on port 42415.
23/12/04 11:20:17 INFO SparkEnv: Registering MapOutputTracker
23/12/04 11:20:17 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 11:20:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 11:20:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 11:20:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 11:20:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2d200327-8835-4b25-a098-17b961b807b0
23/12/04 11:20:17 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 11:20:17 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 11:20:17 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 11:20:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 11:20:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 11:20:17 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 11:20:17 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 11:20:17 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 11:20:17 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 11:20:17 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:20:17 INFO Executor: Java version 11.0.20
23/12/04 11:20:17 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 11:20:17 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@11f4168c for default.
23/12/04 11:20:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38585.
23/12/04 11:20:17 INFO NettyBlockTransferService: Server created on 10.25.86.80:38585
23/12/04 11:20:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 11:20:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 38585, None)
23/12/04 11:20:17 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:38585 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 38585, None)
23/12/04 11:20:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 38585, None)
23/12/04 11:20:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 38585, None)
23/12/04 11:20:17 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:20:17 INFO xnio: XNIO version 3.8.7.Final
23/12/04 11:20:17 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 11:20:17 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 11:20:17 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:20:17 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:20:17 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:20:17 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:20:17 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:20:17 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:20:17 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:20:17 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:20:17 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:20:17 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:20:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 11:20:17 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 11:20:19 INFO CodeGenerator: Code generated in 142.520813 ms
23/12/04 11:20:19 INFO CodeGenerator: Code generated in 12.366699 ms
23/12/04 11:20:19 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:20:19 INFO DAGScheduler: Got job 0 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:20:19 INFO DAGScheduler: Final stage: ResultStage 0 (collect at Ast.scala:253)
23/12/04 11:20:19 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:19 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:19 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:20:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:20:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:20:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:38585 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:20:19 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/04 11:20:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 11:20:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/04 11:20:19 INFO CodeGenerator: Code generated in 6.809573 ms
23/12/04 11:20:19 INFO CodeGenerator: Code generated in 6.719699 ms
23/12/04 11:20:20 INFO CodeGenerator: Code generated in 23.06392 ms
23/12/04 11:20:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1869 bytes result sent to driver
23/12/04 11:20:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 674 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/04 11:20:20 INFO DAGScheduler: ResultStage 0 (collect at Ast.scala:253) finished in 0.790 s
23/12/04 11:20:20 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/04 11:20:20 INFO DAGScheduler: Job 0 finished: collect at Ast.scala:253, took 0.820202 s
23/12/04 11:20:20 INFO DAGScheduler: Registering RDD 8 (collect at Ast.scala:253) as input to shuffle 0
23/12/04 11:20:20 INFO DAGScheduler: Got map stage job 1 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:20:20 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at Ast.scala:253)
23/12/04 11:20:20 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:20 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:20 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.25.86.80:38585 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:20:20 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/12/04 11:20:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 11:20:20 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/12/04 11:20:20 INFO CodeGenerator: Code generated in 4.983475 ms
23/12/04 11:20:20 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1836 bytes result sent to driver
23/12/04 11:20:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 47 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/04 11:20:20 INFO DAGScheduler: ShuffleMapStage 1 (collect at Ast.scala:253) finished in 0.064 s
23/12/04 11:20:20 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:20:20 INFO DAGScheduler: running: HashSet()
23/12/04 11:20:20 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:20:20 INFO DAGScheduler: failed: HashSet()
23/12/04 11:20:20 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:20:20 INFO CodeGenerator: Code generated in 7.915704 ms
23/12/04 11:20:20 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:20:20 INFO DAGScheduler: Got job 2 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:20:20 INFO DAGScheduler: Final stage: ResultStage 3 (collect at Ast.scala:253)
23/12/04 11:20:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
23/12/04 11:20:20 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:20 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.25.86.80:38585 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:20:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:20 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
23/12/04 11:20:20 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:20:20 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
23/12/04 11:20:20 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:20:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
23/12/04 11:20:20 INFO CodeGenerator: Code generated in 8.276587 ms
23/12/04 11:20:20 INFO CodeGenerator: Code generated in 6.091362 ms
23/12/04 11:20:20 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4537 bytes result sent to driver
23/12/04 11:20:20 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 84 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:20 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
23/12/04 11:20:20 INFO DAGScheduler: ResultStage 3 (collect at Ast.scala:253) finished in 0.094 s
23/12/04 11:20:20 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
23/12/04 11:20:20 INFO DAGScheduler: Job 2 finished: collect at Ast.scala:253, took 0.101987 s
23/12/04 11:20:20 INFO CodeGenerator: Code generated in 6.568898 ms
23/12/04 11:20:20 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:20:20 INFO DAGScheduler: Got job 3 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:20:20 INFO DAGScheduler: Final stage: ResultStage 4 (collect at Ast.scala:253)
23/12/04 11:20:20 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:20 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:20 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.25.86.80:38585 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:20:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:20 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/12/04 11:20:20 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/04 11:20:20 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
23/12/04 11:20:20 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 1826 bytes result sent to driver
23/12/04 11:20:20 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:20 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/12/04 11:20:20 INFO DAGScheduler: ResultStage 4 (collect at Ast.scala:253) finished in 0.019 s
23/12/04 11:20:20 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/12/04 11:20:20 INFO DAGScheduler: Job 3 finished: collect at Ast.scala:253, took 0.022150 s
23/12/04 11:20:20 INFO DAGScheduler: Registering RDD 16 (collect at Ast.scala:253) as input to shuffle 1
23/12/04 11:20:20 INFO DAGScheduler: Got map stage job 4 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:20:20 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (collect at Ast.scala:253)
23/12/04 11:20:20 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:20 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:20 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.25.86.80:38585 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:20:20 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:20 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
23/12/04 11:20:20 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 11:20:20 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
23/12/04 11:20:20 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1793 bytes result sent to driver
23/12/04 11:20:20 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 15 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:20 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
23/12/04 11:20:20 INFO DAGScheduler: ShuffleMapStage 5 (collect at Ast.scala:253) finished in 0.023 s
23/12/04 11:20:20 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:20:20 INFO DAGScheduler: running: HashSet()
23/12/04 11:20:20 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:20:20 INFO DAGScheduler: failed: HashSet()
23/12/04 11:20:20 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:20:20 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:20:20 INFO DAGScheduler: Got job 5 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:20:20 INFO DAGScheduler: Final stage: ResultStage 7 (collect at Ast.scala:253)
23/12/04 11:20:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
23/12/04 11:20:20 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:20 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.25.86.80:38585 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:20:20 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:20 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
23/12/04 11:20:20 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:20:20 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)
23/12/04 11:20:20 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:20:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 11:20:20 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 4494 bytes result sent to driver
23/12/04 11:20:20 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:20 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
23/12/04 11:20:20 INFO DAGScheduler: ResultStage 7 (collect at Ast.scala:253) finished in 0.018 s
23/12/04 11:20:20 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
23/12/04 11:20:20 INFO DAGScheduler: Job 5 finished: collect at Ast.scala:253, took 0.021463 s
23/12/04 11:20:20 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:20:20 INFO DAGScheduler: Got job 6 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:20:20 INFO DAGScheduler: Final stage: ResultStage 8 (collect at Ast.scala:253)
23/12/04 11:20:20 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:20 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:20 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.25.86.80:38585 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:20:20 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:20 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
23/12/04 11:20:20 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 11:20:20 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)
23/12/04 11:20:20 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 1826 bytes result sent to driver
23/12/04 11:20:20 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 33 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:20 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
23/12/04 11:20:20 INFO DAGScheduler: ResultStage 8 (collect at Ast.scala:253) finished in 0.039 s
23/12/04 11:20:20 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
23/12/04 11:20:20 INFO DAGScheduler: Job 6 finished: collect at Ast.scala:253, took 0.041717 s
23/12/04 11:20:20 INFO DAGScheduler: Registering RDD 24 (collect at Ast.scala:253) as input to shuffle 2
23/12/04 11:20:20 INFO DAGScheduler: Got map stage job 7 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:20:20 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (collect at Ast.scala:253)
23/12/04 11:20:20 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:20 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:20 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.25.86.80:38585 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:20:20 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.25.86.80:38585 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:20:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:20 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
23/12/04 11:20:20 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/04 11:20:20 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)
23/12/04 11:20:20 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.25.86.80:38585 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:20:20 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.25.86.80:38585 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:20:20 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.25.86.80:38585 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:20:20 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.25.86.80:38585 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:20:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.25.86.80:38585 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:20:20 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 1793 bytes result sent to driver
23/12/04 11:20:20 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 17 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:20 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
23/12/04 11:20:20 INFO DAGScheduler: ShuffleMapStage 9 (collect at Ast.scala:253) finished in 0.023 s
23/12/04 11:20:20 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:20:20 INFO DAGScheduler: running: HashSet()
23/12/04 11:20:20 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:20:20 INFO DAGScheduler: failed: HashSet()
23/12/04 11:20:20 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:20:20 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:20:20 INFO DAGScheduler: Got job 8 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:20:20 INFO DAGScheduler: Final stage: ResultStage 11 (collect at Ast.scala:253)
23/12/04 11:20:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
23/12/04 11:20:20 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:20 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:20:20 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.25.86.80:38585 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:20:20 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:20 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
23/12/04 11:20:20 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:20:20 INFO Executor: Running task 0.0 in stage 11.0 (TID 8)
23/12/04 11:20:20 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:20:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 11:20:20 INFO Executor: Finished task 0.0 in stage 11.0 (TID 8). 4494 bytes result sent to driver
23/12/04 11:20:20 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:20 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
23/12/04 11:20:20 INFO DAGScheduler: ResultStage 11 (collect at Ast.scala:253) finished in 0.016 s
23/12/04 11:20:20 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
23/12/04 11:20:20 INFO DAGScheduler: Job 8 finished: collect at Ast.scala:253, took 0.019767 s
23/12/04 11:20:20 INFO InMemoryFileIndex: It took 19 ms to list leaf files for 1 paths.
23/12/04 11:20:21 INFO SparkContext: Starting job: parquet at Ast.scala:279
23/12/04 11:20:21 INFO DAGScheduler: Got job 9 (parquet at Ast.scala:279) with 1 output partitions
23/12/04 11:20:21 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at Ast.scala:279)
23/12/04 11:20:21 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:21 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:21 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:279), which has no missing parents
23/12/04 11:20:21 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 11:20:21 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 11:20:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.25.86.80:38585 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 11:20:21 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:279) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:21 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
23/12/04 11:20:21 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 11:20:21 INFO Executor: Running task 0.0 in stage 12.0 (TID 9)
23/12/04 11:20:21 INFO Executor: Finished task 0.0 in stage 12.0 (TID 9). 2085 bytes result sent to driver
23/12/04 11:20:21 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 233 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:21 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
23/12/04 11:20:21 INFO DAGScheduler: ResultStage 12 (parquet at Ast.scala:279) finished in 0.246 s
23/12/04 11:20:21 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
23/12/04 11:20:21 INFO DAGScheduler: Job 9 finished: parquet at Ast.scala:279, took 0.247403 s
23/12/04 11:20:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/12/04 11:20:21 INFO SparkContext: Starting job: parquet at Ast.scala:279
23/12/04 11:20:21 INFO DAGScheduler: Got job 10 (parquet at Ast.scala:279) with 1 output partitions
23/12/04 11:20:21 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at Ast.scala:279)
23/12/04 11:20:21 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:21 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:21 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[31] at parquet at Ast.scala:279), which has no missing parents
23/12/04 11:20:21 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 11:20:21 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 11:20:21 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.25.86.80:38585 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 11:20:21 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[31] at parquet at Ast.scala:279) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:21 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
23/12/04 11:20:21 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 11:20:21 INFO Executor: Running task 0.0 in stage 13.0 (TID 10)
23/12/04 11:20:21 INFO Executor: Finished task 0.0 in stage 13.0 (TID 10). 2042 bytes result sent to driver
23/12/04 11:20:21 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:21 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
23/12/04 11:20:21 INFO DAGScheduler: ResultStage 13 (parquet at Ast.scala:279) finished in 0.021 s
23/12/04 11:20:21 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
23/12/04 11:20:21 INFO DAGScheduler: Job 10 finished: parquet at Ast.scala:279, took 0.023410 s
23/12/04 11:20:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/12/04 11:20:21 INFO SparkContext: Starting job: parquet at Ast.scala:279
23/12/04 11:20:21 INFO DAGScheduler: Got job 11 (parquet at Ast.scala:279) with 1 output partitions
23/12/04 11:20:21 INFO DAGScheduler: Final stage: ResultStage 14 (parquet at Ast.scala:279)
23/12/04 11:20:21 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:21 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:21 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[33] at parquet at Ast.scala:279), which has no missing parents
23/12/04 11:20:21 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 11:20:21 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 11:20:21 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.25.86.80:38585 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 11:20:21 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[33] at parquet at Ast.scala:279) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:21 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
23/12/04 11:20:21 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 11:20:21 INFO Executor: Running task 0.0 in stage 14.0 (TID 11)
23/12/04 11:20:21 INFO Executor: Finished task 0.0 in stage 14.0 (TID 11). 2042 bytes result sent to driver
23/12/04 11:20:21 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 20 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:21 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
23/12/04 11:20:21 INFO DAGScheduler: ResultStage 14 (parquet at Ast.scala:279) finished in 0.029 s
23/12/04 11:20:21 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
23/12/04 11:20:21 INFO DAGScheduler: Job 11 finished: parquet at Ast.scala:279, took 0.032614 s
23/12/04 11:20:21 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 11:20:21 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 11:20:21 INFO CodeGenerator: Code generated in 16.558554 ms
23/12/04 11:20:21 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 11:20:21 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 11:20:21 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.25.86.80:38585 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:21 INFO SparkContext: Created broadcast 12 from show at Ast.scala:95
23/12/04 11:20:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:20:21 INFO SparkContext: Starting job: show at Ast.scala:95
23/12/04 11:20:21 INFO DAGScheduler: Got job 12 (show at Ast.scala:95) with 1 output partitions
23/12/04 11:20:21 INFO DAGScheduler: Final stage: ResultStage 15 (show at Ast.scala:95)
23/12/04 11:20:21 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:21 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:21 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[37] at show at Ast.scala:95), which has no missing parents
23/12/04 11:20:21 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 11:20:21 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 11:20:21 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.25.86.80:38585 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:20:21 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[37] at show at Ast.scala:95) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:21 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
23/12/04 11:20:21 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 11:20:21 INFO Executor: Running task 0.0 in stage 15.0 (TID 12)
23/12/04 11:20:21 INFO CodeGenerator: Code generated in 10.962036 ms
23/12/04 11:20:21 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 11:20:21 INFO CodecPool: Got brand-new decompressor [.snappy]
23/12/04 11:20:21 INFO Executor: Finished task 0.0 in stage 15.0 (TID 12). 1928 bytes result sent to driver
23/12/04 11:20:21 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 218 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:21 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
23/12/04 11:20:21 INFO DAGScheduler: ResultStage 15 (show at Ast.scala:95) finished in 0.232 s
23/12/04 11:20:21 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
23/12/04 11:20:21 INFO DAGScheduler: Job 12 finished: show at Ast.scala:95, took 0.234738 s
23/12/04 11:20:21 INFO CodeGenerator: Code generated in 6.681602 ms
23/12/04 11:20:21 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 11:20:21 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 11:20:21 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 11:20:21 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 11:20:21 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.25.86.80:38585 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:21 INFO SparkContext: Created broadcast 14 from show at Ast.scala:96
23/12/04 11:20:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:20:21 INFO SparkContext: Starting job: show at Ast.scala:96
23/12/04 11:20:21 INFO DAGScheduler: Got job 13 (show at Ast.scala:96) with 1 output partitions
23/12/04 11:20:21 INFO DAGScheduler: Final stage: ResultStage 16 (show at Ast.scala:96)
23/12/04 11:20:21 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:21 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:21 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[41] at show at Ast.scala:96), which has no missing parents
23/12/04 11:20:21 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 11:20:21 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 11:20:21 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.25.86.80:38585 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:20:21 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[41] at show at Ast.scala:96) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:21 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
23/12/04 11:20:21 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 13) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 11:20:21 INFO Executor: Running task 0.0 in stage 16.0 (TID 13)
23/12/04 11:20:21 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 11:20:21 INFO Executor: Finished task 0.0 in stage 16.0 (TID 13). 1928 bytes result sent to driver
23/12/04 11:20:21 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 13) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:21 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
23/12/04 11:20:21 INFO DAGScheduler: ResultStage 16 (show at Ast.scala:96) finished in 0.015 s
23/12/04 11:20:21 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
23/12/04 11:20:21 INFO DAGScheduler: Job 13 finished: show at Ast.scala:96, took 0.018583 s
23/12/04 11:20:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:20:22 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 11:20:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:20:22 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 11:20:22 INFO CodeGenerator: Code generated in 8.268245 ms
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.25.86.80:38585 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 16 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 11:20:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:20:22 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 11:20:22 INFO DAGScheduler: Got job 14 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 11:20:22 INFO DAGScheduler: Final stage: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 11:20:22 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:22 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:22 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[45] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.25.86.80:38585 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[45] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:22 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
23/12/04 11:20:22 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 14) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 11:20:22 INFO Executor: Running task 0.0 in stage 17.0 (TID 14)
23/12/04 11:20:22 INFO CodeGenerator: Code generated in 6.285057 ms
23/12/04 11:20:22 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 11:20:22 INFO Executor: Finished task 0.0 in stage 17.0 (TID 14). 2007 bytes result sent to driver
23/12/04 11:20:22 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 14) in 58 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:22 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
23/12/04 11:20:22 INFO DAGScheduler: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.062 s
23/12/04 11:20:22 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
23/12/04 11:20:22 INFO DAGScheduler: Job 14 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.064306 s
23/12/04 11:20:22 INFO CodeGenerator: Code generated in 3.624683 ms
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.25.86.80:38585 (size: 318.0 B, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 18 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 11:20:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:20:22 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 11:20:22 INFO CodeGenerator: Code generated in 14.070955 ms
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.25.86.80:38585 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 19 from show at Ast.scala:108
23/12/04 11:20:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:20:22 INFO SparkContext: Starting job: show at Ast.scala:108
23/12/04 11:20:22 INFO DAGScheduler: Got job 15 (show at Ast.scala:108) with 1 output partitions
23/12/04 11:20:22 INFO DAGScheduler: Final stage: ResultStage 18 (show at Ast.scala:108)
23/12/04 11:20:22 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:22 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:22 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[49] at show at Ast.scala:108), which has no missing parents
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 19.6 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.25.86.80:38585 (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[49] at show at Ast.scala:108) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:22 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
23/12/04 11:20:22 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 15) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 11:20:22 INFO Executor: Running task 0.0 in stage 18.0 (TID 15)
23/12/04 11:20:22 INFO CodeGenerator: Code generated in 10.293083 ms
23/12/04 11:20:22 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 11:20:22 INFO Executor: Finished task 0.0 in stage 18.0 (TID 15). 2118 bytes result sent to driver
23/12/04 11:20:22 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 15) in 22 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:22 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
23/12/04 11:20:22 INFO DAGScheduler: ResultStage 18 (show at Ast.scala:108) finished in 0.026 s
23/12/04 11:20:22 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
23/12/04 11:20:22 INFO DAGScheduler: Job 15 finished: show at Ast.scala:108, took 0.028073 s
23/12/04 11:20:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:20:22 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 11:20:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:20:22 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.25.86.80:38585 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 21 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 11:20:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:20:22 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 11:20:22 INFO DAGScheduler: Got job 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 11:20:22 INFO DAGScheduler: Final stage: ResultStage 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 11:20:22 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:22 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:22 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[53] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.25.86.80:38585 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[53] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:22 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
23/12/04 11:20:22 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 16) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 11:20:22 INFO Executor: Running task 0.0 in stage 19.0 (TID 16)
23/12/04 11:20:22 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 11:20:22 INFO Executor: Finished task 0.0 in stage 19.0 (TID 16). 2007 bytes result sent to driver
23/12/04 11:20:22 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 16) in 8 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:22 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
23/12/04 11:20:22 INFO DAGScheduler: ResultStage 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.014 s
23/12/04 11:20:22 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
23/12/04 11:20:22 INFO DAGScheduler: Job 16 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.014814 s
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.25.86.80:38585 (size: 318.0 B, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 23 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 11:20:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:20:22 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.25.86.80:38585 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 24 from show at Ast.scala:112
23/12/04 11:20:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:20:22 INFO SparkContext: Starting job: show at Ast.scala:112
23/12/04 11:20:22 INFO DAGScheduler: Got job 17 (show at Ast.scala:112) with 1 output partitions
23/12/04 11:20:22 INFO DAGScheduler: Final stage: ResultStage 20 (show at Ast.scala:112)
23/12/04 11:20:22 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:22 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:22 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[57] at show at Ast.scala:112), which has no missing parents
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 19.6 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.25.86.80:38585 (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[57] at show at Ast.scala:112) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:22 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
23/12/04 11:20:22 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 17) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 11:20:22 INFO Executor: Running task 0.0 in stage 20.0 (TID 17)
23/12/04 11:20:22 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 11:20:22 INFO Executor: Finished task 0.0 in stage 20.0 (TID 17). 2118 bytes result sent to driver
23/12/04 11:20:22 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 17) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:22 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
23/12/04 11:20:22 INFO DAGScheduler: ResultStage 20 (show at Ast.scala:112) finished in 0.014 s
23/12/04 11:20:22 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
23/12/04 11:20:22 INFO DAGScheduler: Job 17 finished: show at Ast.scala:112, took 0.015936 s
23/12/04 11:20:22 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 11:20:22 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.25.86.80:38585 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 26 from show at Ast.scala:113
23/12/04 11:20:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:20:22 INFO SparkContext: Starting job: show at Ast.scala:113
23/12/04 11:20:22 INFO DAGScheduler: Got job 18 (show at Ast.scala:113) with 1 output partitions
23/12/04 11:20:22 INFO DAGScheduler: Final stage: ResultStage 21 (show at Ast.scala:113)
23/12/04 11:20:22 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:22 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:22 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[61] at show at Ast.scala:113), which has no missing parents
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.25.86.80:38585 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[61] at show at Ast.scala:113) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:22 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
23/12/04 11:20:22 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 18) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 11:20:22 INFO Executor: Running task 0.0 in stage 21.0 (TID 18)
23/12/04 11:20:22 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 11:20:22 INFO Executor: Finished task 0.0 in stage 21.0 (TID 18). 1928 bytes result sent to driver
23/12/04 11:20:22 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 18) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:22 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
23/12/04 11:20:22 INFO DAGScheduler: ResultStage 21 (show at Ast.scala:113) finished in 0.019 s
23/12/04 11:20:22 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
23/12/04 11:20:22 INFO DAGScheduler: Job 18 finished: show at Ast.scala:113, took 0.021908 s
23/12/04 11:20:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:20:22 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 11:20:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:20:22 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 11:20:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:20:22 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#55)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.25.86.80:38585 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 28 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 11:20:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:20:22 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 11:20:22 INFO DAGScheduler: Got job 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 11:20:22 INFO DAGScheduler: Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 11:20:22 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:22 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:22 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[65] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.25.86.80:38585 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[65] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:22 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
23/12/04 11:20:22 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 19) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 11:20:22 INFO Executor: Running task 0.0 in stage 22.0 (TID 19)
23/12/04 11:20:22 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 11:20:22 INFO Executor: Finished task 0.0 in stage 22.0 (TID 19). 2007 bytes result sent to driver
23/12/04 11:20:22 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 19) in 15 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:22 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
23/12/04 11:20:22 INFO DAGScheduler: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.020 s
23/12/04 11:20:22 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
23/12/04 11:20:22 INFO DAGScheduler: Job 19 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.021535 s
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.25.86.80:38585 (size: 318.0 B, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 30 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 11:20:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:20:22 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 11:20:22 INFO CodeGenerator: Code generated in 10.464593 ms
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.25.86.80:38585 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 31 from show at Ast.scala:126
23/12/04 11:20:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:20:22 INFO SparkContext: Starting job: show at Ast.scala:126
23/12/04 11:20:22 INFO DAGScheduler: Got job 20 (show at Ast.scala:126) with 1 output partitions
23/12/04 11:20:22 INFO DAGScheduler: Final stage: ResultStage 23 (show at Ast.scala:126)
23/12/04 11:20:22 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:22 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:22 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[69] at show at Ast.scala:126), which has no missing parents
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 22.4 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.25.86.80:38585 (size: 8.6 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[69] at show at Ast.scala:126) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:22 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
23/12/04 11:20:22 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 20) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 11:20:22 INFO Executor: Running task 0.0 in stage 23.0 (TID 20)
23/12/04 11:20:22 INFO CodeGenerator: Code generated in 11.915805 ms
23/12/04 11:20:22 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 11:20:22 INFO Executor: Finished task 0.0 in stage 23.0 (TID 20). 2172 bytes result sent to driver
23/12/04 11:20:22 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 20) in 25 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:22 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
23/12/04 11:20:22 INFO DAGScheduler: ResultStage 23 (show at Ast.scala:126) finished in 0.029 s
23/12/04 11:20:22 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
23/12/04 11:20:22 INFO DAGScheduler: Job 20 finished: show at Ast.scala:126, took 0.030527 s
23/12/04 11:20:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:20:22 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 11:20:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:20:22 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 11:20:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:20:22 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#55)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.25.86.80:38585 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 33 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 11:20:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:20:22 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 11:20:22 INFO DAGScheduler: Got job 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 11:20:22 INFO DAGScheduler: Final stage: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 11:20:22 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:22 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:22 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[73] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.25.86.80:38585 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[73] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:22 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
23/12/04 11:20:22 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 21) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 11:20:22 INFO Executor: Running task 0.0 in stage 24.0 (TID 21)
23/12/04 11:20:22 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 11:20:22 INFO Executor: Finished task 0.0 in stage 24.0 (TID 21). 2007 bytes result sent to driver
23/12/04 11:20:22 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 21) in 15 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:22 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
23/12/04 11:20:22 INFO DAGScheduler: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.022 s
23/12/04 11:20:22 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
23/12/04 11:20:22 INFO DAGScheduler: Job 21 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.024579 s
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.25.86.80:38585 (size: 318.0 B, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 35 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 11:20:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 11:20:22 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 11:20:22 INFO CodeGenerator: Code generated in 12.001996 ms
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.25.86.80:38585 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 36 from collect at Ast.scala:253
23/12/04 11:20:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:20:22 INFO CodeGenerator: Code generated in 6.610853 ms
23/12/04 11:20:22 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:20:22 INFO DAGScheduler: Got job 22 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:20:22 INFO DAGScheduler: Final stage: ResultStage 25 (collect at Ast.scala:253)
23/12/04 11:20:22 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:22 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:22 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[79] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 25.1 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.25.86.80:38585 (size: 9.8 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[79] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:22 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
23/12/04 11:20:22 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 22) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 11:20:22 INFO Executor: Running task 0.0 in stage 25.0 (TID 22)
23/12/04 11:20:22 INFO CodeGenerator: Code generated in 10.788861 ms
23/12/04 11:20:22 INFO CodeGenerator: Code generated in 3.41023 ms
23/12/04 11:20:22 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 11:20:22 INFO Executor: Finished task 0.0 in stage 25.0 (TID 22). 2331 bytes result sent to driver
23/12/04 11:20:22 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 22) in 25 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:22 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
23/12/04 11:20:22 INFO DAGScheduler: ResultStage 25 (collect at Ast.scala:253) finished in 0.029 s
23/12/04 11:20:22 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
23/12/04 11:20:22 INFO DAGScheduler: Job 22 finished: collect at Ast.scala:253, took 0.030574 s
23/12/04 11:20:22 INFO DAGScheduler: Registering RDD 80 (collect at Ast.scala:253) as input to shuffle 3
23/12/04 11:20:22 INFO DAGScheduler: Got map stage job 23 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:20:22 INFO DAGScheduler: Final stage: ShuffleMapStage 26 (collect at Ast.scala:253)
23/12/04 11:20:22 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:22 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:22 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[80] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 27.2 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.25.86.80:38585 (size: 10.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[80] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:22 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
23/12/04 11:20:22 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 23) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8468 bytes) 
23/12/04 11:20:22 INFO Executor: Running task 0.0 in stage 26.0 (TID 23)
23/12/04 11:20:22 INFO CodeGenerator: Code generated in 3.974649 ms
23/12/04 11:20:22 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 11:20:22 INFO Executor: Finished task 0.0 in stage 26.0 (TID 23). 2298 bytes result sent to driver
23/12/04 11:20:22 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 23) in 37 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:22 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
23/12/04 11:20:22 INFO DAGScheduler: ShuffleMapStage 26 (collect at Ast.scala:253) finished in 0.044 s
23/12/04 11:20:22 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:20:22 INFO DAGScheduler: running: HashSet()
23/12/04 11:20:22 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:20:22 INFO DAGScheduler: failed: HashSet()
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 10.25.86.80:38585 in memory (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 10.25.86.80:38585 in memory (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 10.25.86.80:38585 in memory (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 10.25.86.80:38585 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 10.25.86.80:38585 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 10.25.86.80:38585 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 10.25.86.80:38585 in memory (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.25.86.80:38585 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 10.25.86.80:38585 in memory (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO CodeGenerator: Code generated in 9.214185 ms
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 10.25.86.80:38585 in memory (size: 9.8 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 10.25.86.80:38585 in memory (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 10.25.86.80:38585 in memory (size: 8.6 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 10.25.86.80:38585 in memory (size: 318.0 B, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 10.25.86.80:38585 in memory (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 10.25.86.80:38585 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 10.25.86.80:38585 in memory (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.25.86.80:38585 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 10.25.86.80:38585 in memory (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 10.25.86.80:38585 in memory (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.25.86.80:38585 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 10.25.86.80:38585 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 10.25.86.80:38585 in memory (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 10.25.86.80:38585 in memory (size: 318.0 B, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 10.25.86.80:38585 in memory (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 10.25.86.80:38585 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 10.25.86.80:38585 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.25.86.80:38585 in memory (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 10.25.86.80:38585 in memory (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 10.25.86.80:38585 in memory (size: 318.0 B, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:20:22 INFO DAGScheduler: Got job 24 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:20:22 INFO DAGScheduler: Final stage: ResultStage 28 (collect at Ast.scala:253)
23/12/04 11:20:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
23/12/04 11:20:22 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:22 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[83] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 38.1 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 9.1 GiB)
23/12/04 11:20:22 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.25.86.80:38585 (size: 16.4 KiB, free: 9.1 GiB)
23/12/04 11:20:22 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[83] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:22 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
23/12/04 11:20:22 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 24) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:20:22 INFO Executor: Running task 0.0 in stage 28.0 (TID 24)
23/12/04 11:20:22 INFO ShuffleBlockFetcherIterator: Getting 1 (432.0 B) non-empty blocks including 1 (432.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:20:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 11:20:23 INFO CodeGenerator: Code generated in 6.671115 ms
23/12/04 11:20:23 INFO CodeGenerator: Code generated in 3.404963 ms
23/12/04 11:20:23 INFO Executor: Finished task 0.0 in stage 28.0 (TID 24). 5956 bytes result sent to driver
23/12/04 11:20:23 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 24) in 20 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:23 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
23/12/04 11:20:23 INFO DAGScheduler: ResultStage 28 (collect at Ast.scala:253) finished in 0.023 s
23/12/04 11:20:23 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
23/12/04 11:20:23 INFO DAGScheduler: Job 24 finished: collect at Ast.scala:253, took 0.024870 s
23/12/04 11:20:23 INFO CodeGenerator: Code generated in 10.721841 ms
23/12/04 11:20:23 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/12/04 11:20:23 INFO SparkContext: Starting job: parquet at Ast.scala:279
23/12/04 11:20:23 INFO DAGScheduler: Got job 25 (parquet at Ast.scala:279) with 1 output partitions
23/12/04 11:20:23 INFO DAGScheduler: Final stage: ResultStage 29 (parquet at Ast.scala:279)
23/12/04 11:20:23 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:23 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:23 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[85] at parquet at Ast.scala:279), which has no missing parents
23/12/04 11:20:23 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 11:20:23 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 11:20:23 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.25.86.80:38585 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 11:20:23 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[85] at parquet at Ast.scala:279) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:23 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
23/12/04 11:20:23 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 25) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 11:20:23 INFO Executor: Running task 0.0 in stage 29.0 (TID 25)
23/12/04 11:20:23 INFO Executor: Finished task 0.0 in stage 29.0 (TID 25). 2042 bytes result sent to driver
23/12/04 11:20:23 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 25) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:23 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
23/12/04 11:20:23 INFO DAGScheduler: ResultStage 29 (parquet at Ast.scala:279) finished in 0.021 s
23/12/04 11:20:23 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
23/12/04 11:20:23 INFO DAGScheduler: Job 25 finished: parquet at Ast.scala:279, took 0.023050 s
23/12/04 11:20:23 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 11:20:23 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 11:20:23 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 11:20:23 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 9.1 GiB)
23/12/04 11:20:23 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.25.86.80:38585 (size: 34.9 KiB, free: 9.1 GiB)
23/12/04 11:20:23 INFO SparkContext: Created broadcast 41 from show at IntegrationSuite.scala:176
23/12/04 11:20:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:20:23 INFO SparkContext: Starting job: show at IntegrationSuite.scala:176
23/12/04 11:20:23 INFO DAGScheduler: Got job 26 (show at IntegrationSuite.scala:176) with 1 output partitions
23/12/04 11:20:23 INFO DAGScheduler: Final stage: ResultStage 30 (show at IntegrationSuite.scala:176)
23/12/04 11:20:23 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:23 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:23 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[89] at show at IntegrationSuite.scala:176), which has no missing parents
23/12/04 11:20:23 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 11:20:23 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 11:20:23 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.25.86.80:38585 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 11:20:23 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[89] at show at IntegrationSuite.scala:176) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:23 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
23/12/04 11:20:23 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 26) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 11:20:23 INFO Executor: Running task 0.0 in stage 30.0 (TID 26)
23/12/04 11:20:23 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/6ed6590ed5b0d3977c42a362e765ef102e2b3acec91e0482e0ab60862c44369d.parquet/part-00000-26ada9c0-e8ca-473c-bb64-4a3a5c7c5ea8-c000.snappy.parquet, range: 0-1302, partition values: [empty row]
23/12/04 11:20:23 INFO Executor: Finished task 0.0 in stage 30.0 (TID 26). 1961 bytes result sent to driver
23/12/04 11:20:23 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 26) in 7 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:23 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
23/12/04 11:20:23 INFO DAGScheduler: ResultStage 30 (show at IntegrationSuite.scala:176) finished in 0.011 s
23/12/04 11:20:23 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
23/12/04 11:20:23 INFO DAGScheduler: Job 26 finished: show at IntegrationSuite.scala:176, took 0.012561 s
23/12/04 11:20:23 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 11:20:23 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 11:20:23 INFO CodeGenerator: Code generated in 5.818276 ms
23/12/04 11:20:23 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 200.4 KiB, free 9.1 GiB)
23/12/04 11:20:23 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 9.1 GiB)
23/12/04 11:20:23 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.25.86.80:38585 (size: 34.8 KiB, free: 9.1 GiB)
23/12/04 11:20:23 INFO SparkContext: Created broadcast 43 from count at IntegrationSuite.scala:178
23/12/04 11:20:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 11:20:23 INFO DAGScheduler: Registering RDD 93 (count at IntegrationSuite.scala:178) as input to shuffle 4
23/12/04 11:20:23 INFO DAGScheduler: Got map stage job 27 (count at IntegrationSuite.scala:178) with 1 output partitions
23/12/04 11:20:23 INFO DAGScheduler: Final stage: ShuffleMapStage 31 (count at IntegrationSuite.scala:178)
23/12/04 11:20:23 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:20:23 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:23 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[93] at count at IntegrationSuite.scala:178), which has no missing parents
23/12/04 11:20:23 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 17.5 KiB, free 9.1 GiB)
23/12/04 11:20:23 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 9.1 GiB)
23/12/04 11:20:23 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.25.86.80:38585 (size: 7.9 KiB, free: 9.1 GiB)
23/12/04 11:20:23 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[93] at count at IntegrationSuite.scala:178) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:23 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
23/12/04 11:20:23 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 27) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8468 bytes) 
23/12/04 11:20:23 INFO Executor: Running task 0.0 in stage 31.0 (TID 27)
23/12/04 11:20:23 INFO CodeGenerator: Code generated in 6.571378 ms
23/12/04 11:20:23 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/6ed6590ed5b0d3977c42a362e765ef102e2b3acec91e0482e0ab60862c44369d.parquet/part-00000-26ada9c0-e8ca-473c-bb64-4a3a5c7c5ea8-c000.snappy.parquet, range: 0-1302, partition values: [empty row]
23/12/04 11:20:23 INFO Executor: Finished task 0.0 in stage 31.0 (TID 27). 2224 bytes result sent to driver
23/12/04 11:20:23 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 27) in 15 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:23 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
23/12/04 11:20:23 INFO DAGScheduler: ShuffleMapStage 31 (count at IntegrationSuite.scala:178) finished in 0.019 s
23/12/04 11:20:23 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:20:23 INFO DAGScheduler: running: HashSet()
23/12/04 11:20:23 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:20:23 INFO DAGScheduler: failed: HashSet()
23/12/04 11:20:23 INFO CodeGenerator: Code generated in 4.46844 ms
23/12/04 11:20:23 INFO SparkContext: Starting job: count at IntegrationSuite.scala:178
23/12/04 11:20:23 INFO DAGScheduler: Got job 28 (count at IntegrationSuite.scala:178) with 1 output partitions
23/12/04 11:20:23 INFO DAGScheduler: Final stage: ResultStage 33 (count at IntegrationSuite.scala:178)
23/12/04 11:20:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
23/12/04 11:20:23 INFO DAGScheduler: Missing parents: List()
23/12/04 11:20:23 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[96] at count at IntegrationSuite.scala:178), which has no missing parents
23/12/04 11:20:23 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 13.1 KiB, free 9.1 GiB)
23/12/04 11:20:23 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 9.1 GiB)
23/12/04 11:20:23 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.25.86.80:38585 (size: 6.1 KiB, free: 9.1 GiB)
23/12/04 11:20:23 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1580
23/12/04 11:20:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[96] at count at IntegrationSuite.scala:178) (first 15 tasks are for partitions Vector(0))
23/12/04 11:20:23 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
23/12/04 11:20:23 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 28) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:20:23 INFO Executor: Running task 0.0 in stage 33.0 (TID 28)
23/12/04 11:20:23 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:20:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 11:20:23 INFO CodeGenerator: Code generated in 5.73769 ms
23/12/04 11:20:23 INFO Executor: Finished task 0.0 in stage 33.0 (TID 28). 4084 bytes result sent to driver
23/12/04 11:20:23 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 28) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:20:23 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
23/12/04 11:20:23 INFO DAGScheduler: ResultStage 33 (count at IntegrationSuite.scala:178) finished in 0.015 s
23/12/04 11:20:23 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:20:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
23/12/04 11:20:23 INFO DAGScheduler: Job 28 finished: count at IntegrationSuite.scala:178, took 0.017298 s
23/12/04 11:20:23 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:20:23 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 11:20:23 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 11:20:23 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 11:20:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 11:20:23 INFO MemoryStore: MemoryStore cleared
23/12/04 11:20:23 INFO BlockManager: BlockManager stopped
23/12/04 11:20:23 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 11:20:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 11:20:23 INFO SparkContext: Successfully stopped SparkContext
23/12/04 11:20:23 INFO ShutdownHookManager: Shutdown hook called
23/12/04 11:20:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-8dec5f88-4587-4b26-898a-26b426c05ac0
23/12/04 11:22:07 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 11:22:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 11:22:07 INFO SparkContext: Running Spark version 3.5.0
23/12/04 11:22:07 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:22:07 INFO SparkContext: Java version 11.0.20
23/12/04 11:22:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 11:22:07 INFO ResourceUtils: ==============================================================
23/12/04 11:22:07 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 11:22:07 INFO ResourceUtils: ==============================================================
23/12/04 11:22:07 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 11:22:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 11:22:07 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 11:22:07 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 11:22:07 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 11:22:07 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 11:22:07 INFO SecurityManager: Changing view acls groups to: 
23/12/04 11:22:07 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 11:22:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 11:22:07 INFO Utils: Successfully started service 'sparkDriver' on port 38685.
23/12/04 11:22:08 INFO SparkEnv: Registering MapOutputTracker
23/12/04 11:22:08 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 11:22:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 11:22:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 11:22:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 11:22:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-161476a3-8f7e-4ecb-ae2a-4dcce60241a4
23/12/04 11:22:08 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 11:22:08 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 11:22:08 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 11:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 11:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 11:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 11:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 11:22:08 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 11:22:08 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 11:22:08 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:22:08 INFO Executor: Java version 11.0.20
23/12/04 11:22:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 11:22:08 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@71734a90 for default.
23/12/04 11:22:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42385.
23/12/04 11:22:08 INFO NettyBlockTransferService: Server created on 10.25.86.80:42385
23/12/04 11:22:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 11:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 42385, None)
23/12/04 11:22:08 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:42385 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 42385, None)
23/12/04 11:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 42385, None)
23/12/04 11:22:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 42385, None)
23/12/04 11:22:08 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:22:08 INFO xnio: XNIO version 3.8.7.Final
23/12/04 11:22:08 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 11:22:08 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 11:22:18 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:22:30 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:22:30 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:22:30 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:22:30 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:22:30 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:22:30 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:22:30 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:22:43 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:22:43 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:22:48 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 11:22:48 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 11:22:48 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 11:22:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 11:22:48 INFO MemoryStore: MemoryStore cleared
23/12/04 11:22:48 INFO BlockManager: BlockManager stopped
23/12/04 11:22:48 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 11:22:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 11:22:48 INFO SparkContext: Successfully stopped SparkContext
23/12/04 11:22:48 INFO ShutdownHookManager: Shutdown hook called
23/12/04 11:22:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-25244e2a-27c2-48c2-9099-6cd782ca2dfa
23/12/04 11:23:53 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 11:23:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 11:23:53 INFO SparkContext: Running Spark version 3.5.0
23/12/04 11:23:53 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:23:53 INFO SparkContext: Java version 11.0.20
23/12/04 11:23:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 11:23:54 INFO ResourceUtils: ==============================================================
23/12/04 11:23:54 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 11:23:54 INFO ResourceUtils: ==============================================================
23/12/04 11:23:54 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 11:23:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 11:23:54 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 11:23:54 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 11:23:54 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 11:23:54 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 11:23:54 INFO SecurityManager: Changing view acls groups to: 
23/12/04 11:23:54 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 11:23:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 11:23:54 INFO Utils: Successfully started service 'sparkDriver' on port 38093.
23/12/04 11:23:54 INFO SparkEnv: Registering MapOutputTracker
23/12/04 11:23:54 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 11:23:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 11:23:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 11:23:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 11:23:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ce6eb82e-7fd6-474e-9731-8686777801f9
23/12/04 11:23:54 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 11:23:54 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 11:23:54 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 11:23:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 11:23:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 11:23:54 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 11:23:54 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 11:23:54 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 11:23:54 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 11:23:54 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:23:54 INFO Executor: Java version 11.0.20
23/12/04 11:23:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 11:23:54 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7bf2aff0 for default.
23/12/04 11:23:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45551.
23/12/04 11:23:54 INFO NettyBlockTransferService: Server created on 10.25.86.80:45551
23/12/04 11:23:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 11:23:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 45551, None)
23/12/04 11:23:54 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:45551 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 45551, None)
23/12/04 11:23:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 45551, None)
23/12/04 11:23:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 45551, None)
23/12/04 11:23:54 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:23:54 INFO xnio: XNIO version 3.8.7.Final
23/12/04 11:23:54 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 11:23:54 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 11:23:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 11:23:55 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 11:24:11 INFO CodeGenerator: Code generated in 136.498999 ms
23/12/04 11:24:11 INFO CodeGenerator: Code generated in 14.531404 ms
23/12/04 11:24:11 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:24:11 INFO DAGScheduler: Got job 0 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:24:11 INFO DAGScheduler: Final stage: ResultStage 0 (collect at Ast.scala:253)
23/12/04 11:24:11 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:24:11 INFO DAGScheduler: Missing parents: List()
23/12/04 11:24:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:24:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:24:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:24:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:45551 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:24:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/04 11:24:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:24:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/04 11:24:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 11:24:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/04 11:24:11 INFO CodeGenerator: Code generated in 7.623839 ms
23/12/04 11:24:11 INFO CodeGenerator: Code generated in 7.702748 ms
23/12/04 11:24:12 INFO CodeGenerator: Code generated in 23.72992 ms
23/12/04 11:24:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1869 bytes result sent to driver
23/12/04 11:24:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 662 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:24:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/04 11:24:12 INFO DAGScheduler: ResultStage 0 (collect at Ast.scala:253) finished in 0.752 s
23/12/04 11:24:12 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:24:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/04 11:24:12 INFO DAGScheduler: Job 0 finished: collect at Ast.scala:253, took 0.781847 s
23/12/04 11:24:12 INFO DAGScheduler: Registering RDD 8 (collect at Ast.scala:253) as input to shuffle 0
23/12/04 11:24:12 INFO DAGScheduler: Got map stage job 1 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:24:12 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at Ast.scala:253)
23/12/04 11:24:12 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:24:12 INFO DAGScheduler: Missing parents: List()
23/12/04 11:24:12 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:24:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:24:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:24:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.25.86.80:45551 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:24:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/12/04 11:24:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:24:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/12/04 11:24:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 11:24:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/12/04 11:24:12 INFO CodeGenerator: Code generated in 5.477572 ms
23/12/04 11:24:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1793 bytes result sent to driver
23/12/04 11:24:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 51 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:24:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/04 11:24:12 INFO DAGScheduler: ShuffleMapStage 1 (collect at Ast.scala:253) finished in 0.071 s
23/12/04 11:24:12 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:24:12 INFO DAGScheduler: running: HashSet()
23/12/04 11:24:12 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:24:12 INFO DAGScheduler: failed: HashSet()
23/12/04 11:24:12 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:24:12 INFO CodeGenerator: Code generated in 8.289966 ms
23/12/04 11:24:12 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:24:12 INFO DAGScheduler: Got job 2 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:24:12 INFO DAGScheduler: Final stage: ResultStage 3 (collect at Ast.scala:253)
23/12/04 11:24:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
23/12/04 11:24:12 INFO DAGScheduler: Missing parents: List()
23/12/04 11:24:12 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:24:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:24:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:24:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.25.86.80:45551 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:24:12 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
23/12/04 11:24:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:24:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
23/12/04 11:24:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:24:12 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
23/12/04 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
23/12/04 11:24:12 INFO CodeGenerator: Code generated in 9.789538 ms
23/12/04 11:24:12 INFO CodeGenerator: Code generated in 6.234276 ms
23/12/04 11:24:12 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4537 bytes result sent to driver
23/12/04 11:24:12 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 111 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:24:12 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
23/12/04 11:24:12 INFO DAGScheduler: ResultStage 3 (collect at Ast.scala:253) finished in 0.122 s
23/12/04 11:24:12 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:24:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
23/12/04 11:24:12 INFO DAGScheduler: Job 2 finished: collect at Ast.scala:253, took 0.130247 s
23/12/04 11:24:12 INFO CodeGenerator: Code generated in 6.662633 ms
23/12/04 11:24:23 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:24:23 INFO DAGScheduler: Got job 3 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:24:23 INFO DAGScheduler: Final stage: ResultStage 4 (collect at Ast.scala:253)
23/12/04 11:24:23 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:24:23 INFO DAGScheduler: Missing parents: List()
23/12/04 11:24:23 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:24:23 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:24:23 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:24:23 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.25.86.80:45551 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:24:23 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
23/12/04 11:24:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:24:23 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/12/04 11:24:23 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/04 11:24:23 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
23/12/04 11:24:23 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 1826 bytes result sent to driver
23/12/04 11:24:23 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:24:23 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/12/04 11:24:23 INFO DAGScheduler: ResultStage 4 (collect at Ast.scala:253) finished in 0.019 s
23/12/04 11:24:23 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:24:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/12/04 11:24:23 INFO DAGScheduler: Job 3 finished: collect at Ast.scala:253, took 0.022780 s
23/12/04 11:24:23 INFO DAGScheduler: Registering RDD 16 (collect at Ast.scala:253) as input to shuffle 1
23/12/04 11:24:23 INFO DAGScheduler: Got map stage job 4 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:24:23 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (collect at Ast.scala:253)
23/12/04 11:24:23 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:24:23 INFO DAGScheduler: Missing parents: List()
23/12/04 11:24:23 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:24:23 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:24:23 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:24:23 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.25.86.80:45551 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:24:23 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
23/12/04 11:24:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:24:23 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
23/12/04 11:24:23 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 11:24:23 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
23/12/04 11:24:23 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1793 bytes result sent to driver
23/12/04 11:24:23 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 17 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:24:23 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
23/12/04 11:24:23 INFO DAGScheduler: ShuffleMapStage 5 (collect at Ast.scala:253) finished in 0.025 s
23/12/04 11:24:23 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:24:23 INFO DAGScheduler: running: HashSet()
23/12/04 11:24:23 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:24:23 INFO DAGScheduler: failed: HashSet()
23/12/04 11:24:23 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:24:23 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:24:23 INFO DAGScheduler: Got job 5 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:24:23 INFO DAGScheduler: Final stage: ResultStage 7 (collect at Ast.scala:253)
23/12/04 11:24:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
23/12/04 11:24:23 INFO DAGScheduler: Missing parents: List()
23/12/04 11:24:23 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:24:23 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:24:23 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:24:23 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.25.86.80:45551 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:24:23 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
23/12/04 11:24:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:24:23 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
23/12/04 11:24:23 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:24:23 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)
23/12/04 11:24:23 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:24:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 11:24:23 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 4494 bytes result sent to driver
23/12/04 11:24:23 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:24:23 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
23/12/04 11:24:23 INFO DAGScheduler: ResultStage 7 (collect at Ast.scala:253) finished in 0.020 s
23/12/04 11:24:23 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:24:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
23/12/04 11:24:23 INFO DAGScheduler: Job 5 finished: collect at Ast.scala:253, took 0.022227 s
23/12/04 11:24:29 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:24:29 INFO DAGScheduler: Got job 6 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:24:29 INFO DAGScheduler: Final stage: ResultStage 8 (collect at Ast.scala:253)
23/12/04 11:24:29 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:24:29 INFO DAGScheduler: Missing parents: List()
23/12/04 11:24:29 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:24:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:24:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:24:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.25.86.80:45551 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:24:29 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
23/12/04 11:24:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:24:29 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
23/12/04 11:24:29 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 11:24:29 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)
23/12/04 11:24:29 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 1826 bytes result sent to driver
23/12/04 11:24:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:24:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
23/12/04 11:24:29 INFO DAGScheduler: ResultStage 8 (collect at Ast.scala:253) finished in 0.016 s
23/12/04 11:24:29 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:24:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
23/12/04 11:24:29 INFO DAGScheduler: Job 6 finished: collect at Ast.scala:253, took 0.018367 s
23/12/04 11:24:29 INFO DAGScheduler: Registering RDD 24 (collect at Ast.scala:253) as input to shuffle 2
23/12/04 11:24:29 INFO DAGScheduler: Got map stage job 7 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:24:29 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (collect at Ast.scala:253)
23/12/04 11:24:29 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:24:29 INFO DAGScheduler: Missing parents: List()
23/12/04 11:24:29 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:24:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:24:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:24:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.25.86.80:45551 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:24:29 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
23/12/04 11:24:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:24:29 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
23/12/04 11:24:29 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/04 11:24:29 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)
23/12/04 11:24:29 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 1793 bytes result sent to driver
23/12/04 11:24:29 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:24:29 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
23/12/04 11:24:29 INFO DAGScheduler: ShuffleMapStage 9 (collect at Ast.scala:253) finished in 0.021 s
23/12/04 11:24:29 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:24:29 INFO DAGScheduler: running: HashSet()
23/12/04 11:24:29 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:24:29 INFO DAGScheduler: failed: HashSet()
23/12/04 11:24:29 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:24:29 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:24:29 INFO DAGScheduler: Got job 8 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:24:29 INFO DAGScheduler: Final stage: ResultStage 11 (collect at Ast.scala:253)
23/12/04 11:24:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
23/12/04 11:24:29 INFO DAGScheduler: Missing parents: List()
23/12/04 11:24:29 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:24:29 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:24:29 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:24:29 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.25.86.80:45551 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:24:29 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
23/12/04 11:24:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:24:29 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
23/12/04 11:24:29 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:24:29 INFO Executor: Running task 0.0 in stage 11.0 (TID 8)
23/12/04 11:24:29 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:24:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 11:24:29 INFO Executor: Finished task 0.0 in stage 11.0 (TID 8). 4494 bytes result sent to driver
23/12/04 11:24:29 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:24:29 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
23/12/04 11:24:29 INFO DAGScheduler: ResultStage 11 (collect at Ast.scala:253) finished in 0.018 s
23/12/04 11:24:29 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:24:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
23/12/04 11:24:29 INFO DAGScheduler: Job 8 finished: collect at Ast.scala:253, took 0.021093 s
23/12/04 11:25:21 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 11:25:21 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 11:25:21 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 11:25:21 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 11:25:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 11:25:21 INFO MemoryStore: MemoryStore cleared
23/12/04 11:25:21 INFO BlockManager: BlockManager stopped
23/12/04 11:25:21 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 11:25:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 11:25:21 INFO SparkContext: Successfully stopped SparkContext
23/12/04 11:25:21 INFO ShutdownHookManager: Shutdown hook called
23/12/04 11:25:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-f3e2c3f3-886e-48ae-9fcd-7560e91e1203
23/12/04 11:27:15 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 11:27:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 11:27:15 INFO SparkContext: Running Spark version 3.5.0
23/12/04 11:27:15 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:27:15 INFO SparkContext: Java version 11.0.20
23/12/04 11:27:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 11:27:15 INFO ResourceUtils: ==============================================================
23/12/04 11:27:15 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 11:27:15 INFO ResourceUtils: ==============================================================
23/12/04 11:27:15 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 11:27:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 11:27:15 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 11:27:15 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 11:27:15 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 11:27:15 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 11:27:15 INFO SecurityManager: Changing view acls groups to: 
23/12/04 11:27:15 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 11:27:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 11:27:15 INFO Utils: Successfully started service 'sparkDriver' on port 34207.
23/12/04 11:27:15 INFO SparkEnv: Registering MapOutputTracker
23/12/04 11:27:15 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 11:27:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 11:27:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 11:27:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 11:27:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-00214a19-dd40-42d6-ab6c-bfd7ba585197
23/12/04 11:27:15 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 11:27:15 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 11:27:15 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 11:27:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 11:27:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 11:27:15 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 11:27:15 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 11:27:15 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 11:27:15 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 11:27:15 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:27:15 INFO Executor: Java version 11.0.20
23/12/04 11:27:15 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 11:27:15 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7bf2aff0 for default.
23/12/04 11:27:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42259.
23/12/04 11:27:15 INFO NettyBlockTransferService: Server created on 10.25.86.80:42259
23/12/04 11:27:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 11:27:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 42259, None)
23/12/04 11:27:15 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:42259 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 42259, None)
23/12/04 11:27:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 42259, None)
23/12/04 11:27:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 42259, None)
23/12/04 11:27:16 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:27:16 INFO xnio: XNIO version 3.8.7.Final
23/12/04 11:27:16 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 11:27:16 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 11:27:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 11:27:16 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 11:27:28 INFO CodeGenerator: Code generated in 131.7069 ms
23/12/04 11:27:28 INFO CodeGenerator: Code generated in 12.084688 ms
23/12/04 11:27:28 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:27:28 INFO DAGScheduler: Got job 0 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:27:28 INFO DAGScheduler: Final stage: ResultStage 0 (collect at Ast.scala:253)
23/12/04 11:27:28 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:27:28 INFO DAGScheduler: Missing parents: List()
23/12/04 11:27:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:27:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:27:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:27:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:42259 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:27:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/04 11:27:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:27:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/04 11:27:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 11:27:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/04 11:27:28 INFO CodeGenerator: Code generated in 6.869787 ms
23/12/04 11:27:28 INFO CodeGenerator: Code generated in 7.335686 ms
23/12/04 11:27:29 INFO CodeGenerator: Code generated in 27.113951 ms
23/12/04 11:27:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1869 bytes result sent to driver
23/12/04 11:27:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 655 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:27:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/04 11:27:29 INFO DAGScheduler: ResultStage 0 (collect at Ast.scala:253) finished in 0.746 s
23/12/04 11:27:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:27:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/04 11:27:29 INFO DAGScheduler: Job 0 finished: collect at Ast.scala:253, took 0.771054 s
23/12/04 11:27:29 INFO DAGScheduler: Registering RDD 8 (collect at Ast.scala:253) as input to shuffle 0
23/12/04 11:27:29 INFO DAGScheduler: Got map stage job 1 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:27:29 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at Ast.scala:253)
23/12/04 11:27:29 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:27:29 INFO DAGScheduler: Missing parents: List()
23/12/04 11:27:29 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.25.86.80:42259 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:27:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/12/04 11:27:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:27:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/12/04 11:27:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 11:27:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/12/04 11:27:29 INFO CodeGenerator: Code generated in 4.603605 ms
23/12/04 11:27:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1836 bytes result sent to driver
23/12/04 11:27:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 47 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:27:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/04 11:27:29 INFO DAGScheduler: ShuffleMapStage 1 (collect at Ast.scala:253) finished in 0.059 s
23/12/04 11:27:29 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:27:29 INFO DAGScheduler: running: HashSet()
23/12/04 11:27:29 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:27:29 INFO DAGScheduler: failed: HashSet()
23/12/04 11:27:29 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:27:29 INFO CodeGenerator: Code generated in 8.647887 ms
23/12/04 11:27:29 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:27:29 INFO DAGScheduler: Got job 2 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:27:29 INFO DAGScheduler: Final stage: ResultStage 3 (collect at Ast.scala:253)
23/12/04 11:27:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
23/12/04 11:27:29 INFO DAGScheduler: Missing parents: List()
23/12/04 11:27:29 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.25.86.80:42259 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:27:29 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
23/12/04 11:27:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:27:29 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
23/12/04 11:27:29 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:27:29 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
23/12/04 11:27:29 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
23/12/04 11:27:29 INFO CodeGenerator: Code generated in 9.919952 ms
23/12/04 11:27:29 INFO CodeGenerator: Code generated in 4.494512 ms
23/12/04 11:27:29 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4537 bytes result sent to driver
23/12/04 11:27:29 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 96 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:27:29 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
23/12/04 11:27:29 INFO DAGScheduler: ResultStage 3 (collect at Ast.scala:253) finished in 0.106 s
23/12/04 11:27:29 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:27:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
23/12/04 11:27:29 INFO DAGScheduler: Job 2 finished: collect at Ast.scala:253, took 0.113633 s
23/12/04 11:27:29 INFO CodeGenerator: Code generated in 7.704941 ms
23/12/04 11:27:29 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:27:29 INFO DAGScheduler: Got job 3 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:27:29 INFO DAGScheduler: Final stage: ResultStage 4 (collect at Ast.scala:253)
23/12/04 11:27:29 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:27:29 INFO DAGScheduler: Missing parents: List()
23/12/04 11:27:29 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.25.86.80:42259 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:27:29 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
23/12/04 11:27:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:27:29 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/12/04 11:27:29 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/04 11:27:29 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
23/12/04 11:27:29 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 1826 bytes result sent to driver
23/12/04 11:27:29 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:27:29 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/12/04 11:27:29 INFO DAGScheduler: ResultStage 4 (collect at Ast.scala:253) finished in 0.018 s
23/12/04 11:27:29 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:27:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/12/04 11:27:29 INFO DAGScheduler: Job 3 finished: collect at Ast.scala:253, took 0.020838 s
23/12/04 11:27:29 INFO DAGScheduler: Registering RDD 16 (collect at Ast.scala:253) as input to shuffle 1
23/12/04 11:27:29 INFO DAGScheduler: Got map stage job 4 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:27:29 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (collect at Ast.scala:253)
23/12/04 11:27:29 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:27:29 INFO DAGScheduler: Missing parents: List()
23/12/04 11:27:29 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.25.86.80:42259 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:27:29 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
23/12/04 11:27:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:27:29 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
23/12/04 11:27:29 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 11:27:29 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
23/12/04 11:27:29 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1793 bytes result sent to driver
23/12/04 11:27:29 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 15 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:27:29 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
23/12/04 11:27:29 INFO DAGScheduler: ShuffleMapStage 5 (collect at Ast.scala:253) finished in 0.022 s
23/12/04 11:27:29 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:27:29 INFO DAGScheduler: running: HashSet()
23/12/04 11:27:29 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:27:29 INFO DAGScheduler: failed: HashSet()
23/12/04 11:27:29 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:27:29 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:27:29 INFO DAGScheduler: Got job 5 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:27:29 INFO DAGScheduler: Final stage: ResultStage 7 (collect at Ast.scala:253)
23/12/04 11:27:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
23/12/04 11:27:29 INFO DAGScheduler: Missing parents: List()
23/12/04 11:27:29 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.25.86.80:42259 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:27:29 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
23/12/04 11:27:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:27:29 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
23/12/04 11:27:29 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:27:29 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)
23/12/04 11:27:29 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 11:27:29 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 4494 bytes result sent to driver
23/12/04 11:27:29 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:27:29 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
23/12/04 11:27:29 INFO DAGScheduler: ResultStage 7 (collect at Ast.scala:253) finished in 0.023 s
23/12/04 11:27:29 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:27:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
23/12/04 11:27:29 INFO DAGScheduler: Job 5 finished: collect at Ast.scala:253, took 0.026481 s
23/12/04 11:27:29 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:27:29 INFO DAGScheduler: Got job 6 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:27:29 INFO DAGScheduler: Final stage: ResultStage 8 (collect at Ast.scala:253)
23/12/04 11:27:29 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:27:29 INFO DAGScheduler: Missing parents: List()
23/12/04 11:27:29 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.25.86.80:42259 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:27:29 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
23/12/04 11:27:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:27:29 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
23/12/04 11:27:29 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 11:27:29 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)
23/12/04 11:27:29 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 1826 bytes result sent to driver
23/12/04 11:27:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:27:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
23/12/04 11:27:29 INFO DAGScheduler: ResultStage 8 (collect at Ast.scala:253) finished in 0.017 s
23/12/04 11:27:29 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:27:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
23/12/04 11:27:29 INFO DAGScheduler: Job 6 finished: collect at Ast.scala:253, took 0.019722 s
23/12/04 11:27:29 INFO DAGScheduler: Registering RDD 24 (collect at Ast.scala:253) as input to shuffle 2
23/12/04 11:27:29 INFO DAGScheduler: Got map stage job 7 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:27:29 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (collect at Ast.scala:253)
23/12/04 11:27:29 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:27:29 INFO DAGScheduler: Missing parents: List()
23/12/04 11:27:29 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.25.86.80:42259 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:27:29 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
23/12/04 11:27:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:27:29 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
23/12/04 11:27:29 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/04 11:27:29 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)
23/12/04 11:27:29 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 1793 bytes result sent to driver
23/12/04 11:27:29 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 15 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:27:29 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
23/12/04 11:27:29 INFO DAGScheduler: ShuffleMapStage 9 (collect at Ast.scala:253) finished in 0.023 s
23/12/04 11:27:29 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:27:29 INFO DAGScheduler: running: HashSet()
23/12/04 11:27:29 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:27:29 INFO DAGScheduler: failed: HashSet()
23/12/04 11:27:29 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:27:29 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:27:29 INFO DAGScheduler: Got job 8 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:27:29 INFO DAGScheduler: Final stage: ResultStage 11 (collect at Ast.scala:253)
23/12/04 11:27:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
23/12/04 11:27:29 INFO DAGScheduler: Missing parents: List()
23/12/04 11:27:29 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:27:29 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.25.86.80:42259 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:27:29 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
23/12/04 11:27:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:27:29 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
23/12/04 11:27:29 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:27:29 INFO Executor: Running task 0.0 in stage 11.0 (TID 8)
23/12/04 11:27:29 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 11:27:29 INFO Executor: Finished task 0.0 in stage 11.0 (TID 8). 4494 bytes result sent to driver
23/12/04 11:27:29 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:27:29 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
23/12/04 11:27:29 INFO DAGScheduler: ResultStage 11 (collect at Ast.scala:253) finished in 0.019 s
23/12/04 11:27:29 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:27:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
23/12/04 11:27:29 INFO DAGScheduler: Job 8 finished: collect at Ast.scala:253, took 0.022636 s
23/12/04 11:30:26 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 11:30:26 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 11:30:26 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 11:30:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 11:30:26 INFO MemoryStore: MemoryStore cleared
23/12/04 11:30:26 INFO BlockManager: BlockManager stopped
23/12/04 11:30:26 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 11:30:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 11:30:26 INFO SparkContext: Successfully stopped SparkContext
23/12/04 11:30:26 INFO ShutdownHookManager: Shutdown hook called
23/12/04 11:30:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-c1d20fcf-81e6-484d-8726-b2239572cc13
23/12/04 11:30:52 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 11:30:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 11:30:52 INFO SparkContext: Running Spark version 3.5.0
23/12/04 11:30:52 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:30:52 INFO SparkContext: Java version 11.0.20
23/12/04 11:30:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 11:30:52 INFO ResourceUtils: ==============================================================
23/12/04 11:30:52 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 11:30:52 INFO ResourceUtils: ==============================================================
23/12/04 11:30:52 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 11:30:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 11:30:52 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 11:30:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 11:30:52 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 11:30:52 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 11:30:52 INFO SecurityManager: Changing view acls groups to: 
23/12/04 11:30:52 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 11:30:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 11:30:52 INFO Utils: Successfully started service 'sparkDriver' on port 37417.
23/12/04 11:30:52 INFO SparkEnv: Registering MapOutputTracker
23/12/04 11:30:52 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 11:30:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 11:30:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 11:30:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 11:30:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d871f14d-e0f0-4acc-8fe4-72a1931f8449
23/12/04 11:30:52 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 11:30:52 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 11:30:52 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 11:30:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 11:30:52 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 11:30:52 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 11:30:52 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 11:30:52 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 11:30:53 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 11:30:53 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 11:30:53 INFO Executor: Java version 11.0.20
23/12/04 11:30:53 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 11:30:53 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@49a4c601 for default.
23/12/04 11:30:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46579.
23/12/04 11:30:53 INFO NettyBlockTransferService: Server created on 10.25.86.80:46579
23/12/04 11:30:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 11:30:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 46579, None)
23/12/04 11:30:53 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:46579 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 46579, None)
23/12/04 11:30:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 46579, None)
23/12/04 11:30:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 46579, None)
23/12/04 11:30:53 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 11:30:53 INFO xnio: XNIO version 3.8.7.Final
23/12/04 11:30:53 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 11:30:53 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 11:30:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 11:30:55 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 11:31:34 INFO CodeGenerator: Code generated in 133.36771 ms
23/12/04 11:31:34 INFO CodeGenerator: Code generated in 12.371735 ms
23/12/04 11:31:34 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:31:34 INFO DAGScheduler: Got job 0 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:31:34 INFO DAGScheduler: Final stage: ResultStage 0 (collect at Ast.scala:253)
23/12/04 11:31:34 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:31:34 INFO DAGScheduler: Missing parents: List()
23/12/04 11:31:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:31:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:31:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:31:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:46579 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:31:34 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/04 11:31:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:31:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/04 11:31:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 11:31:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/04 11:31:34 INFO CodeGenerator: Code generated in 7.332447 ms
23/12/04 11:31:34 INFO CodeGenerator: Code generated in 9.223383 ms
23/12/04 11:31:35 INFO CodeGenerator: Code generated in 23.63035 ms
23/12/04 11:31:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1869 bytes result sent to driver
23/12/04 11:31:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 665 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:31:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/04 11:31:35 INFO DAGScheduler: ResultStage 0 (collect at Ast.scala:253) finished in 0.758 s
23/12/04 11:31:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:31:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/04 11:31:35 INFO DAGScheduler: Job 0 finished: collect at Ast.scala:253, took 0.785354 s
23/12/04 11:31:35 INFO DAGScheduler: Registering RDD 8 (collect at Ast.scala:253) as input to shuffle 0
23/12/04 11:31:35 INFO DAGScheduler: Got map stage job 1 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:31:35 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at Ast.scala:253)
23/12/04 11:31:35 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:31:35 INFO DAGScheduler: Missing parents: List()
23/12/04 11:31:35 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:31:35 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:31:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:31:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.25.86.80:46579 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:31:35 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/12/04 11:31:35 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:31:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/12/04 11:31:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 11:31:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/12/04 11:31:35 INFO CodeGenerator: Code generated in 4.649999 ms
23/12/04 11:31:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1836 bytes result sent to driver
23/12/04 11:31:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 48 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:31:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/04 11:31:35 INFO DAGScheduler: ShuffleMapStage 1 (collect at Ast.scala:253) finished in 0.062 s
23/12/04 11:31:35 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:31:35 INFO DAGScheduler: running: HashSet()
23/12/04 11:31:35 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:31:35 INFO DAGScheduler: failed: HashSet()
23/12/04 11:31:35 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:31:35 INFO CodeGenerator: Code generated in 10.736193 ms
23/12/04 11:31:35 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:31:35 INFO DAGScheduler: Got job 2 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:31:35 INFO DAGScheduler: Final stage: ResultStage 3 (collect at Ast.scala:253)
23/12/04 11:31:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
23/12/04 11:31:35 INFO DAGScheduler: Missing parents: List()
23/12/04 11:31:35 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:31:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:31:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:31:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.25.86.80:46579 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:31:35 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
23/12/04 11:31:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:31:35 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
23/12/04 11:31:35 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:31:35 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
23/12/04 11:31:35 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
23/12/04 11:31:35 INFO CodeGenerator: Code generated in 7.006017 ms
23/12/04 11:31:35 INFO CodeGenerator: Code generated in 5.669346 ms
23/12/04 11:31:35 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4537 bytes result sent to driver
23/12/04 11:31:35 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 86 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:31:35 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
23/12/04 11:31:35 INFO DAGScheduler: ResultStage 3 (collect at Ast.scala:253) finished in 0.096 s
23/12/04 11:31:35 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:31:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
23/12/04 11:31:35 INFO DAGScheduler: Job 2 finished: collect at Ast.scala:253, took 0.104394 s
23/12/04 11:31:35 INFO CodeGenerator: Code generated in 6.311533 ms
23/12/04 11:31:36 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:31:36 INFO DAGScheduler: Got job 3 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:31:36 INFO DAGScheduler: Final stage: ResultStage 4 (collect at Ast.scala:253)
23/12/04 11:31:36 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:31:36 INFO DAGScheduler: Missing parents: List()
23/12/04 11:31:36 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:31:36 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:31:36 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:31:36 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.25.86.80:46579 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:31:36 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
23/12/04 11:31:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:31:36 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/12/04 11:31:36 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/04 11:31:36 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
23/12/04 11:31:36 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 1826 bytes result sent to driver
23/12/04 11:31:36 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:31:36 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/12/04 11:31:36 INFO DAGScheduler: ResultStage 4 (collect at Ast.scala:253) finished in 0.020 s
23/12/04 11:31:36 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:31:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/12/04 11:31:36 INFO DAGScheduler: Job 3 finished: collect at Ast.scala:253, took 0.022520 s
23/12/04 11:31:36 INFO DAGScheduler: Registering RDD 16 (collect at Ast.scala:253) as input to shuffle 1
23/12/04 11:31:36 INFO DAGScheduler: Got map stage job 4 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:31:36 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (collect at Ast.scala:253)
23/12/04 11:31:36 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:31:36 INFO DAGScheduler: Missing parents: List()
23/12/04 11:31:36 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:31:36 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:31:36 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:31:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.25.86.80:46579 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:31:36 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
23/12/04 11:31:36 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:31:36 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
23/12/04 11:31:36 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 11:31:36 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
23/12/04 11:31:36 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1793 bytes result sent to driver
23/12/04 11:31:36 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 20 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:31:36 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
23/12/04 11:31:36 INFO DAGScheduler: ShuffleMapStage 5 (collect at Ast.scala:253) finished in 0.027 s
23/12/04 11:31:36 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:31:36 INFO DAGScheduler: running: HashSet()
23/12/04 11:31:36 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:31:36 INFO DAGScheduler: failed: HashSet()
23/12/04 11:31:36 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:31:36 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:31:36 INFO DAGScheduler: Got job 5 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:31:36 INFO DAGScheduler: Final stage: ResultStage 7 (collect at Ast.scala:253)
23/12/04 11:31:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
23/12/04 11:31:36 INFO DAGScheduler: Missing parents: List()
23/12/04 11:31:36 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:31:36 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:31:37 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:31:37 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.25.86.80:46579 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:31:37 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
23/12/04 11:31:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:31:37 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
23/12/04 11:31:37 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:31:37 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)
23/12/04 11:31:37 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
23/12/04 11:31:37 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 4494 bytes result sent to driver
23/12/04 11:31:37 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 19 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:31:37 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
23/12/04 11:31:37 INFO DAGScheduler: ResultStage 7 (collect at Ast.scala:253) finished in 0.031 s
23/12/04 11:31:37 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:31:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
23/12/04 11:31:37 INFO DAGScheduler: Job 5 finished: collect at Ast.scala:253, took 0.035952 s
23/12/04 11:31:40 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:31:40 INFO DAGScheduler: Got job 6 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:31:40 INFO DAGScheduler: Final stage: ResultStage 8 (collect at Ast.scala:253)
23/12/04 11:31:40 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:31:40 INFO DAGScheduler: Missing parents: List()
23/12/04 11:31:40 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:31:40 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 11:31:40 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 11:31:40 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.25.86.80:46579 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 11:31:40 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
23/12/04 11:31:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:31:40 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
23/12/04 11:31:40 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 11:31:40 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)
23/12/04 11:31:40 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 1826 bytes result sent to driver
23/12/04 11:31:40 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:31:40 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
23/12/04 11:31:40 INFO DAGScheduler: ResultStage 8 (collect at Ast.scala:253) finished in 0.017 s
23/12/04 11:31:40 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:31:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
23/12/04 11:31:40 INFO DAGScheduler: Job 6 finished: collect at Ast.scala:253, took 0.018574 s
23/12/04 11:31:40 INFO DAGScheduler: Registering RDD 24 (collect at Ast.scala:253) as input to shuffle 2
23/12/04 11:31:40 INFO DAGScheduler: Got map stage job 7 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:31:40 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (collect at Ast.scala:253)
23/12/04 11:31:40 INFO DAGScheduler: Parents of final stage: List()
23/12/04 11:31:40 INFO DAGScheduler: Missing parents: List()
23/12/04 11:31:40 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:31:40 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 11:31:40 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 11:31:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.25.86.80:46579 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 11:31:40 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
23/12/04 11:31:40 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:31:40 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
23/12/04 11:31:40 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/04 11:31:40 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)
23/12/04 11:31:40 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 1793 bytes result sent to driver
23/12/04 11:31:40 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:31:40 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
23/12/04 11:31:40 INFO DAGScheduler: ShuffleMapStage 9 (collect at Ast.scala:253) finished in 0.020 s
23/12/04 11:31:40 INFO DAGScheduler: looking for newly runnable stages
23/12/04 11:31:40 INFO DAGScheduler: running: HashSet()
23/12/04 11:31:40 INFO DAGScheduler: waiting: HashSet()
23/12/04 11:31:40 INFO DAGScheduler: failed: HashSet()
23/12/04 11:31:40 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 11:31:40 INFO SparkContext: Starting job: collect at Ast.scala:253
23/12/04 11:31:40 INFO DAGScheduler: Got job 8 (collect at Ast.scala:253) with 1 output partitions
23/12/04 11:31:40 INFO DAGScheduler: Final stage: ResultStage 11 (collect at Ast.scala:253)
23/12/04 11:31:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
23/12/04 11:31:40 INFO DAGScheduler: Missing parents: List()
23/12/04 11:31:40 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:253), which has no missing parents
23/12/04 11:31:40 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 11:31:40 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 11:31:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.25.86.80:46579 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 11:31:40 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
23/12/04 11:31:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:253) (first 15 tasks are for partitions Vector(0))
23/12/04 11:31:40 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
23/12/04 11:31:40 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 11:31:40 INFO Executor: Running task 0.0 in stage 11.0 (TID 8)
23/12/04 11:31:40 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 11:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 11:31:40 INFO Executor: Finished task 0.0 in stage 11.0 (TID 8). 4494 bytes result sent to driver
23/12/04 11:31:40 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 11:31:40 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
23/12/04 11:31:40 INFO DAGScheduler: ResultStage 11 (collect at Ast.scala:253) finished in 0.020 s
23/12/04 11:31:40 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 11:31:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
23/12/04 11:31:40 INFO DAGScheduler: Job 8 finished: collect at Ast.scala:253, took 0.022803 s
23/12/04 11:57:37 ERROR CompilerAccess: A severe compiler error occurred, full details of the error can be found in the error report /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/.metals/.reports/metals-full/2023-12-04/r_compiler-error_(project-effective-scala-c1-s14-3039-4085)_11-57-37-939.md
23/12/04 11:59:19 INFO GenericEndpoint: Unsupported notification method: $/setTrace
23/12/04 12:00:53 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.25.86.80:46579 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 12:00:53 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.25.86.80:46579 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 12:00:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.25.86.80:46579 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 12:00:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.25.86.80:46579 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 12:00:53 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.25.86.80:46579 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 12:00:53 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.25.86.80:46579 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 12:00:53 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.25.86.80:46579 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 12:00:53 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.25.86.80:46579 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 12:00:53 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.25.86.80:46579 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 12:13:27 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 12:13:27 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 12:13:27 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 12:13:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 12:13:27 INFO MemoryStore: MemoryStore cleared
23/12/04 12:13:27 INFO BlockManager: BlockManager stopped
23/12/04 12:13:27 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 12:13:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 12:13:27 INFO SparkContext: Successfully stopped SparkContext
23/12/04 12:13:27 INFO ShutdownHookManager: Shutdown hook called
23/12/04 12:13:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-a7c33ac8-e702-4ed0-9e55-05a0281d3529
23/12/04 12:14:46 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 12:14:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 12:14:47 INFO SparkContext: Running Spark version 3.5.0
23/12/04 12:14:47 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 12:14:47 INFO SparkContext: Java version 11.0.20
23/12/04 12:14:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 12:14:47 INFO ResourceUtils: ==============================================================
23/12/04 12:14:47 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 12:14:47 INFO ResourceUtils: ==============================================================
23/12/04 12:14:47 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 12:14:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 12:14:47 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 12:14:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 12:14:47 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 12:14:47 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 12:14:47 INFO SecurityManager: Changing view acls groups to: 
23/12/04 12:14:47 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 12:14:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 12:14:47 INFO Utils: Successfully started service 'sparkDriver' on port 42311.
23/12/04 12:14:47 INFO SparkEnv: Registering MapOutputTracker
23/12/04 12:14:47 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 12:14:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 12:14:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 12:14:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 12:14:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-762f5b70-e188-4c11-a19f-7d206f487000
23/12/04 12:14:47 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 12:14:47 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 12:14:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 12:14:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 12:14:47 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 12:14:47 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 12:14:47 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 12:14:47 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 12:14:47 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 12:14:47 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 12:14:47 INFO Executor: Java version 11.0.20
23/12/04 12:14:47 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 12:14:47 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5cded6bc for default.
23/12/04 12:14:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39215.
23/12/04 12:14:47 INFO NettyBlockTransferService: Server created on 10.25.86.80:39215
23/12/04 12:14:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 12:14:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 39215, None)
23/12/04 12:14:47 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:39215 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 39215, None)
23/12/04 12:14:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 39215, None)
23/12/04 12:14:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 39215, None)
23/12/04 12:14:47 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 12:14:47 INFO xnio: XNIO version 3.8.7.Final
23/12/04 12:14:47 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 12:14:47 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 12:14:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 12:14:50 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 12:15:03 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 12:15:03 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 12:15:03 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 12:15:03 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 12:15:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 12:15:03 INFO MemoryStore: MemoryStore cleared
23/12/04 12:15:03 INFO BlockManager: BlockManager stopped
23/12/04 12:15:03 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 12:15:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 12:15:03 INFO SparkContext: Successfully stopped SparkContext
23/12/04 12:15:03 INFO ShutdownHookManager: Shutdown hook called
23/12/04 12:15:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-5c137400-71ef-4b67-bdf6-1a8fb9bcaf9d
23/12/04 12:18:39 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 12:18:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 12:18:39 INFO SparkContext: Running Spark version 3.5.0
23/12/04 12:18:39 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 12:18:39 INFO SparkContext: Java version 11.0.20
23/12/04 12:18:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 12:18:40 INFO ResourceUtils: ==============================================================
23/12/04 12:18:40 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 12:18:40 INFO ResourceUtils: ==============================================================
23/12/04 12:18:40 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 12:18:40 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 12:18:40 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 12:18:40 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 12:18:40 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 12:18:40 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 12:18:40 INFO SecurityManager: Changing view acls groups to: 
23/12/04 12:18:40 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 12:18:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 12:18:40 INFO Utils: Successfully started service 'sparkDriver' on port 42655.
23/12/04 12:18:40 INFO SparkEnv: Registering MapOutputTracker
23/12/04 12:18:40 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 12:18:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 12:18:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 12:18:40 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 12:18:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-81144111-024a-497b-84ef-7929a9764858
23/12/04 12:18:40 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 12:18:40 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 12:18:40 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 12:18:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 12:18:40 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 12:18:40 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 12:18:40 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 12:18:40 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 12:18:40 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 12:18:40 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 12:18:40 INFO Executor: Java version 11.0.20
23/12/04 12:18:40 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 12:18:40 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1d2c1ad for default.
23/12/04 12:18:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39127.
23/12/04 12:18:40 INFO NettyBlockTransferService: Server created on 10.25.86.80:39127
23/12/04 12:18:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 12:18:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 39127, None)
23/12/04 12:18:40 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:39127 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 39127, None)
23/12/04 12:18:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 39127, None)
23/12/04 12:18:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 39127, None)
23/12/04 12:18:40 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 12:18:40 INFO xnio: XNIO version 3.8.7.Final
23/12/04 12:18:40 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 12:18:40 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 12:18:42 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 12:18:42 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 12:20:00 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 12:20:00 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 12:20:00 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 12:20:00 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 12:20:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 12:20:00 INFO MemoryStore: MemoryStore cleared
23/12/04 12:20:00 INFO BlockManager: BlockManager stopped
23/12/04 12:20:00 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 12:20:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 12:20:00 INFO SparkContext: Successfully stopped SparkContext
23/12/04 12:20:00 INFO ShutdownHookManager: Shutdown hook called
23/12/04 12:20:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-449b0b34-7bf9-46d4-b396-c5e17c665562
23/12/04 12:23:27 WARN RemoteEndpoint: Unmatched cancel notification for request id 26799
23/12/04 12:36:43 WARN RemoteEndpoint: Unmatched cancel notification for request id 27144
23/12/04 13:11:11 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 13:11:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 13:11:11 INFO SparkContext: Running Spark version 3.5.0
23/12/04 13:11:11 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 13:11:11 INFO SparkContext: Java version 11.0.20
23/12/04 13:11:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 13:11:11 INFO ResourceUtils: ==============================================================
23/12/04 13:11:11 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 13:11:11 INFO ResourceUtils: ==============================================================
23/12/04 13:11:11 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 13:11:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 13:11:11 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 13:11:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 13:11:11 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 13:11:11 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 13:11:11 INFO SecurityManager: Changing view acls groups to: 
23/12/04 13:11:11 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 13:11:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 13:11:12 INFO Utils: Successfully started service 'sparkDriver' on port 36481.
23/12/04 13:11:12 INFO SparkEnv: Registering MapOutputTracker
23/12/04 13:11:12 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 13:11:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 13:11:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 13:11:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 13:11:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2a2f2ab0-98f0-49f4-8c83-3b9bc08abce8
23/12/04 13:11:12 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 13:11:12 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 13:11:12 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 13:11:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 13:11:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 13:11:12 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 13:11:12 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 13:11:12 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 13:11:12 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 13:11:12 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 13:11:12 INFO Executor: Java version 11.0.20
23/12/04 13:11:12 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 13:11:12 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@69edd180 for default.
23/12/04 13:11:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44037.
23/12/04 13:11:12 INFO NettyBlockTransferService: Server created on 10.25.86.80:44037
23/12/04 13:11:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 13:11:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 44037, None)
23/12/04 13:11:12 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:44037 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 44037, None)
23/12/04 13:11:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 44037, None)
23/12/04 13:11:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 44037, None)
23/12/04 13:11:12 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 13:11:12 INFO xnio: XNIO version 3.8.7.Final
23/12/04 13:11:12 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 13:11:12 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 13:11:14 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 13:11:14 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 13:11:44 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 13:11:44 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 13:11:44 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 13:11:44 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 13:11:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 13:11:44 INFO MemoryStore: MemoryStore cleared
23/12/04 13:11:44 INFO BlockManager: BlockManager stopped
23/12/04 13:11:44 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 13:11:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 13:11:44 INFO SparkContext: Successfully stopped SparkContext
23/12/04 13:11:44 INFO ShutdownHookManager: Shutdown hook called
23/12/04 13:11:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-e8871f3c-eae1-4265-84db-a809bc588f25
23/12/04 13:15:17 WARN RemoteEndpoint: Unmatched cancel notification for request id 28494
23/12/04 13:19:50 ERROR CompilerAccess: A severe compiler error occurred, full details of the error can be found in the error report /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/.metals/.reports/metals-full/2023-12-04/r_compiler-error_(project-effective-scala-c1-s14-3039-4085-test)_13-19-50-195.md
23/12/04 13:20:09 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 13:20:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 13:20:10 INFO SparkContext: Running Spark version 3.5.0
23/12/04 13:20:10 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 13:20:10 INFO SparkContext: Java version 11.0.20
23/12/04 13:20:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 13:20:10 INFO ResourceUtils: ==============================================================
23/12/04 13:20:10 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 13:20:10 INFO ResourceUtils: ==============================================================
23/12/04 13:20:10 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 13:20:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 13:20:10 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 13:20:10 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 13:20:10 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 13:20:10 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 13:20:10 INFO SecurityManager: Changing view acls groups to: 
23/12/04 13:20:10 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 13:20:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 13:20:10 INFO Utils: Successfully started service 'sparkDriver' on port 44319.
23/12/04 13:20:10 INFO SparkEnv: Registering MapOutputTracker
23/12/04 13:20:10 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 13:20:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 13:20:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 13:20:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 13:20:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-86f481d1-57ef-47de-963a-aa83c9d7112a
23/12/04 13:20:10 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 13:20:10 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 13:20:10 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 13:20:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 13:20:10 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 13:20:10 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 13:20:10 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 13:20:10 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 13:20:10 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 13:20:10 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 13:20:10 INFO Executor: Java version 11.0.20
23/12/04 13:20:10 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 13:20:10 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@49a4c601 for default.
23/12/04 13:20:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42739.
23/12/04 13:20:10 INFO NettyBlockTransferService: Server created on 10.25.86.80:42739
23/12/04 13:20:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 13:20:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 42739, None)
23/12/04 13:20:10 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:42739 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 42739, None)
23/12/04 13:20:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 42739, None)
23/12/04 13:20:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 42739, None)
23/12/04 13:20:11 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 13:20:11 INFO xnio: XNIO version 3.8.7.Final
23/12/04 13:20:11 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 13:20:11 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 13:20:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 13:20:12 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 13:20:33 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 13:20:33 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 13:20:33 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 13:20:33 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 13:20:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 13:20:33 INFO MemoryStore: MemoryStore cleared
23/12/04 13:20:33 INFO BlockManager: BlockManager stopped
23/12/04 13:20:33 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 13:20:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 13:20:33 INFO SparkContext: Successfully stopped SparkContext
23/12/04 13:20:33 INFO ShutdownHookManager: Shutdown hook called
23/12/04 13:20:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-a3d8ac15-0faa-4e7a-aa1b-67eb04dcbd80
23/12/04 17:07:30 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 17:07:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 17:07:30 INFO SparkContext: Running Spark version 3.5.0
23/12/04 17:07:30 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 17:07:30 INFO SparkContext: Java version 11.0.20
23/12/04 17:07:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 17:07:30 INFO ResourceUtils: ==============================================================
23/12/04 17:07:30 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 17:07:30 INFO ResourceUtils: ==============================================================
23/12/04 17:07:30 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 17:07:30 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 17:07:30 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 17:07:30 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 17:07:30 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 17:07:30 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 17:07:30 INFO SecurityManager: Changing view acls groups to: 
23/12/04 17:07:30 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 17:07:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 17:07:30 INFO Utils: Successfully started service 'sparkDriver' on port 34759.
23/12/04 17:07:30 INFO SparkEnv: Registering MapOutputTracker
23/12/04 17:07:30 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 17:07:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 17:07:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 17:07:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 17:07:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-52700768-6de2-4981-adf8-789b6b71844f
23/12/04 17:07:30 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 17:07:30 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 17:07:30 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 17:07:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 17:07:30 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 17:07:30 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 17:07:30 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 17:07:30 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 17:07:30 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 17:07:30 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 17:07:30 INFO Executor: Java version 11.0.20
23/12/04 17:07:30 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 17:07:30 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@75f8bfe0 for default.
23/12/04 17:07:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44607.
23/12/04 17:07:30 INFO NettyBlockTransferService: Server created on 10.25.86.80:44607
23/12/04 17:07:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 17:07:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 44607, None)
23/12/04 17:07:30 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:44607 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 44607, None)
23/12/04 17:07:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 44607, None)
23/12/04 17:07:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 44607, None)
23/12/04 17:07:31 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 17:07:31 INFO xnio: XNIO version 3.8.7.Final
23/12/04 17:07:31 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 17:07:31 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 17:07:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 17:07:32 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 17:08:36 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 17:08:36 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 17:08:36 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 17:08:36 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 17:08:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 17:08:36 INFO MemoryStore: MemoryStore cleared
23/12/04 17:08:36 INFO BlockManager: BlockManager stopped
23/12/04 17:08:36 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 17:08:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 17:08:36 INFO SparkContext: Successfully stopped SparkContext
23/12/04 17:08:36 INFO ShutdownHookManager: Shutdown hook called
23/12/04 17:08:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-962b50dd-2c22-428a-89b2-00122060928f
23/12/04 17:09:05 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 17:09:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 17:09:05 INFO SparkContext: Running Spark version 3.5.0
23/12/04 17:09:05 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 17:09:05 INFO SparkContext: Java version 11.0.20
23/12/04 17:09:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 17:09:05 INFO ResourceUtils: ==============================================================
23/12/04 17:09:05 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 17:09:05 INFO ResourceUtils: ==============================================================
23/12/04 17:09:05 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 17:09:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 17:09:05 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 17:09:05 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 17:09:05 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 17:09:05 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 17:09:05 INFO SecurityManager: Changing view acls groups to: 
23/12/04 17:09:05 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 17:09:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 17:09:06 INFO Utils: Successfully started service 'sparkDriver' on port 35991.
23/12/04 17:09:06 INFO SparkEnv: Registering MapOutputTracker
23/12/04 17:09:06 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 17:09:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 17:09:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 17:09:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 17:09:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a16441f1-e0c9-4119-9f07-cd78823b5d83
23/12/04 17:09:06 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 17:09:06 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 17:09:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 17:09:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 17:09:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 17:09:06 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 17:09:06 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 17:09:06 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 17:09:06 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 17:09:06 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 17:09:06 INFO Executor: Java version 11.0.20
23/12/04 17:09:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 17:09:06 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@49a4c601 for default.
23/12/04 17:09:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37147.
23/12/04 17:09:06 INFO NettyBlockTransferService: Server created on 10.25.86.80:37147
23/12/04 17:09:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 17:09:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 37147, None)
23/12/04 17:09:06 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:37147 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 37147, None)
23/12/04 17:09:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 37147, None)
23/12/04 17:09:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 37147, None)
23/12/04 17:09:06 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 17:09:06 INFO xnio: XNIO version 3.8.7.Final
23/12/04 17:09:06 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 17:09:06 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 17:09:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 17:09:08 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 17:09:36 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 17:09:36 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 17:09:36 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 17:09:36 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 17:09:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 17:09:36 INFO MemoryStore: MemoryStore cleared
23/12/04 17:09:36 INFO BlockManager: BlockManager stopped
23/12/04 17:09:36 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 17:09:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 17:09:36 INFO SparkContext: Successfully stopped SparkContext
23/12/04 17:09:36 INFO ShutdownHookManager: Shutdown hook called
23/12/04 17:09:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-38beb804-1291-4641-942c-a9fa311957d8
23/12/04 17:19:27 WARN RemoteEndpoint: Unmatched cancel notification for request id 29750
23/12/04 17:59:22 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 17:59:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 17:59:22 INFO SparkContext: Running Spark version 3.5.0
23/12/04 17:59:22 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 17:59:22 INFO SparkContext: Java version 11.0.20
23/12/04 17:59:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 17:59:23 INFO ResourceUtils: ==============================================================
23/12/04 17:59:23 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 17:59:23 INFO ResourceUtils: ==============================================================
23/12/04 17:59:23 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 17:59:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 17:59:23 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 17:59:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 17:59:23 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 17:59:23 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 17:59:23 INFO SecurityManager: Changing view acls groups to: 
23/12/04 17:59:23 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 17:59:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 17:59:23 INFO Utils: Successfully started service 'sparkDriver' on port 46169.
23/12/04 17:59:23 INFO SparkEnv: Registering MapOutputTracker
23/12/04 17:59:23 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 17:59:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 17:59:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 17:59:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 17:59:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3eb4af19-de61-4585-ac54-cf3a5d1527d2
23/12/04 17:59:23 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 17:59:23 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 17:59:23 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 17:59:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 17:59:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 17:59:23 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 17:59:23 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 17:59:23 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 17:59:23 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 17:59:23 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 17:59:23 INFO Executor: Java version 11.0.20
23/12/04 17:59:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 17:59:23 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7b709fc2 for default.
23/12/04 17:59:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33305.
23/12/04 17:59:23 INFO NettyBlockTransferService: Server created on 10.25.86.80:33305
23/12/04 17:59:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 17:59:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 33305, None)
23/12/04 17:59:23 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:33305 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 33305, None)
23/12/04 17:59:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 33305, None)
23/12/04 17:59:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 33305, None)
23/12/04 17:59:27 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 17:59:27 INFO xnio: XNIO version 3.8.7.Final
23/12/04 17:59:27 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 17:59:27 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 18:02:20 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 18:02:20 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 18:02:20 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 18:02:20 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 18:02:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 18:02:20 INFO MemoryStore: MemoryStore cleared
23/12/04 18:02:20 INFO BlockManager: BlockManager stopped
23/12/04 18:02:20 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 18:02:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 18:02:20 INFO SparkContext: Successfully stopped SparkContext
23/12/04 18:02:20 INFO ShutdownHookManager: Shutdown hook called
23/12/04 18:02:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-64d493ec-d3d6-4219-b36f-9eb9324a25c4
23/12/04 18:09:49 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 18:09:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 18:09:49 INFO SparkContext: Running Spark version 3.5.0
23/12/04 18:09:49 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 18:09:49 INFO SparkContext: Java version 11.0.20
23/12/04 18:09:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 18:09:49 INFO ResourceUtils: ==============================================================
23/12/04 18:09:49 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 18:09:49 INFO ResourceUtils: ==============================================================
23/12/04 18:09:49 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 18:09:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 18:09:50 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 18:09:50 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 18:09:50 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 18:09:50 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 18:09:50 INFO SecurityManager: Changing view acls groups to: 
23/12/04 18:09:50 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 18:09:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 18:09:50 INFO Utils: Successfully started service 'sparkDriver' on port 40933.
23/12/04 18:09:50 INFO SparkEnv: Registering MapOutputTracker
23/12/04 18:09:50 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 18:09:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 18:09:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 18:09:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 18:09:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-adfe21dd-49e8-4ac1-a626-dcbfaa1e574f
23/12/04 18:09:50 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 18:09:50 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 18:09:50 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 18:09:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 18:09:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 18:09:50 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 18:09:50 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 18:09:50 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 18:09:50 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 18:09:50 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 18:09:50 INFO Executor: Java version 11.0.20
23/12/04 18:09:50 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 18:09:50 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@213f2ecd for default.
23/12/04 18:09:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43641.
23/12/04 18:09:50 INFO NettyBlockTransferService: Server created on 10.25.86.80:43641
23/12/04 18:09:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 18:09:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 43641, None)
23/12/04 18:09:50 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:43641 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 43641, None)
23/12/04 18:09:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 43641, None)
23/12/04 18:09:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 43641, None)
23/12/04 18:09:52 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 18:09:52 INFO xnio: XNIO version 3.8.7.Final
23/12/04 18:09:52 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 18:09:52 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 18:10:51 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 18:10:51 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 18:10:51 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 18:10:51 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 18:10:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 18:10:51 INFO MemoryStore: MemoryStore cleared
23/12/04 18:10:51 INFO BlockManager: BlockManager stopped
23/12/04 18:10:51 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 18:10:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 18:10:51 INFO SparkContext: Successfully stopped SparkContext
23/12/04 18:10:51 INFO ShutdownHookManager: Shutdown hook called
23/12/04 18:10:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-dcb57d9a-30a1-4e5e-9e6c-f87a036554f7
23/12/04 18:12:24 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 18:12:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 18:12:24 INFO SparkContext: Running Spark version 3.5.0
23/12/04 18:12:24 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 18:12:24 INFO SparkContext: Java version 11.0.20
23/12/04 18:12:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 18:12:24 INFO ResourceUtils: ==============================================================
23/12/04 18:12:24 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 18:12:24 INFO ResourceUtils: ==============================================================
23/12/04 18:12:24 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 18:12:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 18:12:24 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 18:12:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 18:12:24 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 18:12:24 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 18:12:24 INFO SecurityManager: Changing view acls groups to: 
23/12/04 18:12:24 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 18:12:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 18:12:24 INFO Utils: Successfully started service 'sparkDriver' on port 35143.
23/12/04 18:12:24 INFO SparkEnv: Registering MapOutputTracker
23/12/04 18:12:24 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 18:12:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 18:12:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 18:12:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 18:12:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d3585e4b-7343-47e6-986f-b453684971ed
23/12/04 18:12:24 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 18:12:24 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 18:12:24 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 18:12:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 18:12:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 18:12:24 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 18:12:24 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 18:12:24 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 18:12:24 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 18:12:24 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 18:12:24 INFO Executor: Java version 11.0.20
23/12/04 18:12:24 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 18:12:24 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@432c4dc4 for default.
23/12/04 18:12:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41647.
23/12/04 18:12:24 INFO NettyBlockTransferService: Server created on 10.25.86.80:41647
23/12/04 18:12:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 18:12:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 41647, None)
23/12/04 18:12:24 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:41647 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 41647, None)
23/12/04 18:12:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 41647, None)
23/12/04 18:12:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 41647, None)
23/12/04 18:12:25 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 18:12:25 INFO xnio: XNIO version 3.8.7.Final
23/12/04 18:12:25 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 18:12:25 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 18:15:29 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 18:15:29 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 18:15:29 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 18:15:29 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 18:15:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 18:15:29 INFO MemoryStore: MemoryStore cleared
23/12/04 18:15:29 INFO BlockManager: BlockManager stopped
23/12/04 18:15:29 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 18:15:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 18:15:29 INFO SparkContext: Successfully stopped SparkContext
23/12/04 18:15:29 INFO ShutdownHookManager: Shutdown hook called
23/12/04 18:15:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-8a1e0796-e849-47ce-88e0-c96457458752
23/12/04 18:20:54 WARN RemoteEndpoint: Unmatched cancel notification for request id 31829
23/12/04 18:21:45 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 18:21:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 18:21:45 INFO SparkContext: Running Spark version 3.5.0
23/12/04 18:21:45 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 18:21:45 INFO SparkContext: Java version 11.0.20
23/12/04 18:21:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 18:21:46 INFO ResourceUtils: ==============================================================
23/12/04 18:21:46 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 18:21:46 INFO ResourceUtils: ==============================================================
23/12/04 18:21:46 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 18:21:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 18:21:46 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 18:21:46 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 18:21:46 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 18:21:46 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 18:21:46 INFO SecurityManager: Changing view acls groups to: 
23/12/04 18:21:46 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 18:21:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 18:21:46 INFO Utils: Successfully started service 'sparkDriver' on port 35053.
23/12/04 18:21:46 INFO SparkEnv: Registering MapOutputTracker
23/12/04 18:21:46 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 18:21:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 18:21:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 18:21:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 18:21:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c90d8e46-2ef9-41b6-a50a-e9f5d97a9305
23/12/04 18:21:47 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 18:21:47 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 18:21:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 18:21:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 18:21:47 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 18:21:47 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 18:21:47 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 18:21:47 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 18:21:47 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 18:21:47 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 18:21:47 INFO Executor: Java version 11.0.20
23/12/04 18:21:47 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 18:21:47 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@610c9937 for default.
23/12/04 18:21:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35193.
23/12/04 18:21:47 INFO NettyBlockTransferService: Server created on 10.25.86.80:35193
23/12/04 18:21:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 18:21:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 35193, None)
23/12/04 18:21:47 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:35193 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 35193, None)
23/12/04 18:21:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 35193, None)
23/12/04 18:21:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 35193, None)
23/12/04 18:21:50 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 18:21:50 INFO xnio: XNIO version 3.8.7.Final
23/12/04 18:21:50 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 18:21:50 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 18:23:25 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 18:23:57 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 18:23:57 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 18:23:57 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 18:23:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 18:23:57 INFO MemoryStore: MemoryStore cleared
23/12/04 18:23:57 INFO BlockManager: BlockManager stopped
23/12/04 18:23:57 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 18:23:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 18:23:57 INFO SparkContext: Successfully stopped SparkContext
23/12/04 18:23:57 INFO ShutdownHookManager: Shutdown hook called
23/12/04 18:23:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-cf073d27-6754-4b60-9e65-904db0d42788
23/12/04 18:24:20 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 18:24:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 18:24:20 INFO SparkContext: Running Spark version 3.5.0
23/12/04 18:24:20 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 18:24:20 INFO SparkContext: Java version 11.0.20
23/12/04 18:24:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 18:24:20 INFO ResourceUtils: ==============================================================
23/12/04 18:24:20 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 18:24:20 INFO ResourceUtils: ==============================================================
23/12/04 18:24:20 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 18:24:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 18:24:20 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 18:24:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 18:24:20 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 18:24:20 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 18:24:20 INFO SecurityManager: Changing view acls groups to: 
23/12/04 18:24:20 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 18:24:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 18:24:20 INFO Utils: Successfully started service 'sparkDriver' on port 37497.
23/12/04 18:24:20 INFO SparkEnv: Registering MapOutputTracker
23/12/04 18:24:20 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 18:24:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 18:24:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 18:24:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 18:24:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ae6fb167-1093-4071-8ef4-8f80de964f1f
23/12/04 18:24:21 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 18:24:21 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 18:24:21 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 18:24:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 18:24:21 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 18:24:21 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 18:24:21 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 18:24:21 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 18:24:21 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 18:24:21 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 18:24:21 INFO Executor: Java version 11.0.20
23/12/04 18:24:21 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 18:24:21 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4a619956 for default.
23/12/04 18:24:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39081.
23/12/04 18:24:21 INFO NettyBlockTransferService: Server created on 10.25.86.80:39081
23/12/04 18:24:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 18:24:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 39081, None)
23/12/04 18:24:21 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:39081 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 39081, None)
23/12/04 18:24:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 39081, None)
23/12/04 18:24:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 39081, None)
23/12/04 18:24:21 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 18:24:21 INFO xnio: XNIO version 3.8.7.Final
23/12/04 18:24:21 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 18:24:21 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 18:24:21 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 18:24:21 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 18:24:21 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 18:24:21 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 18:24:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 18:24:21 INFO MemoryStore: MemoryStore cleared
23/12/04 18:24:21 INFO BlockManager: BlockManager stopped
23/12/04 18:24:21 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 18:24:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 18:24:21 INFO SparkContext: Successfully stopped SparkContext
23/12/04 18:24:21 INFO ShutdownHookManager: Shutdown hook called
23/12/04 18:24:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-38601539-b558-4b11-9ce2-0ec89832a3a6
23/12/04 19:16:39 WARN RemoteEndpoint: Unmatched cancel notification for request id 33528
23/12/04 19:58:50 WARN RemoteEndpoint: Unmatched cancel notification for request id 34905
23/12/04 21:08:32 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 21:08:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 21:08:32 INFO SparkContext: Running Spark version 3.5.0
23/12/04 21:08:32 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 21:08:32 INFO SparkContext: Java version 11.0.20
23/12/04 21:08:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 21:08:32 INFO ResourceUtils: ==============================================================
23/12/04 21:08:32 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 21:08:32 INFO ResourceUtils: ==============================================================
23/12/04 21:08:32 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 21:08:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 21:08:32 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 21:08:32 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 21:08:32 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 21:08:32 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 21:08:32 INFO SecurityManager: Changing view acls groups to: 
23/12/04 21:08:32 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 21:08:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 21:08:32 INFO Utils: Successfully started service 'sparkDriver' on port 43475.
23/12/04 21:08:32 INFO SparkEnv: Registering MapOutputTracker
23/12/04 21:08:32 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 21:08:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 21:08:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 21:08:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 21:08:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-74e89f66-d253-4d1f-a6fe-6f2e3b7f4127
23/12/04 21:08:32 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 21:08:32 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 21:08:32 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 21:08:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 21:08:32 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 21:08:32 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 21:08:32 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 21:08:32 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 21:08:32 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 21:08:32 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 21:08:32 INFO Executor: Java version 11.0.20
23/12/04 21:08:32 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 21:08:32 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5dc0fa51 for default.
23/12/04 21:08:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44185.
23/12/04 21:08:32 INFO NettyBlockTransferService: Server created on 10.25.86.80:44185
23/12/04 21:08:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 21:08:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 44185, None)
23/12/04 21:08:32 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:44185 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 44185, None)
23/12/04 21:08:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 44185, None)
23/12/04 21:08:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 44185, None)
23/12/04 21:08:38 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 21:08:38 INFO xnio: XNIO version 3.8.7.Final
23/12/04 21:08:38 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 21:08:38 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 21:08:48 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 21:09:19 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 21:09:19 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 21:09:19 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 21:09:19 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 21:09:19 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 21:09:19 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 21:09:19 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 21:13:05 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 21:13:09 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 21:13:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 21:13:12 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 21:14:08 INFO CodeGenerator: Code generated in 117.603842 ms
23/12/04 21:14:08 INFO CodeGenerator: Code generated in 14.667392 ms
23/12/04 21:14:09 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:14:09 INFO DAGScheduler: Got job 0 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:14:09 INFO DAGScheduler: Final stage: ResultStage 0 (collect at Ast.scala:298)
23/12/04 21:14:09 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:14:09 INFO DAGScheduler: Missing parents: List()
23/12/04 21:14:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:14:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 21:14:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 21:14:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:44185 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:14:09 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/04 21:14:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:14:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/04 21:14:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 21:14:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/04 21:14:09 INFO CodeGenerator: Code generated in 10.08994 ms
23/12/04 21:14:09 INFO CodeGenerator: Code generated in 8.014042 ms
23/12/04 21:14:09 INFO CodeGenerator: Code generated in 34.359352 ms
23/12/04 21:14:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1869 bytes result sent to driver
23/12/04 21:14:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 739 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:14:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/04 21:14:09 INFO DAGScheduler: ResultStage 0 (collect at Ast.scala:298) finished in 0.838 s
23/12/04 21:14:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:14:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/04 21:14:21 INFO DAGScheduler: Job 0 finished: collect at Ast.scala:298, took 12.657655 s
23/12/04 21:14:21 INFO DAGScheduler: Registering RDD 8 (collect at Ast.scala:298) as input to shuffle 0
23/12/04 21:14:21 INFO DAGScheduler: Got map stage job 1 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:14:21 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at Ast.scala:298)
23/12/04 21:14:21 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:14:21 INFO DAGScheduler: Missing parents: List()
23/12/04 21:14:21 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:14:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 21:14:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 21:14:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.25.86.80:44185 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:14:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/12/04 21:14:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:14:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/12/04 21:14:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 21:14:21 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/12/04 21:14:21 INFO CodeGenerator: Code generated in 4.949503 ms
23/12/04 21:14:21 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1793 bytes result sent to driver
23/12/04 21:14:21 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 48 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:14:21 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/04 21:14:21 INFO DAGScheduler: ShuffleMapStage 1 (collect at Ast.scala:298) finished in 0.062 s
23/12/04 21:14:21 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:14:21 INFO DAGScheduler: running: HashSet()
23/12/04 21:14:21 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:14:21 INFO DAGScheduler: failed: HashSet()
23/12/04 21:14:21 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:14:21 INFO CodeGenerator: Code generated in 8.417677 ms
23/12/04 21:14:21 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:14:21 INFO DAGScheduler: Got job 2 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:14:21 INFO DAGScheduler: Final stage: ResultStage 3 (collect at Ast.scala:298)
23/12/04 21:14:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
23/12/04 21:14:21 INFO DAGScheduler: Missing parents: List()
23/12/04 21:14:21 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:14:21 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 21:14:21 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 21:14:21 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.25.86.80:44185 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:14:21 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
23/12/04 21:14:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:14:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
23/12/04 21:14:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:14:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
23/12/04 21:14:21 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
23/12/04 21:14:21 INFO CodeGenerator: Code generated in 8.172914 ms
23/12/04 21:14:21 INFO CodeGenerator: Code generated in 4.992356 ms
23/12/04 21:14:22 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4537 bytes result sent to driver
23/12/04 21:14:22 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 89 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:14:22 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
23/12/04 21:14:22 INFO DAGScheduler: ResultStage 3 (collect at Ast.scala:298) finished in 0.100 s
23/12/04 21:14:22 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:14:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
23/12/04 21:14:22 INFO DAGScheduler: Job 2 finished: collect at Ast.scala:298, took 0.109250 s
23/12/04 21:14:22 INFO CodeGenerator: Code generated in 7.3888 ms
23/12/04 21:14:25 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:14:25 INFO DAGScheduler: Got job 3 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:14:25 INFO DAGScheduler: Final stage: ResultStage 4 (collect at Ast.scala:298)
23/12/04 21:14:25 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:14:25 INFO DAGScheduler: Missing parents: List()
23/12/04 21:14:25 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:14:25 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 21:14:25 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 21:14:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.25.86.80:44185 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:14:25 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
23/12/04 21:14:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:14:25 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/12/04 21:14:25 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/04 21:14:25 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
23/12/04 21:14:25 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 1826 bytes result sent to driver
23/12/04 21:14:25 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:14:25 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/12/04 21:14:25 INFO DAGScheduler: ResultStage 4 (collect at Ast.scala:298) finished in 0.019 s
23/12/04 21:14:25 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:14:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/12/04 21:14:25 INFO DAGScheduler: Job 3 finished: collect at Ast.scala:298, took 0.022592 s
23/12/04 21:14:25 INFO DAGScheduler: Registering RDD 16 (collect at Ast.scala:298) as input to shuffle 1
23/12/04 21:14:25 INFO DAGScheduler: Got map stage job 4 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:14:25 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (collect at Ast.scala:298)
23/12/04 21:14:25 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:14:25 INFO DAGScheduler: Missing parents: List()
23/12/04 21:14:25 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:14:25 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 21:14:25 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 21:14:25 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.25.86.80:44185 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:14:25 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
23/12/04 21:14:25 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:14:25 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
23/12/04 21:14:25 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 21:14:25 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
23/12/04 21:14:25 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1793 bytes result sent to driver
23/12/04 21:14:25 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 17 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:14:25 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
23/12/04 21:14:25 INFO DAGScheduler: ShuffleMapStage 5 (collect at Ast.scala:298) finished in 0.024 s
23/12/04 21:14:25 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:14:25 INFO DAGScheduler: running: HashSet()
23/12/04 21:14:25 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:14:25 INFO DAGScheduler: failed: HashSet()
23/12/04 21:14:25 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:14:25 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:14:25 INFO DAGScheduler: Got job 5 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:14:25 INFO DAGScheduler: Final stage: ResultStage 7 (collect at Ast.scala:298)
23/12/04 21:14:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
23/12/04 21:14:25 INFO DAGScheduler: Missing parents: List()
23/12/04 21:14:25 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:14:25 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 21:14:25 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 21:14:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.25.86.80:44185 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:14:25 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
23/12/04 21:14:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:14:25 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
23/12/04 21:14:25 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:14:25 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)
23/12/04 21:14:25 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 21:14:25 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 4494 bytes result sent to driver
23/12/04 21:14:25 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 15 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:14:25 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
23/12/04 21:14:25 INFO DAGScheduler: ResultStage 7 (collect at Ast.scala:298) finished in 0.025 s
23/12/04 21:14:25 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:14:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
23/12/04 21:14:25 INFO DAGScheduler: Job 5 finished: collect at Ast.scala:298, took 0.028653 s
23/12/04 21:14:30 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:14:30 INFO DAGScheduler: Got job 6 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:14:30 INFO DAGScheduler: Final stage: ResultStage 8 (collect at Ast.scala:298)
23/12/04 21:14:30 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:14:30 INFO DAGScheduler: Missing parents: List()
23/12/04 21:14:30 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:14:30 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 21:14:30 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 21:14:30 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.25.86.80:44185 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:14:30 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
23/12/04 21:14:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:14:30 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
23/12/04 21:14:30 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 21:14:30 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)
23/12/04 21:14:30 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 1826 bytes result sent to driver
23/12/04 21:14:30 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:14:30 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
23/12/04 21:14:30 INFO DAGScheduler: ResultStage 8 (collect at Ast.scala:298) finished in 0.017 s
23/12/04 21:14:30 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:14:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
23/12/04 21:14:30 INFO DAGScheduler: Job 6 finished: collect at Ast.scala:298, took 0.019409 s
23/12/04 21:14:30 INFO DAGScheduler: Registering RDD 24 (collect at Ast.scala:298) as input to shuffle 2
23/12/04 21:14:30 INFO DAGScheduler: Got map stage job 7 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:14:30 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (collect at Ast.scala:298)
23/12/04 21:14:30 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:14:30 INFO DAGScheduler: Missing parents: List()
23/12/04 21:14:30 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:14:30 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 21:14:30 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 21:14:30 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.25.86.80:44185 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:14:30 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
23/12/04 21:14:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:14:30 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
23/12/04 21:14:30 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/04 21:14:30 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)
23/12/04 21:14:30 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 1793 bytes result sent to driver
23/12/04 21:14:30 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:14:30 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
23/12/04 21:14:30 INFO DAGScheduler: ShuffleMapStage 9 (collect at Ast.scala:298) finished in 0.020 s
23/12/04 21:14:30 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:14:30 INFO DAGScheduler: running: HashSet()
23/12/04 21:14:30 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:14:30 INFO DAGScheduler: failed: HashSet()
23/12/04 21:14:30 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:14:30 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:14:30 INFO DAGScheduler: Got job 8 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:14:30 INFO DAGScheduler: Final stage: ResultStage 11 (collect at Ast.scala:298)
23/12/04 21:14:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
23/12/04 21:14:30 INFO DAGScheduler: Missing parents: List()
23/12/04 21:14:30 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:14:30 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 21:14:30 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 21:14:30 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.25.86.80:44185 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:14:30 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
23/12/04 21:14:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:14:30 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
23/12/04 21:14:30 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:14:30 INFO Executor: Running task 0.0 in stage 11.0 (TID 8)
23/12/04 21:14:30 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 21:14:30 INFO Executor: Finished task 0.0 in stage 11.0 (TID 8). 4494 bytes result sent to driver
23/12/04 21:14:30 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:14:30 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
23/12/04 21:14:30 INFO DAGScheduler: ResultStage 11 (collect at Ast.scala:298) finished in 0.017 s
23/12/04 21:14:30 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:14:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
23/12/04 21:14:30 INFO DAGScheduler: Job 8 finished: collect at Ast.scala:298, took 0.021887 s
23/12/04 21:15:03 INFO InMemoryFileIndex: It took 17 ms to list leaf files for 1 paths.
23/12/04 21:15:03 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:15:03 INFO DAGScheduler: Got job 9 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:15:03 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at Ast.scala:324)
23/12/04 21:15:03 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:15:03 INFO DAGScheduler: Missing parents: List()
23/12/04 21:15:03 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:15:03 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:15:03 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:15:03 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.25.86.80:44185 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:15:03 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
23/12/04 21:15:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:15:03 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
23/12/04 21:15:03 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:15:03 INFO Executor: Running task 0.0 in stage 12.0 (TID 9)
23/12/04 21:15:03 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.25.86.80:44185 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:15:03 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.25.86.80:44185 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:15:03 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.25.86.80:44185 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:15:03 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.25.86.80:44185 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:15:03 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.25.86.80:44185 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:15:03 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.25.86.80:44185 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:15:03 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.25.86.80:44185 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:15:03 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.25.86.80:44185 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:15:03 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.25.86.80:44185 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:15:03 INFO Executor: Finished task 0.0 in stage 12.0 (TID 9). 2128 bytes result sent to driver
23/12/04 21:15:03 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 273 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:15:03 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
23/12/04 21:15:03 INFO DAGScheduler: ResultStage 12 (parquet at Ast.scala:324) finished in 0.283 s
23/12/04 21:15:03 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:15:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
23/12/04 21:15:03 INFO DAGScheduler: Job 9 finished: parquet at Ast.scala:324, took 0.285806 s
23/12/04 21:15:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/12/04 21:15:03 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:15:03 INFO DAGScheduler: Got job 10 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:15:03 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at Ast.scala:324)
23/12/04 21:15:03 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:15:03 INFO DAGScheduler: Missing parents: List()
23/12/04 21:15:03 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[31] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:15:03 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:15:03 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:15:03 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.25.86.80:44185 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:15:03 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580
23/12/04 21:15:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[31] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:15:03 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
23/12/04 21:15:03 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:15:03 INFO Executor: Running task 0.0 in stage 13.0 (TID 10)
23/12/04 21:15:03 INFO Executor: Finished task 0.0 in stage 13.0 (TID 10). 2042 bytes result sent to driver
23/12/04 21:15:03 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:15:03 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
23/12/04 21:15:03 INFO DAGScheduler: ResultStage 13 (parquet at Ast.scala:324) finished in 0.018 s
23/12/04 21:15:03 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:15:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
23/12/04 21:15:03 INFO DAGScheduler: Job 10 finished: parquet at Ast.scala:324, took 0.020337 s
23/12/04 21:15:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/12/04 21:15:03 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:15:03 INFO DAGScheduler: Got job 11 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:15:03 INFO DAGScheduler: Final stage: ResultStage 14 (parquet at Ast.scala:324)
23/12/04 21:15:03 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:15:03 INFO DAGScheduler: Missing parents: List()
23/12/04 21:15:03 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[33] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:15:03 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:15:03 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:15:03 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.25.86.80:44185 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:15:03 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580
23/12/04 21:15:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[33] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:15:03 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
23/12/04 21:15:03 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:15:03 INFO Executor: Running task 0.0 in stage 14.0 (TID 11)
23/12/04 21:15:03 INFO Executor: Finished task 0.0 in stage 14.0 (TID 11). 2042 bytes result sent to driver
23/12/04 21:15:03 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:15:03 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
23/12/04 21:15:03 INFO DAGScheduler: ResultStage 14 (parquet at Ast.scala:324) finished in 0.022 s
23/12/04 21:15:03 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:15:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
23/12/04 21:15:03 INFO DAGScheduler: Job 11 finished: parquet at Ast.scala:324, took 0.024250 s
23/12/04 21:15:16 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:15:16 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:15:16 INFO CodeGenerator: Code generated in 17.805573 ms
23/12/04 21:15:16 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:15:16 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:15:16 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.25.86.80:44185 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:15:16 INFO SparkContext: Created broadcast 12 from show at Ast.scala:95
23/12/04 21:15:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:15:16 INFO SparkContext: Starting job: show at Ast.scala:95
23/12/04 21:15:16 INFO DAGScheduler: Got job 12 (show at Ast.scala:95) with 1 output partitions
23/12/04 21:15:16 INFO DAGScheduler: Final stage: ResultStage 15 (show at Ast.scala:95)
23/12/04 21:15:16 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:15:16 INFO DAGScheduler: Missing parents: List()
23/12/04 21:15:16 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[37] at show at Ast.scala:95), which has no missing parents
23/12/04 21:15:16 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:15:16 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:15:16 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.25.86.80:44185 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:15:16 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580
23/12/04 21:15:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[37] at show at Ast.scala:95) (first 15 tasks are for partitions Vector(0))
23/12/04 21:15:16 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
23/12/04 21:15:16 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:15:16 INFO Executor: Running task 0.0 in stage 15.0 (TID 12)
23/12/04 21:15:16 INFO CodeGenerator: Code generated in 10.961952 ms
23/12/04 21:15:16 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:15:16 INFO CodecPool: Got brand-new decompressor [.snappy]
23/12/04 21:15:16 INFO Executor: Finished task 0.0 in stage 15.0 (TID 12). 1928 bytes result sent to driver
23/12/04 21:15:16 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 225 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:15:16 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
23/12/04 21:15:16 INFO DAGScheduler: ResultStage 15 (show at Ast.scala:95) finished in 0.239 s
23/12/04 21:15:16 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:15:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
23/12/04 21:15:16 INFO DAGScheduler: Job 12 finished: show at Ast.scala:95, took 0.243174 s
23/12/04 21:15:16 INFO CodeGenerator: Code generated in 7.5934 ms
23/12/04 21:15:18 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:15:18 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:15:18 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:15:18 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:15:18 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.25.86.80:44185 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:15:18 INFO SparkContext: Created broadcast 14 from show at Ast.scala:96
23/12/04 21:15:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:15:18 INFO SparkContext: Starting job: show at Ast.scala:96
23/12/04 21:15:18 INFO DAGScheduler: Got job 13 (show at Ast.scala:96) with 1 output partitions
23/12/04 21:15:18 INFO DAGScheduler: Final stage: ResultStage 16 (show at Ast.scala:96)
23/12/04 21:15:18 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:15:18 INFO DAGScheduler: Missing parents: List()
23/12/04 21:15:18 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[41] at show at Ast.scala:96), which has no missing parents
23/12/04 21:15:18 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:15:18 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:15:18 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.25.86.80:44185 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:15:18 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580
23/12/04 21:15:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[41] at show at Ast.scala:96) (first 15 tasks are for partitions Vector(0))
23/12/04 21:15:18 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
23/12/04 21:15:18 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 13) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:15:18 INFO Executor: Running task 0.0 in stage 16.0 (TID 13)
23/12/04 21:15:18 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:15:18 INFO Executor: Finished task 0.0 in stage 16.0 (TID 13). 1928 bytes result sent to driver
23/12/04 21:15:18 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 13) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:15:18 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
23/12/04 21:15:18 INFO DAGScheduler: ResultStage 16 (show at Ast.scala:96) finished in 0.016 s
23/12/04 21:15:18 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:15:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
23/12/04 21:15:18 INFO DAGScheduler: Job 13 finished: show at Ast.scala:96, took 0.019375 s
23/12/04 21:15:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:15:18 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:15:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:15:18 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 21:15:18 INFO CodeGenerator: Code generated in 6.936464 ms
23/12/04 21:15:18 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:15:18 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:15:18 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.25.86.80:44185 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:15:18 INFO SparkContext: Created broadcast 16 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:15:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:15:18 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:15:18 INFO DAGScheduler: Got job 14 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:15:18 INFO DAGScheduler: Final stage: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:15:18 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:15:18 INFO DAGScheduler: Missing parents: List()
23/12/04 21:15:18 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[45] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:15:18 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:15:18 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:15:18 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.25.86.80:44185 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:15:18 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580
23/12/04 21:15:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[45] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:15:18 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
23/12/04 21:15:18 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 14) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:15:18 INFO Executor: Running task 0.0 in stage 17.0 (TID 14)
23/12/04 21:15:18 INFO CodeGenerator: Code generated in 9.047301 ms
23/12/04 21:15:18 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:15:18 INFO Executor: Finished task 0.0 in stage 17.0 (TID 14). 2007 bytes result sent to driver
23/12/04 21:15:18 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 14) in 61 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:15:18 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
23/12/04 21:15:18 INFO DAGScheduler: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.064 s
23/12/04 21:15:18 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:15:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
23/12/04 21:15:18 INFO DAGScheduler: Job 14 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.065921 s
23/12/04 21:15:18 INFO CodeGenerator: Code generated in 4.543167 ms
23/12/04 21:15:18 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:15:18 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:15:18 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.25.86.80:44185 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:15:18 INFO SparkContext: Created broadcast 18 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:15:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:15:18 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:15:18 INFO CodeGenerator: Code generated in 9.432859 ms
23/12/04 21:15:18 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:15:18 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:15:18 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.25.86.80:44185 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:15:18 INFO SparkContext: Created broadcast 19 from show at Ast.scala:108
23/12/04 21:15:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:15:18 INFO SparkContext: Starting job: show at Ast.scala:108
23/12/04 21:15:18 INFO DAGScheduler: Got job 15 (show at Ast.scala:108) with 1 output partitions
23/12/04 21:15:18 INFO DAGScheduler: Final stage: ResultStage 18 (show at Ast.scala:108)
23/12/04 21:15:18 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:15:18 INFO DAGScheduler: Missing parents: List()
23/12/04 21:15:18 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[49] at show at Ast.scala:108), which has no missing parents
23/12/04 21:15:18 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 19.6 KiB, free 9.1 GiB)
23/12/04 21:15:18 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 9.1 GiB)
23/12/04 21:15:18 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.25.86.80:44185 (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 21:15:18 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1580
23/12/04 21:15:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[49] at show at Ast.scala:108) (first 15 tasks are for partitions Vector(0))
23/12/04 21:15:18 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
23/12/04 21:15:18 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 15) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:15:18 INFO Executor: Running task 0.0 in stage 18.0 (TID 15)
23/12/04 21:15:19 INFO CodeGenerator: Code generated in 14.513663 ms
23/12/04 21:15:19 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:15:19 INFO Executor: Finished task 0.0 in stage 18.0 (TID 15). 2118 bytes result sent to driver
23/12/04 21:15:19 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 15) in 29 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:15:19 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
23/12/04 21:15:19 INFO DAGScheduler: ResultStage 18 (show at Ast.scala:108) finished in 0.036 s
23/12/04 21:15:19 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:15:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
23/12/04 21:15:19 INFO DAGScheduler: Job 15 finished: show at Ast.scala:108, took 0.037532 s
23/12/04 21:15:26 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 21:15:26 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 21:15:26 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 21:15:26 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:15:26 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:15:26 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:15:26 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 21:15:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 21:15:26 INFO MemoryStore: MemoryStore cleared
23/12/04 21:15:26 INFO BlockManager: BlockManager stopped
23/12/04 21:15:26 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 21:15:26 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 21:15:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 21:15:26 INFO SparkContext: Successfully stopped SparkContext
23/12/04 21:15:26 INFO ShutdownHookManager: Shutdown hook called
23/12/04 21:15:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-7454b0a9-409f-406a-8f9f-240e8f05c3ea
23/12/04 21:15:59 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 21:15:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 21:15:59 INFO SparkContext: Running Spark version 3.5.0
23/12/04 21:15:59 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 21:15:59 INFO SparkContext: Java version 11.0.20
23/12/04 21:15:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 21:15:59 INFO ResourceUtils: ==============================================================
23/12/04 21:15:59 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 21:15:59 INFO ResourceUtils: ==============================================================
23/12/04 21:15:59 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 21:15:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 21:15:59 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 21:15:59 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 21:15:59 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 21:15:59 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 21:15:59 INFO SecurityManager: Changing view acls groups to: 
23/12/04 21:15:59 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 21:15:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 21:16:00 INFO Utils: Successfully started service 'sparkDriver' on port 34619.
23/12/04 21:16:00 INFO SparkEnv: Registering MapOutputTracker
23/12/04 21:16:00 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 21:16:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 21:16:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 21:16:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 21:16:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5f7afaa5-fef0-49ba-aefd-6b9d95c3fd93
23/12/04 21:16:00 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 21:16:00 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 21:16:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 21:16:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 21:16:00 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/12/04 21:16:00 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/12/04 21:16:00 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/12/04 21:16:00 INFO Utils: Successfully started service 'SparkUI' on port 4044.
23/12/04 21:16:00 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 21:16:00 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 21:16:00 INFO Executor: Java version 11.0.20
23/12/04 21:16:00 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 21:16:00 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@300e57ab for default.
23/12/04 21:16:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37571.
23/12/04 21:16:00 INFO NettyBlockTransferService: Server created on 10.25.86.80:37571
23/12/04 21:16:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 21:16:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 37571, None)
23/12/04 21:16:00 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:37571 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 37571, None)
23/12/04 21:16:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 37571, None)
23/12/04 21:16:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 37571, None)
23/12/04 21:16:00 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 21:16:00 INFO xnio: XNIO version 3.8.7.Final
23/12/04 21:16:00 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 21:16:00 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 21:16:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 21:16:00 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 21:16:02 INFO CodeGenerator: Code generated in 134.078539 ms
23/12/04 21:16:02 INFO CodeGenerator: Code generated in 13.550861 ms
23/12/04 21:16:02 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:16:02 INFO DAGScheduler: Got job 0 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:16:02 INFO DAGScheduler: Final stage: ResultStage 0 (collect at Ast.scala:298)
23/12/04 21:16:02 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:02 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:16:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 21:16:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 21:16:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:37571 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:16:02 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/04 21:16:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 21:16:02 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/04 21:16:02 INFO CodeGenerator: Code generated in 9.382042 ms
23/12/04 21:16:02 INFO CodeGenerator: Code generated in 7.56782 ms
23/12/04 21:16:03 INFO CodeGenerator: Code generated in 23.565545 ms
23/12/04 21:16:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1869 bytes result sent to driver
23/12/04 21:16:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 648 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/04 21:16:03 INFO DAGScheduler: ResultStage 0 (collect at Ast.scala:298) finished in 0.746 s
23/12/04 21:16:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/04 21:16:03 INFO DAGScheduler: Job 0 finished: collect at Ast.scala:298, took 0.770188 s
23/12/04 21:16:03 INFO DAGScheduler: Registering RDD 8 (collect at Ast.scala:298) as input to shuffle 0
23/12/04 21:16:03 INFO DAGScheduler: Got map stage job 1 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:16:03 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at Ast.scala:298)
23/12/04 21:16:03 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:03 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:03 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.25.86.80:37571 (size: 8.2 KiB, free: 9.1 GiB)
23/12/04 21:16:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:03 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/12/04 21:16:03 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 21:16:03 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/12/04 21:16:03 INFO CodeGenerator: Code generated in 5.003569 ms
23/12/04 21:16:03 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1836 bytes result sent to driver
23/12/04 21:16:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 49 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:03 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/04 21:16:03 INFO DAGScheduler: ShuffleMapStage 1 (collect at Ast.scala:298) finished in 0.064 s
23/12/04 21:16:03 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:16:03 INFO DAGScheduler: running: HashSet()
23/12/04 21:16:03 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:16:03 INFO DAGScheduler: failed: HashSet()
23/12/04 21:16:03 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:16:03 INFO CodeGenerator: Code generated in 8.238415 ms
23/12/04 21:16:03 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:16:03 INFO DAGScheduler: Got job 2 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:16:03 INFO DAGScheduler: Final stage: ResultStage 3 (collect at Ast.scala:298)
23/12/04 21:16:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
23/12/04 21:16:03 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:03 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.25.86.80:37571 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:16:03 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:03 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
23/12/04 21:16:03 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:16:03 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
23/12/04 21:16:03 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:16:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
23/12/04 21:16:03 INFO CodeGenerator: Code generated in 5.94959 ms
23/12/04 21:16:03 INFO CodeGenerator: Code generated in 5.397447 ms
23/12/04 21:16:03 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4537 bytes result sent to driver
23/12/04 21:16:03 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 84 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:03 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
23/12/04 21:16:03 INFO DAGScheduler: ResultStage 3 (collect at Ast.scala:298) finished in 0.095 s
23/12/04 21:16:03 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
23/12/04 21:16:03 INFO DAGScheduler: Job 2 finished: collect at Ast.scala:298, took 0.104239 s
23/12/04 21:16:03 INFO CodeGenerator: Code generated in 6.815677 ms
23/12/04 21:16:03 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:16:03 INFO DAGScheduler: Got job 3 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:16:03 INFO DAGScheduler: Final stage: ResultStage 4 (collect at Ast.scala:298)
23/12/04 21:16:03 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:03 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:03 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.25.86.80:37571 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:16:03 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:03 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/12/04 21:16:03 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/04 21:16:03 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
23/12/04 21:16:03 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 1826 bytes result sent to driver
23/12/04 21:16:03 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:03 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/12/04 21:16:03 INFO DAGScheduler: ResultStage 4 (collect at Ast.scala:298) finished in 0.019 s
23/12/04 21:16:03 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/12/04 21:16:03 INFO DAGScheduler: Job 3 finished: collect at Ast.scala:298, took 0.022341 s
23/12/04 21:16:03 INFO DAGScheduler: Registering RDD 16 (collect at Ast.scala:298) as input to shuffle 1
23/12/04 21:16:03 INFO DAGScheduler: Got map stage job 4 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:16:03 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (collect at Ast.scala:298)
23/12/04 21:16:03 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:03 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:03 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.25.86.80:37571 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:16:03 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:03 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
23/12/04 21:16:03 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 21:16:03 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
23/12/04 21:16:03 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1793 bytes result sent to driver
23/12/04 21:16:03 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 18 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:03 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
23/12/04 21:16:03 INFO DAGScheduler: ShuffleMapStage 5 (collect at Ast.scala:298) finished in 0.026 s
23/12/04 21:16:03 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:16:03 INFO DAGScheduler: running: HashSet()
23/12/04 21:16:03 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:16:03 INFO DAGScheduler: failed: HashSet()
23/12/04 21:16:03 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:16:03 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:16:03 INFO DAGScheduler: Got job 5 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:16:03 INFO DAGScheduler: Final stage: ResultStage 7 (collect at Ast.scala:298)
23/12/04 21:16:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
23/12/04 21:16:03 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:03 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.25.86.80:37571 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:16:03 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:03 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
23/12/04 21:16:03 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:16:03 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)
23/12/04 21:16:03 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:16:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 21:16:03 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 4494 bytes result sent to driver
23/12/04 21:16:03 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:03 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
23/12/04 21:16:03 INFO DAGScheduler: ResultStage 7 (collect at Ast.scala:298) finished in 0.020 s
23/12/04 21:16:03 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
23/12/04 21:16:03 INFO DAGScheduler: Job 5 finished: collect at Ast.scala:298, took 0.023658 s
23/12/04 21:16:03 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:16:03 INFO DAGScheduler: Got job 6 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:16:03 INFO DAGScheduler: Final stage: ResultStage 8 (collect at Ast.scala:298)
23/12/04 21:16:03 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:03 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:03 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.25.86.80:37571 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:16:03 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:03 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
23/12/04 21:16:03 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 21:16:03 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)
23/12/04 21:16:03 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 1826 bytes result sent to driver
23/12/04 21:16:03 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:03 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
23/12/04 21:16:03 INFO DAGScheduler: ResultStage 8 (collect at Ast.scala:298) finished in 0.016 s
23/12/04 21:16:03 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
23/12/04 21:16:03 INFO DAGScheduler: Job 6 finished: collect at Ast.scala:298, took 0.018418 s
23/12/04 21:16:03 INFO DAGScheduler: Registering RDD 24 (collect at Ast.scala:298) as input to shuffle 2
23/12/04 21:16:03 INFO DAGScheduler: Got map stage job 7 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:16:03 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (collect at Ast.scala:298)
23/12/04 21:16:03 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:03 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:03 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.25.86.80:37571 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:16:03 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:03 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
23/12/04 21:16:03 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/04 21:16:03 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)
23/12/04 21:16:03 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 1793 bytes result sent to driver
23/12/04 21:16:03 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:03 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
23/12/04 21:16:03 INFO DAGScheduler: ShuffleMapStage 9 (collect at Ast.scala:298) finished in 0.019 s
23/12/04 21:16:03 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:16:03 INFO DAGScheduler: running: HashSet()
23/12/04 21:16:03 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:16:03 INFO DAGScheduler: failed: HashSet()
23/12/04 21:16:03 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:16:03 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:16:03 INFO DAGScheduler: Got job 8 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:16:03 INFO DAGScheduler: Final stage: ResultStage 11 (collect at Ast.scala:298)
23/12/04 21:16:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
23/12/04 21:16:03 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:03 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 21:16:03 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.25.86.80:37571 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:16:03 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:03 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
23/12/04 21:16:03 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:16:03 INFO Executor: Running task 0.0 in stage 11.0 (TID 8)
23/12/04 21:16:03 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:16:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
23/12/04 21:16:03 INFO Executor: Finished task 0.0 in stage 11.0 (TID 8). 4494 bytes result sent to driver
23/12/04 21:16:03 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:03 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
23/12/04 21:16:03 INFO DAGScheduler: ResultStage 11 (collect at Ast.scala:298) finished in 0.020 s
23/12/04 21:16:03 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
23/12/04 21:16:03 INFO DAGScheduler: Job 8 finished: collect at Ast.scala:298, took 0.023220 s
23/12/04 21:16:03 INFO InMemoryFileIndex: It took 18 ms to list leaf files for 1 paths.
23/12/04 21:16:04 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:16:04 INFO DAGScheduler: Got job 9 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:16:04 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at Ast.scala:324)
23/12/04 21:16:04 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:04 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:04 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:16:04 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:16:04 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:16:04 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.25.86.80:37571 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:04 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
23/12/04 21:16:04 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:16:04 INFO Executor: Running task 0.0 in stage 12.0 (TID 9)
23/12/04 21:16:04 INFO Executor: Finished task 0.0 in stage 12.0 (TID 9). 2085 bytes result sent to driver
23/12/04 21:16:04 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 216 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:04 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
23/12/04 21:16:04 INFO DAGScheduler: ResultStage 12 (parquet at Ast.scala:324) finished in 0.231 s
23/12/04 21:16:04 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
23/12/04 21:16:04 INFO DAGScheduler: Job 9 finished: parquet at Ast.scala:324, took 0.233413 s
23/12/04 21:16:04 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/12/04 21:16:04 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.25.86.80:37571 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.25.86.80:37571 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.25.86.80:37571 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.25.86.80:37571 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.25.86.80:37571 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:16:04 INFO DAGScheduler: Got job 10 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:16:04 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at Ast.scala:324)
23/12/04 21:16:04 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:04 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:04 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[31] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:16:04 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.25.86.80:37571 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.25.86.80:37571 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:16:04 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:16:04 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.25.86.80:37571 in memory (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.25.86.80:37571 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[31] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:04 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
23/12/04 21:16:04 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.25.86.80:37571 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:16:04 INFO Executor: Running task 0.0 in stage 13.0 (TID 10)
23/12/04 21:16:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.25.86.80:37571 in memory (size: 8.2 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO Executor: Finished task 0.0 in stage 13.0 (TID 10). 2042 bytes result sent to driver
23/12/04 21:16:04 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 16 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:04 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
23/12/04 21:16:04 INFO DAGScheduler: ResultStage 13 (parquet at Ast.scala:324) finished in 0.030 s
23/12/04 21:16:04 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
23/12/04 21:16:04 INFO DAGScheduler: Job 10 finished: parquet at Ast.scala:324, took 0.032360 s
23/12/04 21:16:04 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/12/04 21:16:04 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:16:04 INFO DAGScheduler: Got job 11 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:16:04 INFO DAGScheduler: Final stage: ResultStage 14 (parquet at Ast.scala:324)
23/12/04 21:16:04 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:04 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:04 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[33] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:16:04 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:16:04 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:16:04 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.25.86.80:37571 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[33] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:04 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
23/12/04 21:16:04 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:16:04 INFO Executor: Running task 0.0 in stage 14.0 (TID 11)
23/12/04 21:16:04 INFO Executor: Finished task 0.0 in stage 14.0 (TID 11). 2042 bytes result sent to driver
23/12/04 21:16:04 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:04 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
23/12/04 21:16:04 INFO DAGScheduler: ResultStage 14 (parquet at Ast.scala:324) finished in 0.018 s
23/12/04 21:16:04 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
23/12/04 21:16:04 INFO DAGScheduler: Job 11 finished: parquet at Ast.scala:324, took 0.021504 s
23/12/04 21:16:04 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:16:04 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:16:04 INFO CodeGenerator: Code generated in 14.277591 ms
23/12/04 21:16:04 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:16:04 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:16:04 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.25.86.80:37571 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO SparkContext: Created broadcast 12 from show at Ast.scala:95
23/12/04 21:16:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:16:04 INFO SparkContext: Starting job: show at Ast.scala:95
23/12/04 21:16:04 INFO DAGScheduler: Got job 12 (show at Ast.scala:95) with 1 output partitions
23/12/04 21:16:04 INFO DAGScheduler: Final stage: ResultStage 15 (show at Ast.scala:95)
23/12/04 21:16:04 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:04 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:04 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[37] at show at Ast.scala:95), which has no missing parents
23/12/04 21:16:04 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:16:04 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:16:04 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.25.86.80:37571 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[37] at show at Ast.scala:95) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:04 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
23/12/04 21:16:04 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:16:04 INFO Executor: Running task 0.0 in stage 15.0 (TID 12)
23/12/04 21:16:04 INFO CodeGenerator: Code generated in 10.502382 ms
23/12/04 21:16:04 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:16:04 INFO CodecPool: Got brand-new decompressor [.snappy]
23/12/04 21:16:04 INFO Executor: Finished task 0.0 in stage 15.0 (TID 12). 1928 bytes result sent to driver
23/12/04 21:16:04 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 208 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:04 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
23/12/04 21:16:04 INFO DAGScheduler: ResultStage 15 (show at Ast.scala:95) finished in 0.222 s
23/12/04 21:16:04 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
23/12/04 21:16:04 INFO DAGScheduler: Job 12 finished: show at Ast.scala:95, took 0.225277 s
23/12/04 21:16:04 INFO CodeGenerator: Code generated in 7.155957 ms
23/12/04 21:16:04 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:16:04 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:16:04 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:16:04 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:16:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.25.86.80:37571 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO SparkContext: Created broadcast 14 from show at Ast.scala:96
23/12/04 21:16:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:16:04 INFO SparkContext: Starting job: show at Ast.scala:96
23/12/04 21:16:04 INFO DAGScheduler: Got job 13 (show at Ast.scala:96) with 1 output partitions
23/12/04 21:16:04 INFO DAGScheduler: Final stage: ResultStage 16 (show at Ast.scala:96)
23/12/04 21:16:04 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:04 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:04 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[41] at show at Ast.scala:96), which has no missing parents
23/12/04 21:16:04 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:16:04 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:16:04 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.25.86.80:37571 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:16:04 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[41] at show at Ast.scala:96) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:04 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
23/12/04 21:16:04 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 13) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:16:04 INFO Executor: Running task 0.0 in stage 16.0 (TID 13)
23/12/04 21:16:04 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:16:04 INFO Executor: Finished task 0.0 in stage 16.0 (TID 13). 1928 bytes result sent to driver
23/12/04 21:16:04 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 13) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:04 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
23/12/04 21:16:04 INFO DAGScheduler: ResultStage 16 (show at Ast.scala:96) finished in 0.016 s
23/12/04 21:16:04 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
23/12/04 21:16:04 INFO DAGScheduler: Job 13 finished: show at Ast.scala:96, took 0.019400 s
23/12/04 21:16:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:16:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:16:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:16:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 9.22064 ms
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.25.86.80:37571 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 16 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:16:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:16:05 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:16:05 INFO DAGScheduler: Got job 14 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:16:05 INFO DAGScheduler: Final stage: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:16:05 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:05 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:05 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[45] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.25.86.80:37571 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[45] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:05 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
23/12/04 21:16:05 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 14) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:16:05 INFO Executor: Running task 0.0 in stage 17.0 (TID 14)
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 9.836835 ms
23/12/04 21:16:05 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:16:05 INFO Executor: Finished task 0.0 in stage 17.0 (TID 14). 2007 bytes result sent to driver
23/12/04 21:16:05 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 14) in 88 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:05 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
23/12/04 21:16:05 INFO DAGScheduler: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.092 s
23/12/04 21:16:05 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
23/12/04 21:16:05 INFO DAGScheduler: Job 14 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.093754 s
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 5.331921 ms
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.25.86.80:37571 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 18 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:16:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:16:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 14.960908 ms
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.25.86.80:37571 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 19 from show at Ast.scala:108
23/12/04 21:16:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:16:05 INFO SparkContext: Starting job: show at Ast.scala:108
23/12/04 21:16:05 INFO DAGScheduler: Got job 15 (show at Ast.scala:108) with 1 output partitions
23/12/04 21:16:05 INFO DAGScheduler: Final stage: ResultStage 18 (show at Ast.scala:108)
23/12/04 21:16:05 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:05 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:05 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[49] at show at Ast.scala:108), which has no missing parents
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 19.6 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.25.86.80:37571 (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[49] at show at Ast.scala:108) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:05 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
23/12/04 21:16:05 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 15) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:16:05 INFO Executor: Running task 0.0 in stage 18.0 (TID 15)
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 13.774446 ms
23/12/04 21:16:05 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:16:05 INFO Executor: Finished task 0.0 in stage 18.0 (TID 15). 2118 bytes result sent to driver
23/12/04 21:16:05 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 15) in 29 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:05 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
23/12/04 21:16:05 INFO DAGScheduler: ResultStage 18 (show at Ast.scala:108) finished in 0.036 s
23/12/04 21:16:05 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
23/12/04 21:16:05 INFO DAGScheduler: Job 15 finished: show at Ast.scala:108, took 0.038822 s
23/12/04 21:16:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:16:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:16:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:16:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.25.86.80:37571 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 21 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:16:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:16:05 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:16:05 INFO DAGScheduler: Got job 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:16:05 INFO DAGScheduler: Final stage: ResultStage 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:16:05 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:05 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:05 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[53] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.25.86.80:37571 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[53] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:05 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
23/12/04 21:16:05 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 16) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:16:05 INFO Executor: Running task 0.0 in stage 19.0 (TID 16)
23/12/04 21:16:05 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:16:05 INFO Executor: Finished task 0.0 in stage 19.0 (TID 16). 2007 bytes result sent to driver
23/12/04 21:16:05 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 16) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:05 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
23/12/04 21:16:05 INFO DAGScheduler: ResultStage 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.015 s
23/12/04 21:16:05 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
23/12/04 21:16:05 INFO DAGScheduler: Job 16 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.017173 s
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.25.86.80:37571 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 23 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:16:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:16:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.25.86.80:37571 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 24 from show at Ast.scala:112
23/12/04 21:16:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:16:05 INFO SparkContext: Starting job: show at Ast.scala:112
23/12/04 21:16:05 INFO DAGScheduler: Got job 17 (show at Ast.scala:112) with 1 output partitions
23/12/04 21:16:05 INFO DAGScheduler: Final stage: ResultStage 20 (show at Ast.scala:112)
23/12/04 21:16:05 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:05 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:05 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[57] at show at Ast.scala:112), which has no missing parents
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 19.6 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.25.86.80:37571 (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[57] at show at Ast.scala:112) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:05 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
23/12/04 21:16:05 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 17) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:16:05 INFO Executor: Running task 0.0 in stage 20.0 (TID 17)
23/12/04 21:16:05 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:16:05 INFO Executor: Finished task 0.0 in stage 20.0 (TID 17). 2118 bytes result sent to driver
23/12/04 21:16:05 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 17) in 8 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:05 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
23/12/04 21:16:05 INFO DAGScheduler: ResultStage 20 (show at Ast.scala:112) finished in 0.013 s
23/12/04 21:16:05 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
23/12/04 21:16:05 INFO DAGScheduler: Job 17 finished: show at Ast.scala:112, took 0.014477 s
23/12/04 21:16:05 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:16:05 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.25.86.80:37571 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 26 from show at Ast.scala:113
23/12/04 21:16:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:16:05 INFO SparkContext: Starting job: show at Ast.scala:113
23/12/04 21:16:05 INFO DAGScheduler: Got job 18 (show at Ast.scala:113) with 1 output partitions
23/12/04 21:16:05 INFO DAGScheduler: Final stage: ResultStage 21 (show at Ast.scala:113)
23/12/04 21:16:05 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:05 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:05 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[61] at show at Ast.scala:113), which has no missing parents
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.25.86.80:37571 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[61] at show at Ast.scala:113) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:05 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
23/12/04 21:16:05 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 18) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:16:05 INFO Executor: Running task 0.0 in stage 21.0 (TID 18)
23/12/04 21:16:05 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:16:05 INFO Executor: Finished task 0.0 in stage 21.0 (TID 18). 1928 bytes result sent to driver
23/12/04 21:16:05 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 18) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:05 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
23/12/04 21:16:05 INFO DAGScheduler: ResultStage 21 (show at Ast.scala:113) finished in 0.014 s
23/12/04 21:16:05 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
23/12/04 21:16:05 INFO DAGScheduler: Job 18 finished: show at Ast.scala:113, took 0.015413 s
23/12/04 21:16:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:16:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:16:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:16:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 21:16:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:16:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#55)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.25.86.80:37571 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 28 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:16:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:16:05 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:16:05 INFO DAGScheduler: Got job 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:16:05 INFO DAGScheduler: Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:16:05 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:05 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:05 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[65] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.25.86.80:37571 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[65] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:05 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
23/12/04 21:16:05 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 19) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:16:05 INFO Executor: Running task 0.0 in stage 22.0 (TID 19)
23/12/04 21:16:05 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:16:05 INFO Executor: Finished task 0.0 in stage 22.0 (TID 19). 2007 bytes result sent to driver
23/12/04 21:16:05 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 19) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:05 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
23/12/04 21:16:05 INFO DAGScheduler: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.016 s
23/12/04 21:16:05 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
23/12/04 21:16:05 INFO DAGScheduler: Job 19 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.017736 s
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.25.86.80:37571 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 30 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:16:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:16:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 10.689663 ms
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.25.86.80:37571 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 31 from show at Ast.scala:126
23/12/04 21:16:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:16:05 INFO SparkContext: Starting job: show at Ast.scala:126
23/12/04 21:16:05 INFO DAGScheduler: Got job 20 (show at Ast.scala:126) with 1 output partitions
23/12/04 21:16:05 INFO DAGScheduler: Final stage: ResultStage 23 (show at Ast.scala:126)
23/12/04 21:16:05 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:05 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:05 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[69] at show at Ast.scala:126), which has no missing parents
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 22.4 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.25.86.80:37571 (size: 8.6 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[69] at show at Ast.scala:126) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:05 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
23/12/04 21:16:05 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 20) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:16:05 INFO Executor: Running task 0.0 in stage 23.0 (TID 20)
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 10.939118 ms
23/12/04 21:16:05 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:16:05 INFO Executor: Finished task 0.0 in stage 23.0 (TID 20). 2172 bytes result sent to driver
23/12/04 21:16:05 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 20) in 22 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:05 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
23/12/04 21:16:05 INFO DAGScheduler: ResultStage 23 (show at Ast.scala:126) finished in 0.026 s
23/12/04 21:16:05 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
23/12/04 21:16:05 INFO DAGScheduler: Job 20 finished: show at Ast.scala:126, took 0.028314 s
23/12/04 21:16:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:16:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:16:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:16:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 21:16:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:16:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#55)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.25.86.80:37571 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 33 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:16:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:16:05 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:16:05 INFO DAGScheduler: Got job 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:16:05 INFO DAGScheduler: Final stage: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:16:05 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:05 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:05 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[73] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.25.86.80:37571 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[73] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:05 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
23/12/04 21:16:05 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 21) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:16:05 INFO Executor: Running task 0.0 in stage 24.0 (TID 21)
23/12/04 21:16:05 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:16:05 INFO Executor: Finished task 0.0 in stage 24.0 (TID 21). 2007 bytes result sent to driver
23/12/04 21:16:05 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 21) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:05 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
23/12/04 21:16:05 INFO DAGScheduler: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.015 s
23/12/04 21:16:05 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
23/12/04 21:16:05 INFO DAGScheduler: Job 21 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.016260 s
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.25.86.80:37571 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 35 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:16:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:16:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 8.88628 ms
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.25.86.80:37571 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 36 from collect at Ast.scala:298
23/12/04 21:16:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 3.993225 ms
23/12/04 21:16:05 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:16:05 INFO DAGScheduler: Got job 22 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:16:05 INFO DAGScheduler: Final stage: ResultStage 25 (collect at Ast.scala:298)
23/12/04 21:16:05 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:05 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:05 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[79] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 25.1 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.25.86.80:37571 (size: 9.8 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[79] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:05 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
23/12/04 21:16:05 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 22) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:16:05 INFO Executor: Running task 0.0 in stage 25.0 (TID 22)
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 9.979142 ms
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 2.932784 ms
23/12/04 21:16:05 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:16:05 INFO Executor: Finished task 0.0 in stage 25.0 (TID 22). 2331 bytes result sent to driver
23/12/04 21:16:05 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 22) in 23 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:05 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
23/12/04 21:16:05 INFO DAGScheduler: ResultStage 25 (collect at Ast.scala:298) finished in 0.026 s
23/12/04 21:16:05 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
23/12/04 21:16:05 INFO DAGScheduler: Job 22 finished: collect at Ast.scala:298, took 0.028032 s
23/12/04 21:16:05 INFO DAGScheduler: Registering RDD 80 (collect at Ast.scala:298) as input to shuffle 3
23/12/04 21:16:05 INFO DAGScheduler: Got map stage job 23 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:16:05 INFO DAGScheduler: Final stage: ShuffleMapStage 26 (collect at Ast.scala:298)
23/12/04 21:16:05 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:16:05 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:05 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[80] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 27.2 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.25.86.80:37571 (size: 10.9 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[80] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:05 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
23/12/04 21:16:05 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 23) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8468 bytes) 
23/12/04 21:16:05 INFO Executor: Running task 0.0 in stage 26.0 (TID 23)
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 3.593763 ms
23/12/04 21:16:05 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:16:05 INFO Executor: Finished task 0.0 in stage 26.0 (TID 23). 2298 bytes result sent to driver
23/12/04 21:16:05 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 23) in 16 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:05 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
23/12/04 21:16:05 INFO DAGScheduler: ShuffleMapStage 26 (collect at Ast.scala:298) finished in 0.021 s
23/12/04 21:16:05 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:16:05 INFO DAGScheduler: running: HashSet()
23/12/04 21:16:05 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:16:05 INFO DAGScheduler: failed: HashSet()
23/12/04 21:16:05 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 6.786109 ms
23/12/04 21:16:05 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:16:05 INFO DAGScheduler: Got job 24 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:16:05 INFO DAGScheduler: Final stage: ResultStage 28 (collect at Ast.scala:298)
23/12/04 21:16:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
23/12/04 21:16:05 INFO DAGScheduler: Missing parents: List()
23/12/04 21:16:05 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[83] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 38.1 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 9.1 GiB)
23/12/04 21:16:05 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.25.86.80:37571 (size: 16.4 KiB, free: 9.1 GiB)
23/12/04 21:16:05 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1580
23/12/04 21:16:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[83] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:16:05 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
23/12/04 21:16:05 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 24) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:16:05 INFO Executor: Running task 0.0 in stage 28.0 (TID 24)
23/12/04 21:16:05 INFO ShuffleBlockFetcherIterator: Getting 1 (432.0 B) non-empty blocks including 1 (432.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:16:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 5.463367 ms
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 3.057424 ms
23/12/04 21:16:05 INFO Executor: Finished task 0.0 in stage 28.0 (TID 24). 5956 bytes result sent to driver
23/12/04 21:16:05 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 24) in 18 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:16:05 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
23/12/04 21:16:05 INFO DAGScheduler: ResultStage 28 (collect at Ast.scala:298) finished in 0.023 s
23/12/04 21:16:05 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:16:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
23/12/04 21:16:05 INFO DAGScheduler: Job 24 finished: collect at Ast.scala:298, took 0.025038 s
23/12/04 21:16:05 INFO CodeGenerator: Code generated in 4.961455 ms
23/12/04 21:16:06 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 21:16:06 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 21:16:06 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 21:16:06 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4044
23/12/04 21:16:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 21:16:06 INFO MemoryStore: MemoryStore cleared
23/12/04 21:16:06 INFO BlockManager: BlockManager stopped
23/12/04 21:16:06 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 21:16:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 21:16:06 INFO SparkContext: Successfully stopped SparkContext
23/12/04 21:16:06 INFO ShutdownHookManager: Shutdown hook called
23/12/04 21:16:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-734252af-29fd-46c5-9122-736b9ff70181
23/12/04 21:18:57 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 21:18:57 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 21:18:57 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 21:18:57 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 21:18:57 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 21:18:57 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 21:18:57 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4042
23/12/04 21:18:57 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4043
23/12/04 21:18:57 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4041
23/12/04 21:18:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 21:18:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 21:18:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 21:18:57 INFO MemoryStore: MemoryStore cleared
23/12/04 21:18:57 INFO MemoryStore: MemoryStore cleared
23/12/04 21:18:57 INFO BlockManager: BlockManager stopped
23/12/04 21:18:57 INFO BlockManager: BlockManager stopped
23/12/04 21:18:57 INFO MemoryStore: MemoryStore cleared
23/12/04 21:18:57 INFO BlockManager: BlockManager stopped
23/12/04 21:18:57 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 21:18:57 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 21:18:57 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 21:18:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 21:18:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 21:18:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 21:18:57 INFO SparkContext: Successfully stopped SparkContext
23/12/04 21:18:57 INFO ShutdownHookManager: Shutdown hook called
23/12/04 21:18:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-283c8958-d1dd-4aee-abd7-d7fb50e4f36e
23/12/04 21:18:57 INFO SparkContext: Successfully stopped SparkContext
23/12/04 21:18:57 INFO SparkContext: Successfully stopped SparkContext
23/12/04 21:18:57 INFO ShutdownHookManager: Shutdown hook called
23/12/04 21:18:57 INFO ShutdownHookManager: Shutdown hook called
23/12/04 21:18:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-c19eda68-048a-439e-894a-6367a3d64d17
23/12/04 21:18:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-bf7bf02b-fade-49b6-a524-880355d75372
23/12/04 21:20:04 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 21:20:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 21:20:04 INFO SparkContext: Running Spark version 3.5.0
23/12/04 21:20:04 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 21:20:04 INFO SparkContext: Java version 11.0.20
23/12/04 21:20:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 21:20:04 INFO ResourceUtils: ==============================================================
23/12/04 21:20:04 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 21:20:04 INFO ResourceUtils: ==============================================================
23/12/04 21:20:04 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 21:20:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 21:20:04 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 21:20:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 21:20:04 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 21:20:04 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 21:20:04 INFO SecurityManager: Changing view acls groups to: 
23/12/04 21:20:04 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 21:20:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 21:20:04 INFO Utils: Successfully started service 'sparkDriver' on port 35051.
23/12/04 21:20:04 INFO SparkEnv: Registering MapOutputTracker
23/12/04 21:20:04 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 21:20:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 21:20:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 21:20:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 21:20:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a8c59ea8-aa18-4b74-b082-b10a9c356a65
23/12/04 21:20:04 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 21:20:04 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 21:20:04 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 21:20:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/12/04 21:20:04 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 21:20:04 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 21:20:04 INFO Executor: Java version 11.0.20
23/12/04 21:20:04 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 21:20:04 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@780a094 for default.
23/12/04 21:20:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42169.
23/12/04 21:20:04 INFO NettyBlockTransferService: Server created on 10.25.86.80:42169
23/12/04 21:20:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 21:20:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 42169, None)
23/12/04 21:20:04 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:42169 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 42169, None)
23/12/04 21:20:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 42169, None)
23/12/04 21:20:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 42169, None)
23/12/04 21:20:21 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 21:20:21 INFO xnio: XNIO version 3.8.7.Final
23/12/04 21:20:21 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 21:20:21 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 21:20:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 21:20:25 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 21:21:00 INFO CodeGenerator: Code generated in 141.828199 ms
23/12/04 21:21:00 INFO CodeGenerator: Code generated in 12.138234 ms
23/12/04 21:21:00 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:21:00 INFO DAGScheduler: Got job 0 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:21:00 INFO DAGScheduler: Final stage: ResultStage 0 (collect at Ast.scala:298)
23/12/04 21:21:00 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:21:00 INFO DAGScheduler: Missing parents: List()
23/12/04 21:21:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:21:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 21:21:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 21:21:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:42169 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:21:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/04 21:21:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:21:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/04 21:21:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 21:21:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/04 21:21:00 INFO CodeGenerator: Code generated in 7.589942 ms
23/12/04 21:21:00 INFO CodeGenerator: Code generated in 9.196401 ms
23/12/04 21:21:01 INFO CodeGenerator: Code generated in 28.007694 ms
23/12/04 21:21:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1869 bytes result sent to driver
23/12/04 21:21:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 671 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:21:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/04 21:21:01 INFO DAGScheduler: ResultStage 0 (collect at Ast.scala:298) finished in 0.776 s
23/12/04 21:21:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:21:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/04 21:21:01 INFO DAGScheduler: Job 0 finished: collect at Ast.scala:298, took 0.811473 s
23/12/04 21:21:01 INFO DAGScheduler: Registering RDD 8 (collect at Ast.scala:298) as input to shuffle 0
23/12/04 21:21:01 INFO DAGScheduler: Got map stage job 1 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:21:01 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at Ast.scala:298)
23/12/04 21:21:01 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:21:01 INFO DAGScheduler: Missing parents: List()
23/12/04 21:21:01 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:21:01 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 21:21:01 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 21:21:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.25.86.80:42169 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:21:01 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/12/04 21:21:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:21:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/12/04 21:21:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 21:21:01 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/12/04 21:21:01 INFO CodeGenerator: Code generated in 7.595581 ms
23/12/04 21:21:01 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1793 bytes result sent to driver
23/12/04 21:21:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 59 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:21:01 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/04 21:21:01 INFO DAGScheduler: ShuffleMapStage 1 (collect at Ast.scala:298) finished in 0.075 s
23/12/04 21:21:01 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:21:01 INFO DAGScheduler: running: HashSet()
23/12/04 21:21:01 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:21:01 INFO DAGScheduler: failed: HashSet()
23/12/04 21:21:01 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:21:01 INFO CodeGenerator: Code generated in 10.396012 ms
23/12/04 21:21:01 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:21:01 INFO DAGScheduler: Got job 2 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:21:01 INFO DAGScheduler: Final stage: ResultStage 3 (collect at Ast.scala:298)
23/12/04 21:21:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
23/12/04 21:21:01 INFO DAGScheduler: Missing parents: List()
23/12/04 21:21:01 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:21:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 21:21:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 21:21:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.25.86.80:42169 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:21:01 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
23/12/04 21:21:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:21:01 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
23/12/04 21:21:01 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:21:01 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
23/12/04 21:21:01 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:21:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
23/12/04 21:21:01 INFO CodeGenerator: Code generated in 7.133869 ms
23/12/04 21:21:01 INFO CodeGenerator: Code generated in 5.535738 ms
23/12/04 21:21:01 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4537 bytes result sent to driver
23/12/04 21:21:01 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 86 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:21:01 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
23/12/04 21:21:01 INFO DAGScheduler: ResultStage 3 (collect at Ast.scala:298) finished in 0.097 s
23/12/04 21:21:01 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:21:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
23/12/04 21:21:01 INFO DAGScheduler: Job 2 finished: collect at Ast.scala:298, took 0.105038 s
23/12/04 21:21:01 INFO CodeGenerator: Code generated in 9.109692 ms
23/12/04 21:21:01 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:21:01 INFO DAGScheduler: Got job 3 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:21:01 INFO DAGScheduler: Final stage: ResultStage 4 (collect at Ast.scala:298)
23/12/04 21:21:01 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:21:01 INFO DAGScheduler: Missing parents: List()
23/12/04 21:21:01 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:21:01 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 21:21:01 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 21:21:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.25.86.80:42169 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:21:01 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
23/12/04 21:21:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:21:01 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/12/04 21:21:01 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/04 21:21:01 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
23/12/04 21:21:01 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 1826 bytes result sent to driver
23/12/04 21:21:01 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 15 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:21:01 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/12/04 21:21:01 INFO DAGScheduler: ResultStage 4 (collect at Ast.scala:298) finished in 0.023 s
23/12/04 21:21:01 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:21:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/12/04 21:21:01 INFO DAGScheduler: Job 3 finished: collect at Ast.scala:298, took 0.026169 s
23/12/04 21:21:01 INFO DAGScheduler: Registering RDD 16 (collect at Ast.scala:298) as input to shuffle 1
23/12/04 21:21:01 INFO DAGScheduler: Got map stage job 4 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:21:01 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (collect at Ast.scala:298)
23/12/04 21:21:01 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:21:01 INFO DAGScheduler: Missing parents: List()
23/12/04 21:21:01 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:21:01 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 21:21:01 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 21:21:01 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.25.86.80:42169 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:21:01 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
23/12/04 21:21:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:21:01 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
23/12/04 21:21:01 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 21:21:01 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
23/12/04 21:21:01 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1793 bytes result sent to driver
23/12/04 21:21:01 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 21 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:21:01 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
23/12/04 21:21:01 INFO DAGScheduler: ShuffleMapStage 5 (collect at Ast.scala:298) finished in 0.033 s
23/12/04 21:21:01 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:21:01 INFO DAGScheduler: running: HashSet()
23/12/04 21:21:01 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:21:01 INFO DAGScheduler: failed: HashSet()
23/12/04 21:21:01 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:21:01 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:21:01 INFO DAGScheduler: Got job 5 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:21:01 INFO DAGScheduler: Final stage: ResultStage 7 (collect at Ast.scala:298)
23/12/04 21:21:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
23/12/04 21:21:01 INFO DAGScheduler: Missing parents: List()
23/12/04 21:21:01 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:21:01 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 21:21:01 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 21:21:01 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.25.86.80:42169 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:21:01 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
23/12/04 21:21:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:21:01 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
23/12/04 21:21:01 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:21:01 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)
23/12/04 21:21:01 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:21:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
23/12/04 21:21:01 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 4494 bytes result sent to driver
23/12/04 21:21:01 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 17 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:21:01 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
23/12/04 21:21:01 INFO DAGScheduler: ResultStage 7 (collect at Ast.scala:298) finished in 0.025 s
23/12/04 21:21:01 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:21:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
23/12/04 21:21:01 INFO DAGScheduler: Job 5 finished: collect at Ast.scala:298, took 0.029328 s
23/12/04 21:21:02 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:21:02 INFO DAGScheduler: Got job 6 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:21:02 INFO DAGScheduler: Final stage: ResultStage 8 (collect at Ast.scala:298)
23/12/04 21:21:02 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:21:02 INFO DAGScheduler: Missing parents: List()
23/12/04 21:21:02 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:21:02 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 21:21:02 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 21:21:02 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.25.86.80:42169 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:21:02 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
23/12/04 21:21:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:21:02 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
23/12/04 21:21:02 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 21:21:02 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)
23/12/04 21:21:02 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 1826 bytes result sent to driver
23/12/04 21:21:02 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 31 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:21:02 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
23/12/04 21:21:02 INFO DAGScheduler: ResultStage 8 (collect at Ast.scala:298) finished in 0.043 s
23/12/04 21:21:02 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:21:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
23/12/04 21:21:02 INFO DAGScheduler: Job 6 finished: collect at Ast.scala:298, took 0.048355 s
23/12/04 21:21:02 INFO DAGScheduler: Registering RDD 24 (collect at Ast.scala:298) as input to shuffle 2
23/12/04 21:21:02 INFO DAGScheduler: Got map stage job 7 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:21:02 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (collect at Ast.scala:298)
23/12/04 21:21:02 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:21:02 INFO DAGScheduler: Missing parents: List()
23/12/04 21:21:02 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:21:02 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 21:21:02 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 21:21:02 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.25.86.80:42169 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:21:02 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
23/12/04 21:21:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:21:02 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
23/12/04 21:21:02 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/04 21:21:02 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)
23/12/04 21:21:02 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 1793 bytes result sent to driver
23/12/04 21:21:02 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 19 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:21:02 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
23/12/04 21:21:02 INFO DAGScheduler: ShuffleMapStage 9 (collect at Ast.scala:298) finished in 0.041 s
23/12/04 21:21:02 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:21:02 INFO DAGScheduler: running: HashSet()
23/12/04 21:21:02 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:21:02 INFO DAGScheduler: failed: HashSet()
23/12/04 21:21:02 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:21:02 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:21:02 INFO DAGScheduler: Got job 8 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:21:02 INFO DAGScheduler: Final stage: ResultStage 11 (collect at Ast.scala:298)
23/12/04 21:21:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
23/12/04 21:21:02 INFO DAGScheduler: Missing parents: List()
23/12/04 21:21:02 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:21:02 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 21:21:02 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 21:21:02 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.25.86.80:42169 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:21:02 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
23/12/04 21:21:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:21:02 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
23/12/04 21:21:02 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:21:02 INFO Executor: Running task 0.0 in stage 11.0 (TID 8)
23/12/04 21:21:02 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:21:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
23/12/04 21:21:02 INFO Executor: Finished task 0.0 in stage 11.0 (TID 8). 4494 bytes result sent to driver
23/12/04 21:21:02 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 20 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:21:02 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
23/12/04 21:21:02 INFO DAGScheduler: ResultStage 11 (collect at Ast.scala:298) finished in 0.030 s
23/12/04 21:21:02 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:21:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
23/12/04 21:21:02 INFO DAGScheduler: Job 8 finished: collect at Ast.scala:298, took 0.034983 s
23/12/04 21:21:36 INFO InMemoryFileIndex: It took 16 ms to list leaf files for 1 paths.
23/12/04 21:21:36 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:21:36 INFO DAGScheduler: Got job 9 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:21:36 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at Ast.scala:324)
23/12/04 21:21:36 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:21:36 INFO DAGScheduler: Missing parents: List()
23/12/04 21:21:36 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:21:36 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:21:36 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:21:36 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.25.86.80:42169 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:21:36 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
23/12/04 21:21:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:21:36 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
23/12/04 21:21:36 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:21:36 INFO Executor: Running task 0.0 in stage 12.0 (TID 9)
23/12/04 21:21:37 INFO Executor: Finished task 0.0 in stage 12.0 (TID 9). 2085 bytes result sent to driver
23/12/04 21:21:37 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 233 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:21:37 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
23/12/04 21:21:37 INFO DAGScheduler: ResultStage 12 (parquet at Ast.scala:324) finished in 0.247 s
23/12/04 21:21:37 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:21:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
23/12/04 21:21:37 INFO DAGScheduler: Job 9 finished: parquet at Ast.scala:324, took 0.249834 s
23/12/04 21:21:37 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/12/04 21:21:37 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:21:37 INFO DAGScheduler: Got job 10 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:21:37 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at Ast.scala:324)
23/12/04 21:21:37 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:21:37 INFO DAGScheduler: Missing parents: List()
23/12/04 21:21:37 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[31] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:21:37 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.25.86.80:42169 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:21:37 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.25.86.80:42169 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:21:37 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:21:37 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:21:37 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.25.86.80:42169 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:21:37 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580
23/12/04 21:21:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[31] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:21:37 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
23/12/04 21:21:37 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:21:37 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.25.86.80:42169 in memory (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:21:37 INFO Executor: Running task 0.0 in stage 13.0 (TID 10)
23/12/04 21:21:37 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.25.86.80:42169 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:21:37 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.25.86.80:42169 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:21:37 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.25.86.80:42169 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:21:37 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.25.86.80:42169 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:21:37 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.25.86.80:42169 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:21:37 INFO Executor: Finished task 0.0 in stage 13.0 (TID 10). 2042 bytes result sent to driver
23/12/04 21:21:37 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 20 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:21:37 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
23/12/04 21:21:37 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.25.86.80:42169 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:21:37 INFO DAGScheduler: ResultStage 13 (parquet at Ast.scala:324) finished in 0.031 s
23/12/04 21:21:37 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:21:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
23/12/04 21:21:37 INFO DAGScheduler: Job 10 finished: parquet at Ast.scala:324, took 0.033241 s
23/12/04 21:21:37 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.25.86.80:42169 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:21:37 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/12/04 21:21:37 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:21:37 INFO DAGScheduler: Got job 11 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:21:37 INFO DAGScheduler: Final stage: ResultStage 14 (parquet at Ast.scala:324)
23/12/04 21:21:37 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:21:37 INFO DAGScheduler: Missing parents: List()
23/12/04 21:21:37 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[33] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:21:37 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:21:37 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:21:37 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.25.86.80:42169 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:21:37 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580
23/12/04 21:21:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[33] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:21:37 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
23/12/04 21:21:37 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:21:37 INFO Executor: Running task 0.0 in stage 14.0 (TID 11)
23/12/04 21:21:37 INFO Executor: Finished task 0.0 in stage 14.0 (TID 11). 2042 bytes result sent to driver
23/12/04 21:21:37 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:21:37 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
23/12/04 21:21:37 INFO DAGScheduler: ResultStage 14 (parquet at Ast.scala:324) finished in 0.018 s
23/12/04 21:21:37 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:21:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
23/12/04 21:21:37 INFO DAGScheduler: Job 11 finished: parquet at Ast.scala:324, took 0.020386 s
23/12/04 21:21:54 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:21:54 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:21:54 INFO CodeGenerator: Code generated in 11.180127 ms
23/12/04 21:21:54 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:21:54 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:21:54 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.25.86.80:42169 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:21:54 INFO SparkContext: Created broadcast 12 from show at Ast.scala:95
23/12/04 21:21:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:21:54 INFO SparkContext: Starting job: show at Ast.scala:95
23/12/04 21:21:54 INFO DAGScheduler: Got job 12 (show at Ast.scala:95) with 1 output partitions
23/12/04 21:21:54 INFO DAGScheduler: Final stage: ResultStage 15 (show at Ast.scala:95)
23/12/04 21:21:54 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:21:54 INFO DAGScheduler: Missing parents: List()
23/12/04 21:21:54 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[37] at show at Ast.scala:95), which has no missing parents
23/12/04 21:21:54 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:21:54 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:21:54 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.25.86.80:42169 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:21:54 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580
23/12/04 21:21:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[37] at show at Ast.scala:95) (first 15 tasks are for partitions Vector(0))
23/12/04 21:21:54 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
23/12/04 21:21:54 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:21:54 INFO Executor: Running task 0.0 in stage 15.0 (TID 12)
23/12/04 21:21:54 INFO CodeGenerator: Code generated in 11.691126 ms
23/12/04 21:21:54 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:21:54 INFO CodecPool: Got brand-new decompressor [.snappy]
23/12/04 21:21:54 INFO Executor: Finished task 0.0 in stage 15.0 (TID 12). 1928 bytes result sent to driver
23/12/04 21:21:54 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 243 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:21:54 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
23/12/04 21:21:54 INFO DAGScheduler: ResultStage 15 (show at Ast.scala:95) finished in 0.262 s
23/12/04 21:21:54 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:21:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
23/12/04 21:21:54 INFO DAGScheduler: Job 12 finished: show at Ast.scala:95, took 0.265562 s
23/12/04 21:21:54 INFO CodeGenerator: Code generated in 7.183111 ms
23/12/04 21:22:00 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:22:00 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:22:00 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:22:00 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:22:00 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.25.86.80:42169 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:22:00 INFO SparkContext: Created broadcast 14 from show at Ast.scala:96
23/12/04 21:22:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:22:00 INFO SparkContext: Starting job: show at Ast.scala:96
23/12/04 21:22:00 INFO DAGScheduler: Got job 13 (show at Ast.scala:96) with 1 output partitions
23/12/04 21:22:00 INFO DAGScheduler: Final stage: ResultStage 16 (show at Ast.scala:96)
23/12/04 21:22:00 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:22:00 INFO DAGScheduler: Missing parents: List()
23/12/04 21:22:00 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[41] at show at Ast.scala:96), which has no missing parents
23/12/04 21:22:00 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:22:00 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:22:00 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.25.86.80:42169 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:22:00 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580
23/12/04 21:22:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[41] at show at Ast.scala:96) (first 15 tasks are for partitions Vector(0))
23/12/04 21:22:00 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
23/12/04 21:22:00 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 13) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:22:00 INFO Executor: Running task 0.0 in stage 16.0 (TID 13)
23/12/04 21:22:00 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:22:00 INFO Executor: Finished task 0.0 in stage 16.0 (TID 13). 1928 bytes result sent to driver
23/12/04 21:22:00 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 13) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:22:00 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
23/12/04 21:22:00 INFO DAGScheduler: ResultStage 16 (show at Ast.scala:96) finished in 0.016 s
23/12/04 21:22:00 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:22:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
23/12/04 21:22:00 INFO DAGScheduler: Job 13 finished: show at Ast.scala:96, took 0.018341 s
23/12/04 21:22:00 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:22:00 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:22:00 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:22:00 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 21:22:00 INFO CodeGenerator: Code generated in 7.973262 ms
23/12/04 21:22:00 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:22:00 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:22:00 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.25.86.80:42169 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:22:00 INFO SparkContext: Created broadcast 16 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:22:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:22:00 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:22:00 INFO DAGScheduler: Got job 14 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:22:00 INFO DAGScheduler: Final stage: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:22:00 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:22:00 INFO DAGScheduler: Missing parents: List()
23/12/04 21:22:00 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[45] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:22:00 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:22:00 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:22:00 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.25.86.80:42169 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:22:00 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580
23/12/04 21:22:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[45] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:22:00 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
23/12/04 21:22:00 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 14) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:22:00 INFO Executor: Running task 0.0 in stage 17.0 (TID 14)
23/12/04 21:22:01 INFO CodeGenerator: Code generated in 6.462186 ms
23/12/04 21:22:01 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:22:01 INFO Executor: Finished task 0.0 in stage 17.0 (TID 14). 2007 bytes result sent to driver
23/12/04 21:22:01 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 14) in 58 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:22:01 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
23/12/04 21:22:01 INFO DAGScheduler: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.062 s
23/12/04 21:22:01 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:22:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
23/12/04 21:22:01 INFO DAGScheduler: Job 14 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.063469 s
23/12/04 21:22:01 INFO CodeGenerator: Code generated in 4.855594 ms
23/12/04 21:22:01 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:22:01 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:22:01 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.25.86.80:42169 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:22:01 INFO SparkContext: Created broadcast 18 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:22:01 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:22:01 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:22:01 INFO CodeGenerator: Code generated in 9.733127 ms
23/12/04 21:22:01 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:22:01 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:22:01 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.25.86.80:42169 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:22:01 INFO SparkContext: Created broadcast 19 from show at Ast.scala:108
23/12/04 21:22:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:22:01 INFO SparkContext: Starting job: show at Ast.scala:108
23/12/04 21:22:01 INFO DAGScheduler: Got job 15 (show at Ast.scala:108) with 1 output partitions
23/12/04 21:22:01 INFO DAGScheduler: Final stage: ResultStage 18 (show at Ast.scala:108)
23/12/04 21:22:01 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:22:01 INFO DAGScheduler: Missing parents: List()
23/12/04 21:22:01 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[49] at show at Ast.scala:108), which has no missing parents
23/12/04 21:22:01 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 19.6 KiB, free 9.1 GiB)
23/12/04 21:22:01 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 9.1 GiB)
23/12/04 21:22:01 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.25.86.80:42169 (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 21:22:01 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1580
23/12/04 21:22:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[49] at show at Ast.scala:108) (first 15 tasks are for partitions Vector(0))
23/12/04 21:22:01 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
23/12/04 21:22:01 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 15) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:22:01 INFO Executor: Running task 0.0 in stage 18.0 (TID 15)
23/12/04 21:22:01 INFO CodeGenerator: Code generated in 12.089142 ms
23/12/04 21:22:01 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:22:01 INFO Executor: Finished task 0.0 in stage 18.0 (TID 15). 2118 bytes result sent to driver
23/12/04 21:22:01 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 15) in 23 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:22:01 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
23/12/04 21:22:01 INFO DAGScheduler: ResultStage 18 (show at Ast.scala:108) finished in 0.028 s
23/12/04 21:22:01 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:22:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
23/12/04 21:22:01 INFO DAGScheduler: Job 15 finished: show at Ast.scala:108, took 0.029852 s
23/12/04 21:22:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:22:09 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:22:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:22:09 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 21:22:09 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:22:09 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:22:09 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.25.86.80:42169 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:22:09 INFO SparkContext: Created broadcast 21 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:22:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:22:09 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:22:09 INFO DAGScheduler: Got job 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:22:09 INFO DAGScheduler: Final stage: ResultStage 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:22:09 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:22:09 INFO DAGScheduler: Missing parents: List()
23/12/04 21:22:09 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[53] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:22:09 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:22:09 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:22:09 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.25.86.80:42169 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:22:09 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1580
23/12/04 21:22:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[53] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:22:09 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
23/12/04 21:22:09 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 16) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:22:09 INFO Executor: Running task 0.0 in stage 19.0 (TID 16)
23/12/04 21:22:09 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:22:09 INFO Executor: Finished task 0.0 in stage 19.0 (TID 16). 2007 bytes result sent to driver
23/12/04 21:22:09 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 16) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:22:09 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
23/12/04 21:22:09 INFO DAGScheduler: ResultStage 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.016 s
23/12/04 21:22:09 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:22:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
23/12/04 21:22:09 INFO DAGScheduler: Job 16 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.018098 s
23/12/04 21:22:09 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:22:09 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:22:09 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.25.86.80:42169 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:22:09 INFO SparkContext: Created broadcast 23 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:22:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:22:09 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:22:09 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:22:09 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:22:09 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.25.86.80:42169 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:22:09 INFO SparkContext: Created broadcast 24 from show at Ast.scala:112
23/12/04 21:22:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:22:09 INFO SparkContext: Starting job: show at Ast.scala:112
23/12/04 21:22:09 INFO DAGScheduler: Got job 17 (show at Ast.scala:112) with 1 output partitions
23/12/04 21:22:09 INFO DAGScheduler: Final stage: ResultStage 20 (show at Ast.scala:112)
23/12/04 21:22:09 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:22:09 INFO DAGScheduler: Missing parents: List()
23/12/04 21:22:09 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[57] at show at Ast.scala:112), which has no missing parents
23/12/04 21:22:09 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 19.6 KiB, free 9.1 GiB)
23/12/04 21:22:09 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 9.1 GiB)
23/12/04 21:22:09 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.25.86.80:42169 (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 21:22:09 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1580
23/12/04 21:22:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[57] at show at Ast.scala:112) (first 15 tasks are for partitions Vector(0))
23/12/04 21:22:09 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
23/12/04 21:22:09 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 17) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:22:09 INFO Executor: Running task 0.0 in stage 20.0 (TID 17)
23/12/04 21:22:09 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:22:09 INFO Executor: Finished task 0.0 in stage 20.0 (TID 17). 2118 bytes result sent to driver
23/12/04 21:22:09 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 17) in 8 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:22:09 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
23/12/04 21:22:09 INFO DAGScheduler: ResultStage 20 (show at Ast.scala:112) finished in 0.014 s
23/12/04 21:22:09 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:22:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
23/12/04 21:22:09 INFO DAGScheduler: Job 17 finished: show at Ast.scala:112, took 0.016139 s
23/12/04 21:22:11 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:22:11 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:22:11 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:22:11 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:22:11 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.25.86.80:42169 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:22:11 INFO SparkContext: Created broadcast 26 from show at Ast.scala:113
23/12/04 21:22:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:22:11 INFO SparkContext: Starting job: show at Ast.scala:113
23/12/04 21:22:11 INFO DAGScheduler: Got job 18 (show at Ast.scala:113) with 1 output partitions
23/12/04 21:22:11 INFO DAGScheduler: Final stage: ResultStage 21 (show at Ast.scala:113)
23/12/04 21:22:11 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:22:11 INFO DAGScheduler: Missing parents: List()
23/12/04 21:22:11 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[61] at show at Ast.scala:113), which has no missing parents
23/12/04 21:22:11 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:22:11 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:22:11 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.25.86.80:42169 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:22:11 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1580
23/12/04 21:22:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[61] at show at Ast.scala:113) (first 15 tasks are for partitions Vector(0))
23/12/04 21:22:11 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
23/12/04 21:22:11 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 18) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:22:11 INFO Executor: Running task 0.0 in stage 21.0 (TID 18)
23/12/04 21:22:11 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:22:11 INFO Executor: Finished task 0.0 in stage 21.0 (TID 18). 1928 bytes result sent to driver
23/12/04 21:22:11 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 18) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:22:11 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
23/12/04 21:22:11 INFO DAGScheduler: ResultStage 21 (show at Ast.scala:113) finished in 0.018 s
23/12/04 21:22:11 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:22:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
23/12/04 21:22:11 INFO DAGScheduler: Job 18 finished: show at Ast.scala:113, took 0.020529 s
23/12/04 21:22:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:22:11 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:22:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:22:11 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 21:22:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:22:11 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#55)
23/12/04 21:22:11 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:22:11 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:22:11 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.25.86.80:42169 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:22:11 INFO SparkContext: Created broadcast 28 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:22:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:22:11 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:22:11 INFO DAGScheduler: Got job 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:22:11 INFO DAGScheduler: Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:22:11 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:22:11 INFO DAGScheduler: Missing parents: List()
23/12/04 21:22:11 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[65] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:22:11 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:22:11 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:22:11 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.25.86.80:42169 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:22:11 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1580
23/12/04 21:22:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[65] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:22:11 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
23/12/04 21:22:11 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 19) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:22:11 INFO Executor: Running task 0.0 in stage 22.0 (TID 19)
23/12/04 21:22:11 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:22:11 INFO Executor: Finished task 0.0 in stage 22.0 (TID 19). 2007 bytes result sent to driver
23/12/04 21:22:11 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 19) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:22:11 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
23/12/04 21:22:11 INFO DAGScheduler: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.015 s
23/12/04 21:22:11 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:22:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
23/12/04 21:22:11 INFO DAGScheduler: Job 19 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.016797 s
23/12/04 21:22:11 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:22:11 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:22:11 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.25.86.80:42169 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:22:11 INFO SparkContext: Created broadcast 30 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:22:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:22:11 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:22:11 INFO CodeGenerator: Code generated in 12.427293 ms
23/12/04 21:22:11 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:22:11 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:22:11 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.25.86.80:42169 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:22:11 INFO SparkContext: Created broadcast 31 from show at Ast.scala:126
23/12/04 21:22:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:22:11 INFO SparkContext: Starting job: show at Ast.scala:126
23/12/04 21:22:11 INFO DAGScheduler: Got job 20 (show at Ast.scala:126) with 1 output partitions
23/12/04 21:22:11 INFO DAGScheduler: Final stage: ResultStage 23 (show at Ast.scala:126)
23/12/04 21:22:11 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:22:11 INFO DAGScheduler: Missing parents: List()
23/12/04 21:22:11 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[69] at show at Ast.scala:126), which has no missing parents
23/12/04 21:22:11 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 22.4 KiB, free 9.1 GiB)
23/12/04 21:22:11 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 9.1 GiB)
23/12/04 21:22:11 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.25.86.80:42169 (size: 8.6 KiB, free: 9.1 GiB)
23/12/04 21:22:11 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1580
23/12/04 21:22:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[69] at show at Ast.scala:126) (first 15 tasks are for partitions Vector(0))
23/12/04 21:22:11 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
23/12/04 21:22:11 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 20) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:22:11 INFO Executor: Running task 0.0 in stage 23.0 (TID 20)
23/12/04 21:22:11 INFO CodeGenerator: Code generated in 13.721011 ms
23/12/04 21:22:11 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:22:11 INFO Executor: Finished task 0.0 in stage 23.0 (TID 20). 2172 bytes result sent to driver
23/12/04 21:22:11 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 20) in 24 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:22:11 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
23/12/04 21:22:11 INFO DAGScheduler: ResultStage 23 (show at Ast.scala:126) finished in 0.031 s
23/12/04 21:22:11 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:22:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
23/12/04 21:22:11 INFO DAGScheduler: Job 20 finished: show at Ast.scala:126, took 0.033471 s
23/12/04 21:22:31 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:22:31 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:22:31 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:22:31 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 21:22:31 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:22:31 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#55)
23/12/04 21:22:31 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:22:31 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:22:31 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.25.86.80:42169 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:22:31 INFO SparkContext: Created broadcast 33 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:22:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:22:31 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:22:31 INFO DAGScheduler: Got job 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:22:31 INFO DAGScheduler: Final stage: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:22:31 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:22:31 INFO DAGScheduler: Missing parents: List()
23/12/04 21:22:31 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[73] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:22:31 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:22:31 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:22:31 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.25.86.80:42169 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:22:31 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1580
23/12/04 21:22:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[73] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:22:31 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
23/12/04 21:22:31 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 21) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:22:31 INFO Executor: Running task 0.0 in stage 24.0 (TID 21)
23/12/04 21:22:31 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:22:31 INFO Executor: Finished task 0.0 in stage 24.0 (TID 21). 2007 bytes result sent to driver
23/12/04 21:22:31 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 21) in 7 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:22:31 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
23/12/04 21:22:31 INFO DAGScheduler: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.010 s
23/12/04 21:22:31 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:22:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
23/12/04 21:22:31 INFO DAGScheduler: Job 21 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.011785 s
23/12/04 21:22:31 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:22:31 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:22:31 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.25.86.80:42169 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:22:31 INFO SparkContext: Created broadcast 35 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:22:31 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:22:31 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:22:31 INFO CodeGenerator: Code generated in 9.148538 ms
23/12/04 21:22:31 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:22:31 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:22:31 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.25.86.80:42169 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:22:31 INFO SparkContext: Created broadcast 36 from collect at Ast.scala:298
23/12/04 21:22:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:22:31 INFO CodeGenerator: Code generated in 4.158449 ms
23/12/04 21:22:31 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:22:31 INFO DAGScheduler: Got job 22 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:22:31 INFO DAGScheduler: Final stage: ResultStage 25 (collect at Ast.scala:298)
23/12/04 21:22:31 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:22:31 INFO DAGScheduler: Missing parents: List()
23/12/04 21:22:31 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[79] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:22:31 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 25.1 KiB, free 9.1 GiB)
23/12/04 21:22:31 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 9.1 GiB)
23/12/04 21:22:31 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.25.86.80:42169 (size: 9.8 KiB, free: 9.1 GiB)
23/12/04 21:22:31 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1580
23/12/04 21:22:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[79] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:22:31 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
23/12/04 21:22:31 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 22) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:22:31 INFO Executor: Running task 0.0 in stage 25.0 (TID 22)
23/12/04 21:22:31 INFO CodeGenerator: Code generated in 9.758553 ms
23/12/04 21:22:31 INFO CodeGenerator: Code generated in 3.402968 ms
23/12/04 21:22:31 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:22:31 INFO Executor: Finished task 0.0 in stage 25.0 (TID 22). 2331 bytes result sent to driver
23/12/04 21:22:31 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 22) in 25 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:22:31 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
23/12/04 21:22:31 INFO DAGScheduler: ResultStage 25 (collect at Ast.scala:298) finished in 0.032 s
23/12/04 21:22:31 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:22:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
23/12/04 21:22:31 INFO DAGScheduler: Job 22 finished: collect at Ast.scala:298, took 0.034487 s
23/12/04 21:22:31 INFO DAGScheduler: Registering RDD 80 (collect at Ast.scala:298) as input to shuffle 3
23/12/04 21:22:31 INFO DAGScheduler: Got map stage job 23 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:22:31 INFO DAGScheduler: Final stage: ShuffleMapStage 26 (collect at Ast.scala:298)
23/12/04 21:22:31 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:22:31 INFO DAGScheduler: Missing parents: List()
23/12/04 21:22:31 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[80] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:22:31 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 27.2 KiB, free 9.1 GiB)
23/12/04 21:22:31 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 9.1 GiB)
23/12/04 21:22:31 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.25.86.80:42169 (size: 10.9 KiB, free: 9.1 GiB)
23/12/04 21:22:31 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1580
23/12/04 21:22:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[80] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:22:31 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
23/12/04 21:22:31 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 23) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8468 bytes) 
23/12/04 21:22:31 INFO Executor: Running task 0.0 in stage 26.0 (TID 23)
23/12/04 21:22:31 INFO CodeGenerator: Code generated in 4.565443 ms
23/12/04 21:22:31 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-83d36080-b979-4081-aa96-005dc36b9cfd-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:22:31 INFO Executor: Finished task 0.0 in stage 26.0 (TID 23). 2298 bytes result sent to driver
23/12/04 21:22:31 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 23) in 16 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:22:31 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
23/12/04 21:22:31 INFO DAGScheduler: ShuffleMapStage 26 (collect at Ast.scala:298) finished in 0.020 s
23/12/04 21:22:31 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:22:31 INFO DAGScheduler: running: HashSet()
23/12/04 21:22:31 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:22:31 INFO DAGScheduler: failed: HashSet()
23/12/04 21:22:31 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:22:31 INFO CodeGenerator: Code generated in 5.809476 ms
23/12/04 21:22:31 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:22:31 INFO DAGScheduler: Got job 24 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:22:31 INFO DAGScheduler: Final stage: ResultStage 28 (collect at Ast.scala:298)
23/12/04 21:22:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
23/12/04 21:22:31 INFO DAGScheduler: Missing parents: List()
23/12/04 21:22:31 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[83] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:22:31 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 38.1 KiB, free 9.1 GiB)
23/12/04 21:22:31 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 9.1 GiB)
23/12/04 21:22:31 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.25.86.80:42169 (size: 16.4 KiB, free: 9.1 GiB)
23/12/04 21:22:31 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1580
23/12/04 21:22:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[83] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:22:31 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
23/12/04 21:22:31 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 24) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:22:31 INFO Executor: Running task 0.0 in stage 28.0 (TID 24)
23/12/04 21:22:31 INFO ShuffleBlockFetcherIterator: Getting 1 (432.0 B) non-empty blocks including 1 (432.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:22:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 21:22:31 INFO CodeGenerator: Code generated in 7.027073 ms
23/12/04 21:22:31 INFO CodeGenerator: Code generated in 7.24001 ms
23/12/04 21:22:31 INFO Executor: Finished task 0.0 in stage 28.0 (TID 24). 5956 bytes result sent to driver
23/12/04 21:22:31 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 24) in 26 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:22:31 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
23/12/04 21:22:31 INFO DAGScheduler: ResultStage 28 (collect at Ast.scala:298) finished in 0.032 s
23/12/04 21:22:31 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:22:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
23/12/04 21:22:31 INFO DAGScheduler: Job 24 finished: collect at Ast.scala:298, took 0.034302 s
23/12/04 21:22:31 INFO CodeGenerator: Code generated in 7.898248 ms
23/12/04 21:25:10 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 21:25:10 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 21:25:10 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 21:25:10 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4040
23/12/04 21:25:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 21:25:10 INFO MemoryStore: MemoryStore cleared
23/12/04 21:25:10 INFO BlockManager: BlockManager stopped
23/12/04 21:25:10 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 21:25:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 21:25:10 INFO SparkContext: Successfully stopped SparkContext
23/12/04 21:25:10 INFO ShutdownHookManager: Shutdown hook called
23/12/04 21:25:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-3f37eee0-f9ac-498c-b277-c83b6390ba4d
23/12/04 21:32:29 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 21:32:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 21:32:30 INFO SparkContext: Running Spark version 3.5.0
23/12/04 21:32:30 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 21:32:30 INFO SparkContext: Java version 11.0.20
23/12/04 21:32:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 21:32:30 INFO ResourceUtils: ==============================================================
23/12/04 21:32:30 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 21:32:30 INFO ResourceUtils: ==============================================================
23/12/04 21:32:30 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 21:32:30 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 21:32:30 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 21:32:30 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 21:32:30 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 21:32:30 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 21:32:30 INFO SecurityManager: Changing view acls groups to: 
23/12/04 21:32:30 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 21:32:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 21:32:30 INFO Utils: Successfully started service 'sparkDriver' on port 35361.
23/12/04 21:32:30 INFO SparkEnv: Registering MapOutputTracker
23/12/04 21:32:30 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 21:32:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 21:32:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 21:32:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 21:32:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-700c39f6-13e2-4d25-aecd-534e46b6ff31
23/12/04 21:32:30 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 21:32:30 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 21:32:30 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 21:32:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/12/04 21:32:30 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 21:32:30 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 21:32:30 INFO Executor: Java version 11.0.20
23/12/04 21:32:30 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 21:32:30 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6914b9a7 for default.
23/12/04 21:32:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34641.
23/12/04 21:32:30 INFO NettyBlockTransferService: Server created on 10.25.86.80:34641
23/12/04 21:32:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 21:32:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 34641, None)
23/12/04 21:32:30 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:34641 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 34641, None)
23/12/04 21:32:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 34641, None)
23/12/04 21:32:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 34641, None)
23/12/04 21:32:33 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 21:32:33 INFO xnio: XNIO version 3.8.7.Final
23/12/04 21:32:33 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 21:32:33 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 21:32:36 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 21:32:36 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 21:32:48 INFO CodeGenerator: Code generated in 133.414523 ms
23/12/04 21:32:48 INFO CodeGenerator: Code generated in 12.480228 ms
23/12/04 21:32:48 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:32:48 INFO DAGScheduler: Got job 0 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:32:48 INFO DAGScheduler: Final stage: ResultStage 0 (collect at Ast.scala:298)
23/12/04 21:32:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:32:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:32:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:32:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 21:32:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 21:32:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:34641 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:32:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/04 21:32:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:32:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/04 21:32:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 21:32:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/04 21:32:48 INFO CodeGenerator: Code generated in 7.213411 ms
23/12/04 21:32:48 INFO CodeGenerator: Code generated in 6.115408 ms
23/12/04 21:32:49 INFO CodeGenerator: Code generated in 24.025684 ms
23/12/04 21:32:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1869 bytes result sent to driver
23/12/04 21:32:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 661 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:32:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/04 21:32:49 INFO DAGScheduler: ResultStage 0 (collect at Ast.scala:298) finished in 0.755 s
23/12/04 21:32:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:32:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/04 21:32:49 INFO DAGScheduler: Job 0 finished: collect at Ast.scala:298, took 0.781790 s
23/12/04 21:32:49 INFO DAGScheduler: Registering RDD 8 (collect at Ast.scala:298) as input to shuffle 0
23/12/04 21:32:49 INFO DAGScheduler: Got map stage job 1 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:32:49 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at Ast.scala:298)
23/12/04 21:32:49 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:32:49 INFO DAGScheduler: Missing parents: List()
23/12/04 21:32:49 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:32:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 21:32:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 21:32:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.25.86.80:34641 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:32:49 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/12/04 21:32:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:32:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/12/04 21:32:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 21:32:49 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/12/04 21:32:49 INFO CodeGenerator: Code generated in 4.678702 ms
23/12/04 21:32:49 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1793 bytes result sent to driver
23/12/04 21:32:49 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 48 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:32:49 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/04 21:32:49 INFO DAGScheduler: ShuffleMapStage 1 (collect at Ast.scala:298) finished in 0.063 s
23/12/04 21:32:49 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:32:49 INFO DAGScheduler: running: HashSet()
23/12/04 21:32:49 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:32:49 INFO DAGScheduler: failed: HashSet()
23/12/04 21:32:49 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:32:49 INFO CodeGenerator: Code generated in 8.201503 ms
23/12/04 21:32:49 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:32:49 INFO DAGScheduler: Got job 2 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:32:49 INFO DAGScheduler: Final stage: ResultStage 3 (collect at Ast.scala:298)
23/12/04 21:32:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
23/12/04 21:32:49 INFO DAGScheduler: Missing parents: List()
23/12/04 21:32:49 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:32:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 21:32:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 21:32:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.25.86.80:34641 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:32:49 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
23/12/04 21:32:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:32:49 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
23/12/04 21:32:49 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:32:49 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
23/12/04 21:32:49 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:32:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
23/12/04 21:32:49 INFO CodeGenerator: Code generated in 8.946885 ms
23/12/04 21:32:49 INFO CodeGenerator: Code generated in 7.065437 ms
23/12/04 21:32:49 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4537 bytes result sent to driver
23/12/04 21:32:49 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 98 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:32:49 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
23/12/04 21:32:49 INFO DAGScheduler: ResultStage 3 (collect at Ast.scala:298) finished in 0.109 s
23/12/04 21:32:49 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:32:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
23/12/04 21:32:49 INFO DAGScheduler: Job 2 finished: collect at Ast.scala:298, took 0.116841 s
23/12/04 21:32:49 INFO CodeGenerator: Code generated in 9.895053 ms
23/12/04 21:32:49 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/04 21:32:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/12/04 21:32:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/12/04 21:32:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/04 21:32:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/12/04 21:32:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/12/04 21:32:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/04 21:32:49 INFO SparkContext: Starting job: parquet at Ast.scala:308
23/12/04 21:32:49 INFO DAGScheduler: Got job 3 (parquet at Ast.scala:308) with 1 output partitions
23/12/04 21:32:49 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at Ast.scala:308)
23/12/04 21:32:49 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:32:49 INFO DAGScheduler: Missing parents: List()
23/12/04 21:32:49 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at parquet at Ast.scala:308), which has no missing parents
23/12/04 21:32:49 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 216.2 KiB, free 9.1 GiB)
23/12/04 21:32:49 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 9.1 GiB)
23/12/04 21:32:49 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.25.86.80:34641 (size: 77.2 KiB, free: 9.1 GiB)
23/12/04 21:32:49 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
23/12/04 21:32:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at parquet at Ast.scala:308) (first 15 tasks are for partitions Vector(0))
23/12/04 21:32:49 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/12/04 21:32:49 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8328 bytes) 
23/12/04 21:32:49 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
23/12/04 21:32:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/12/04 21:32:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/12/04 21:32:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/04 21:32:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/12/04 21:32:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/12/04 21:32:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/04 21:32:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "index",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "doubleField",
    "type" : "double",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "stringField",
    "type" : "string",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "intField",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 index;
  required double doubleField;
  required binary stringField (STRING);
  required int32 intField;
}

       
23/12/04 21:32:49 INFO CodecPool: Got brand-new compressor [.snappy]
23/12/04 21:32:50 INFO FileOutputCommitter: Saved output of task 'attempt_202312042132496195446080045299627_0004_m_000000_3' to file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/_temporary/0/task_202312042132496195446080045299627_0004_m_000000
23/12/04 21:32:50 INFO SparkHadoopMapRedUtil: attempt_202312042132496195446080045299627_0004_m_000000_3: Committed. Elapsed time: 1 ms.
23/12/04 21:32:50 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 2742 bytes result sent to driver
23/12/04 21:32:50 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 451 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:32:50 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/12/04 21:32:50 INFO DAGScheduler: ResultStage 4 (parquet at Ast.scala:308) finished in 0.485 s
23/12/04 21:32:50 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:32:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/12/04 21:32:50 INFO DAGScheduler: Job 3 finished: parquet at Ast.scala:308, took 0.488341 s
23/12/04 21:32:50 INFO FileFormatWriter: Start to commit write Job 6d7918e7-e9c9-4bcb-a365-69f624415e73.
23/12/04 21:32:50 INFO FileFormatWriter: Write Job 6d7918e7-e9c9-4bcb-a365-69f624415e73 committed. Elapsed time: 28 ms.
23/12/04 21:32:50 INFO FileFormatWriter: Finished processing stats for write job 6d7918e7-e9c9-4bcb-a365-69f624415e73.
23/12/04 21:32:50 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.25.86.80:34641 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:32:50 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.25.86.80:34641 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:32:50 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.25.86.80:34641 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:32:50 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.25.86.80:34641 in memory (size: 77.2 KiB, free: 9.1 GiB)
23/12/04 21:32:50 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:32:50 INFO DAGScheduler: Got job 4 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:32:50 INFO DAGScheduler: Final stage: ResultStage 5 (collect at Ast.scala:298)
23/12/04 21:32:50 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:32:50 INFO DAGScheduler: Missing parents: List()
23/12/04 21:32:50 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[18] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:32:50 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 21:32:50 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 21:32:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.25.86.80:34641 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:32:50 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
23/12/04 21:32:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[18] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:32:50 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
23/12/04 21:32:50 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/04 21:32:50 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
23/12/04 21:32:50 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1826 bytes result sent to driver
23/12/04 21:32:50 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 15 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:32:50 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
23/12/04 21:32:50 INFO DAGScheduler: ResultStage 5 (collect at Ast.scala:298) finished in 0.021 s
23/12/04 21:32:50 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:32:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
23/12/04 21:32:50 INFO DAGScheduler: Job 4 finished: collect at Ast.scala:298, took 0.024469 s
23/12/04 21:32:50 INFO DAGScheduler: Registering RDD 19 (collect at Ast.scala:298) as input to shuffle 1
23/12/04 21:32:50 INFO DAGScheduler: Got map stage job 5 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:32:50 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (collect at Ast.scala:298)
23/12/04 21:32:50 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:32:50 INFO DAGScheduler: Missing parents: List()
23/12/04 21:32:50 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[19] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:32:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 21:32:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 21:32:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.25.86.80:34641 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:32:50 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
23/12/04 21:32:50 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[19] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:32:50 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
23/12/04 21:32:50 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 21:32:50 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
23/12/04 21:32:50 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 1793 bytes result sent to driver
23/12/04 21:32:50 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 15 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:32:50 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
23/12/04 21:32:50 INFO DAGScheduler: ShuffleMapStage 6 (collect at Ast.scala:298) finished in 0.021 s
23/12/04 21:32:50 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:32:50 INFO DAGScheduler: running: HashSet()
23/12/04 21:32:50 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:32:50 INFO DAGScheduler: failed: HashSet()
23/12/04 21:32:50 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:32:50 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:32:50 INFO DAGScheduler: Got job 6 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:32:50 INFO DAGScheduler: Final stage: ResultStage 8 (collect at Ast.scala:298)
23/12/04 21:32:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
23/12/04 21:32:50 INFO DAGScheduler: Missing parents: List()
23/12/04 21:32:50 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[22] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:32:50 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 21:32:50 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 21:32:50 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.25.86.80:34641 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:32:50 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
23/12/04 21:32:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[22] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:32:50 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
23/12/04 21:32:50 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:32:50 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)
23/12/04 21:32:50 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:32:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 21:32:50 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 4494 bytes result sent to driver
23/12/04 21:32:50 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:32:50 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
23/12/04 21:32:50 INFO DAGScheduler: ResultStage 8 (collect at Ast.scala:298) finished in 0.014 s
23/12/04 21:32:50 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:32:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
23/12/04 21:32:50 INFO DAGScheduler: Job 6 finished: collect at Ast.scala:298, took 0.017302 s
23/12/04 21:32:50 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:32:50 INFO DAGScheduler: Got job 7 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:32:50 INFO DAGScheduler: Final stage: ResultStage 9 (collect at Ast.scala:298)
23/12/04 21:32:50 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:32:50 INFO DAGScheduler: Missing parents: List()
23/12/04 21:32:50 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[26] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:32:50 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 21:32:50 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 21:32:50 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.25.86.80:34641 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:32:50 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
23/12/04 21:32:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[26] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:32:50 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
23/12/04 21:32:50 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 21:32:50 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)
23/12/04 21:32:50 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 1826 bytes result sent to driver
23/12/04 21:32:50 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:32:50 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
23/12/04 21:32:50 INFO DAGScheduler: ResultStage 9 (collect at Ast.scala:298) finished in 0.020 s
23/12/04 21:32:50 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:32:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
23/12/04 21:32:50 INFO DAGScheduler: Job 7 finished: collect at Ast.scala:298, took 0.023215 s
23/12/04 21:32:50 INFO DAGScheduler: Registering RDD 27 (collect at Ast.scala:298) as input to shuffle 2
23/12/04 21:32:50 INFO DAGScheduler: Got map stage job 8 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:32:50 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (collect at Ast.scala:298)
23/12/04 21:32:50 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:32:50 INFO DAGScheduler: Missing parents: List()
23/12/04 21:32:50 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[27] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:32:50 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 21:32:50 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 21:32:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.25.86.80:34641 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:32:50 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
23/12/04 21:32:50 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[27] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:32:50 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
23/12/04 21:32:50 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/04 21:32:50 INFO Executor: Running task 0.0 in stage 10.0 (TID 8)
23/12/04 21:32:50 INFO Executor: Finished task 0.0 in stage 10.0 (TID 8). 1793 bytes result sent to driver
23/12/04 21:32:50 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 15 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:32:50 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
23/12/04 21:32:50 INFO DAGScheduler: ShuffleMapStage 10 (collect at Ast.scala:298) finished in 0.023 s
23/12/04 21:32:50 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:32:50 INFO DAGScheduler: running: HashSet()
23/12/04 21:32:50 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:32:50 INFO DAGScheduler: failed: HashSet()
23/12/04 21:32:50 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:32:50 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:32:50 INFO DAGScheduler: Got job 9 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:32:50 INFO DAGScheduler: Final stage: ResultStage 12 (collect at Ast.scala:298)
23/12/04 21:32:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
23/12/04 21:32:50 INFO DAGScheduler: Missing parents: List()
23/12/04 21:32:50 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[30] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:32:50 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 21:32:50 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 21:32:50 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.25.86.80:34641 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:32:50 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
23/12/04 21:32:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[30] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:32:50 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
23/12/04 21:32:50 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:32:50 INFO Executor: Running task 0.0 in stage 12.0 (TID 9)
23/12/04 21:32:50 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:32:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 21:32:50 INFO Executor: Finished task 0.0 in stage 12.0 (TID 9). 4494 bytes result sent to driver
23/12/04 21:32:50 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:32:50 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
23/12/04 21:32:50 INFO DAGScheduler: ResultStage 12 (collect at Ast.scala:298) finished in 0.018 s
23/12/04 21:32:50 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:32:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
23/12/04 21:32:50 INFO DAGScheduler: Job 9 finished: collect at Ast.scala:298, took 0.023557 s
23/12/04 21:33:10 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 1 paths.
23/12/04 21:33:10 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:33:10 INFO DAGScheduler: Got job 10 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:33:10 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at Ast.scala:324)
23/12/04 21:33:10 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:33:10 INFO DAGScheduler: Missing parents: List()
23/12/04 21:33:10 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[32] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:33:10 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:33:10 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:33:10 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.25.86.80:34641 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:33:10 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580
23/12/04 21:33:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[32] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:33:10 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
23/12/04 21:33:10 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:33:10 INFO Executor: Running task 0.0 in stage 13.0 (TID 10)
23/12/04 21:33:11 INFO Executor: Finished task 0.0 in stage 13.0 (TID 10). 2085 bytes result sent to driver
23/12/04 21:33:11 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 59 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:33:11 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
23/12/04 21:33:11 INFO DAGScheduler: ResultStage 13 (parquet at Ast.scala:324) finished in 0.069 s
23/12/04 21:33:11 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:33:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
23/12/04 21:33:11 INFO DAGScheduler: Job 10 finished: parquet at Ast.scala:324, took 0.072350 s
23/12/04 21:33:11 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/12/04 21:33:11 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:33:11 INFO DAGScheduler: Got job 11 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:33:11 INFO DAGScheduler: Final stage: ResultStage 14 (parquet at Ast.scala:324)
23/12/04 21:33:11 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:33:11 INFO DAGScheduler: Missing parents: List()
23/12/04 21:33:11 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[34] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:33:11 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:33:11 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:33:11 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.25.86.80:34641 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:33:11 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580
23/12/04 21:33:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[34] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:33:11 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
23/12/04 21:33:11 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:33:11 INFO Executor: Running task 0.0 in stage 14.0 (TID 11)
23/12/04 21:33:11 INFO Executor: Finished task 0.0 in stage 14.0 (TID 11). 2042 bytes result sent to driver
23/12/04 21:33:11 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 17 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:33:11 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
23/12/04 21:33:11 INFO DAGScheduler: ResultStage 14 (parquet at Ast.scala:324) finished in 0.031 s
23/12/04 21:33:11 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:33:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
23/12/04 21:33:11 INFO DAGScheduler: Job 11 finished: parquet at Ast.scala:324, took 0.038145 s
23/12/04 21:33:11 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/12/04 21:33:11 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:33:11 INFO DAGScheduler: Got job 12 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:33:11 INFO DAGScheduler: Final stage: ResultStage 15 (parquet at Ast.scala:324)
23/12/04 21:33:11 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:33:11 INFO DAGScheduler: Missing parents: List()
23/12/04 21:33:11 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[36] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:33:11 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:33:11 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:33:11 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.25.86.80:34641 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:33:11 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1580
23/12/04 21:33:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[36] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:33:11 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
23/12/04 21:33:11 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:33:11 INFO Executor: Running task 0.0 in stage 15.0 (TID 12)
23/12/04 21:33:11 INFO Executor: Finished task 0.0 in stage 15.0 (TID 12). 2042 bytes result sent to driver
23/12/04 21:33:11 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 16 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:33:11 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
23/12/04 21:33:11 INFO DAGScheduler: ResultStage 15 (parquet at Ast.scala:324) finished in 0.027 s
23/12/04 21:33:11 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:33:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
23/12/04 21:33:11 INFO DAGScheduler: Job 12 finished: parquet at Ast.scala:324, took 0.030210 s
23/12/04 21:33:36 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:33:36 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:33:36 INFO CodeGenerator: Code generated in 11.403558 ms
23/12/04 21:33:36 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:33:36 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:33:36 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.25.86.80:34641 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:33:36 INFO SparkContext: Created broadcast 13 from show at Ast.scala:95
23/12/04 21:33:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:33:36 INFO SparkContext: Starting job: show at Ast.scala:95
23/12/04 21:33:36 INFO DAGScheduler: Got job 13 (show at Ast.scala:95) with 1 output partitions
23/12/04 21:33:36 INFO DAGScheduler: Final stage: ResultStage 16 (show at Ast.scala:95)
23/12/04 21:33:36 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:33:36 INFO DAGScheduler: Missing parents: List()
23/12/04 21:33:36 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[40] at show at Ast.scala:95), which has no missing parents
23/12/04 21:33:36 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:33:36 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:33:36 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.25.86.80:34641 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:33:36 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1580
23/12/04 21:33:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[40] at show at Ast.scala:95) (first 15 tasks are for partitions Vector(0))
23/12/04 21:33:36 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
23/12/04 21:33:36 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 13) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:33:36 INFO Executor: Running task 0.0 in stage 16.0 (TID 13)
23/12/04 21:33:36 INFO CodeGenerator: Code generated in 13.672541 ms
23/12/04 21:33:36 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:33:36 INFO CodecPool: Got brand-new decompressor [.snappy]
23/12/04 21:33:36 INFO Executor: Finished task 0.0 in stage 16.0 (TID 13). 1928 bytes result sent to driver
23/12/04 21:33:36 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 13) in 87 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:33:36 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
23/12/04 21:33:36 INFO DAGScheduler: ResultStage 16 (show at Ast.scala:95) finished in 0.101 s
23/12/04 21:33:36 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:33:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
23/12/04 21:33:36 INFO DAGScheduler: Job 13 finished: show at Ast.scala:95, took 0.104585 s
23/12/04 21:33:36 INFO CodeGenerator: Code generated in 6.730152 ms
23/12/04 21:33:38 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:33:38 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:33:38 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:33:38 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:33:38 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.25.86.80:34641 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:33:38 INFO SparkContext: Created broadcast 15 from show at Ast.scala:96
23/12/04 21:33:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:33:38 INFO SparkContext: Starting job: show at Ast.scala:96
23/12/04 21:33:38 INFO DAGScheduler: Got job 14 (show at Ast.scala:96) with 1 output partitions
23/12/04 21:33:38 INFO DAGScheduler: Final stage: ResultStage 17 (show at Ast.scala:96)
23/12/04 21:33:38 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:33:38 INFO DAGScheduler: Missing parents: List()
23/12/04 21:33:38 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[44] at show at Ast.scala:96), which has no missing parents
23/12/04 21:33:38 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:33:38 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:33:38 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.25.86.80:34641 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:33:38 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1580
23/12/04 21:33:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[44] at show at Ast.scala:96) (first 15 tasks are for partitions Vector(0))
23/12/04 21:33:38 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
23/12/04 21:33:38 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 14) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:33:38 INFO Executor: Running task 0.0 in stage 17.0 (TID 14)
23/12/04 21:33:38 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:33:38 INFO Executor: Finished task 0.0 in stage 17.0 (TID 14). 1928 bytes result sent to driver
23/12/04 21:33:38 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 14) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:33:38 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
23/12/04 21:33:38 INFO DAGScheduler: ResultStage 17 (show at Ast.scala:96) finished in 0.023 s
23/12/04 21:33:38 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:33:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
23/12/04 21:33:38 INFO DAGScheduler: Job 14 finished: show at Ast.scala:96, took 0.025149 s
23/12/04 21:33:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:33:38 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#43)
23/12/04 21:33:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:33:38 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#51)
23/12/04 21:33:38 INFO CodeGenerator: Code generated in 9.164075 ms
23/12/04 21:33:38 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:33:39 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:33:39 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.25.86.80:34641 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:33:39 INFO SparkContext: Created broadcast 17 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:33:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:33:39 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:33:39 INFO DAGScheduler: Got job 15 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:33:39 INFO DAGScheduler: Final stage: ResultStage 18 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:33:39 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:33:39 INFO DAGScheduler: Missing parents: List()
23/12/04 21:33:39 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[48] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:33:39 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:33:39 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:33:39 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.25.86.80:34641 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:33:39 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1580
23/12/04 21:33:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[48] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:33:39 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
23/12/04 21:33:39 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 15) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:33:39 INFO Executor: Running task 0.0 in stage 18.0 (TID 15)
23/12/04 21:33:39 INFO CodeGenerator: Code generated in 10.375206 ms
23/12/04 21:33:39 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:33:39 INFO Executor: Finished task 0.0 in stage 18.0 (TID 15). 2007 bytes result sent to driver
23/12/04 21:33:39 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 15) in 52 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:33:39 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
23/12/04 21:33:39 INFO DAGScheduler: ResultStage 18 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.058 s
23/12/04 21:33:39 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:33:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
23/12/04 21:33:39 INFO DAGScheduler: Job 15 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.060327 s
23/12/04 21:33:39 INFO CodeGenerator: Code generated in 4.772821 ms
23/12/04 21:33:39 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:33:39 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:33:39 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.25.86.80:34641 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:33:39 INFO SparkContext: Created broadcast 19 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:33:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:33:39 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#43)
23/12/04 21:33:39 INFO CodeGenerator: Code generated in 14.738254 ms
23/12/04 21:33:39 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:33:39 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:33:39 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.25.86.80:34641 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:33:39 INFO SparkContext: Created broadcast 20 from show at Ast.scala:108
23/12/04 21:33:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:33:39 INFO SparkContext: Starting job: show at Ast.scala:108
23/12/04 21:33:39 INFO DAGScheduler: Got job 16 (show at Ast.scala:108) with 1 output partitions
23/12/04 21:33:39 INFO DAGScheduler: Final stage: ResultStage 19 (show at Ast.scala:108)
23/12/04 21:33:39 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:33:39 INFO DAGScheduler: Missing parents: List()
23/12/04 21:33:39 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[52] at show at Ast.scala:108), which has no missing parents
23/12/04 21:33:39 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 19.6 KiB, free 9.1 GiB)
23/12/04 21:33:39 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 9.1 GiB)
23/12/04 21:33:39 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.25.86.80:34641 (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 21:33:39 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1580
23/12/04 21:33:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[52] at show at Ast.scala:108) (first 15 tasks are for partitions Vector(0))
23/12/04 21:33:39 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
23/12/04 21:33:39 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 16) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:33:39 INFO Executor: Running task 0.0 in stage 19.0 (TID 16)
23/12/04 21:33:39 INFO CodeGenerator: Code generated in 14.080011 ms
23/12/04 21:33:39 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:33:39 INFO Executor: Finished task 0.0 in stage 19.0 (TID 16). 2118 bytes result sent to driver
23/12/04 21:33:39 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 16) in 29 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:33:39 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
23/12/04 21:33:39 INFO DAGScheduler: ResultStage 19 (show at Ast.scala:108) finished in 0.035 s
23/12/04 21:33:39 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:33:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
23/12/04 21:33:39 INFO DAGScheduler: Job 16 finished: show at Ast.scala:108, took 0.037408 s
23/12/04 21:33:42 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:33:42 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#43)
23/12/04 21:33:42 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:33:42 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#51)
23/12/04 21:33:42 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:33:42 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:33:42 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.25.86.80:34641 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:33:42 INFO SparkContext: Created broadcast 22 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:33:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:33:42 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:33:42 INFO DAGScheduler: Got job 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:33:42 INFO DAGScheduler: Final stage: ResultStage 20 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:33:42 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:33:42 INFO DAGScheduler: Missing parents: List()
23/12/04 21:33:42 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[56] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:33:42 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:33:42 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:33:42 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.25.86.80:34641 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:33:42 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1580
23/12/04 21:33:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[56] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:33:42 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
23/12/04 21:33:42 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 17) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:33:42 INFO Executor: Running task 0.0 in stage 20.0 (TID 17)
23/12/04 21:33:42 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:33:42 INFO Executor: Finished task 0.0 in stage 20.0 (TID 17). 2007 bytes result sent to driver
23/12/04 21:33:42 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 17) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:33:42 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
23/12/04 21:33:42 INFO DAGScheduler: ResultStage 20 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.015 s
23/12/04 21:33:42 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:33:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
23/12/04 21:33:42 INFO DAGScheduler: Job 17 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.016716 s
23/12/04 21:33:42 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:33:42 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:33:42 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.25.86.80:34641 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:33:42 INFO SparkContext: Created broadcast 24 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:33:42 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:33:42 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#43)
23/12/04 21:33:42 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:33:42 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:33:42 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.25.86.80:34641 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:33:42 INFO SparkContext: Created broadcast 25 from show at Ast.scala:112
23/12/04 21:33:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:33:42 INFO SparkContext: Starting job: show at Ast.scala:112
23/12/04 21:33:42 INFO DAGScheduler: Got job 18 (show at Ast.scala:112) with 1 output partitions
23/12/04 21:33:42 INFO DAGScheduler: Final stage: ResultStage 21 (show at Ast.scala:112)
23/12/04 21:33:42 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:33:42 INFO DAGScheduler: Missing parents: List()
23/12/04 21:33:42 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[60] at show at Ast.scala:112), which has no missing parents
23/12/04 21:33:42 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 19.6 KiB, free 9.1 GiB)
23/12/04 21:33:42 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 9.1 GiB)
23/12/04 21:33:42 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.25.86.80:34641 (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 21:33:42 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1580
23/12/04 21:33:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[60] at show at Ast.scala:112) (first 15 tasks are for partitions Vector(0))
23/12/04 21:33:42 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
23/12/04 21:33:42 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 18) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:33:42 INFO Executor: Running task 0.0 in stage 21.0 (TID 18)
23/12/04 21:33:42 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:33:42 INFO Executor: Finished task 0.0 in stage 21.0 (TID 18). 2118 bytes result sent to driver
23/12/04 21:33:42 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 18) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:33:42 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
23/12/04 21:33:42 INFO DAGScheduler: ResultStage 21 (show at Ast.scala:112) finished in 0.017 s
23/12/04 21:33:42 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:33:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
23/12/04 21:33:42 INFO DAGScheduler: Job 18 finished: show at Ast.scala:112, took 0.019084 s
23/12/04 21:33:43 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:33:43 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:33:43 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:33:43 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:33:43 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.25.86.80:34641 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:33:43 INFO SparkContext: Created broadcast 27 from show at Ast.scala:113
23/12/04 21:33:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:33:43 INFO SparkContext: Starting job: show at Ast.scala:113
23/12/04 21:33:43 INFO DAGScheduler: Got job 19 (show at Ast.scala:113) with 1 output partitions
23/12/04 21:33:43 INFO DAGScheduler: Final stage: ResultStage 22 (show at Ast.scala:113)
23/12/04 21:33:43 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:33:43 INFO DAGScheduler: Missing parents: List()
23/12/04 21:33:43 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[64] at show at Ast.scala:113), which has no missing parents
23/12/04 21:33:43 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:33:43 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:33:43 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.25.86.80:34641 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:33:43 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1580
23/12/04 21:33:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[64] at show at Ast.scala:113) (first 15 tasks are for partitions Vector(0))
23/12/04 21:33:43 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
23/12/04 21:33:43 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 19) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:33:43 INFO Executor: Running task 0.0 in stage 22.0 (TID 19)
23/12/04 21:33:43 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:33:43 INFO Executor: Finished task 0.0 in stage 22.0 (TID 19). 1928 bytes result sent to driver
23/12/04 21:33:43 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 19) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:33:43 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
23/12/04 21:33:43 INFO DAGScheduler: ResultStage 22 (show at Ast.scala:113) finished in 0.016 s
23/12/04 21:33:43 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:33:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
23/12/04 21:33:43 INFO DAGScheduler: Job 19 finished: show at Ast.scala:113, took 0.017572 s
23/12/04 21:33:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:33:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#43)
23/12/04 21:33:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:33:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#51)
23/12/04 21:33:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:33:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#59)
23/12/04 21:33:43 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:33:43 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:33:43 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.25.86.80:34641 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:33:43 INFO SparkContext: Created broadcast 29 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:33:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:33:43 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:33:43 INFO DAGScheduler: Got job 20 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:33:43 INFO DAGScheduler: Final stage: ResultStage 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:33:43 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:33:43 INFO DAGScheduler: Missing parents: List()
23/12/04 21:33:43 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[68] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:33:43 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:33:43 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:33:43 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.25.86.80:34641 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:33:43 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1580
23/12/04 21:33:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[68] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:33:43 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
23/12/04 21:33:43 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 20) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:33:43 INFO Executor: Running task 0.0 in stage 23.0 (TID 20)
23/12/04 21:33:43 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:33:43 INFO Executor: Finished task 0.0 in stage 23.0 (TID 20). 2007 bytes result sent to driver
23/12/04 21:33:43 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 20) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:33:43 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
23/12/04 21:33:43 INFO DAGScheduler: ResultStage 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.015 s
23/12/04 21:33:43 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:33:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
23/12/04 21:33:43 INFO DAGScheduler: Job 20 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.016461 s
23/12/04 21:33:43 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:33:43 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:33:43 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.25.86.80:34641 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:33:43 INFO SparkContext: Created broadcast 31 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:33:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:33:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#43)
23/12/04 21:33:43 INFO CodeGenerator: Code generated in 10.239646 ms
23/12/04 21:33:43 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:33:43 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:33:43 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.25.86.80:34641 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:33:43 INFO SparkContext: Created broadcast 32 from show at Ast.scala:126
23/12/04 21:33:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:33:43 INFO SparkContext: Starting job: show at Ast.scala:126
23/12/04 21:33:43 INFO DAGScheduler: Got job 21 (show at Ast.scala:126) with 1 output partitions
23/12/04 21:33:43 INFO DAGScheduler: Final stage: ResultStage 24 (show at Ast.scala:126)
23/12/04 21:33:43 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:33:43 INFO DAGScheduler: Missing parents: List()
23/12/04 21:33:43 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[72] at show at Ast.scala:126), which has no missing parents
23/12/04 21:33:43 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 22.4 KiB, free 9.1 GiB)
23/12/04 21:33:43 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 9.1 GiB)
23/12/04 21:33:43 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.25.86.80:34641 (size: 8.6 KiB, free: 9.1 GiB)
23/12/04 21:33:43 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1580
23/12/04 21:33:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[72] at show at Ast.scala:126) (first 15 tasks are for partitions Vector(0))
23/12/04 21:33:43 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
23/12/04 21:33:43 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 21) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:33:43 INFO Executor: Running task 0.0 in stage 24.0 (TID 21)
23/12/04 21:33:43 INFO CodeGenerator: Code generated in 14.23634 ms
23/12/04 21:33:43 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:33:43 INFO Executor: Finished task 0.0 in stage 24.0 (TID 21). 2172 bytes result sent to driver
23/12/04 21:33:43 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 21) in 26 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:33:43 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
23/12/04 21:33:43 INFO DAGScheduler: ResultStage 24 (show at Ast.scala:126) finished in 0.031 s
23/12/04 21:33:43 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:33:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
23/12/04 21:33:43 INFO DAGScheduler: Job 21 finished: show at Ast.scala:126, took 0.033178 s
23/12/04 21:34:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:34:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#43)
23/12/04 21:34:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:34:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#51)
23/12/04 21:34:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:34:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#59)
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.25.86.80:34641 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO SparkContext: Created broadcast 34 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:34:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:34:48 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:34:48 INFO DAGScheduler: Got job 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:34:48 INFO DAGScheduler: Final stage: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:34:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:34:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:34:48 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[76] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.25.86.80:34641 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1580
23/12/04 21:34:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[76] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:34:48 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
23/12/04 21:34:48 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 22) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:34:48 INFO Executor: Running task 0.0 in stage 25.0 (TID 22)
23/12/04 21:34:48 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:34:48 INFO Executor: Finished task 0.0 in stage 25.0 (TID 22). 2007 bytes result sent to driver
23/12/04 21:34:48 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 22) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:34:48 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
23/12/04 21:34:48 INFO DAGScheduler: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.014 s
23/12/04 21:34:48 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:34:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
23/12/04 21:34:48 INFO DAGScheduler: Job 22 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.014983 s
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.25.86.80:34641 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:34:48 INFO SparkContext: Created broadcast 36 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:34:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:34:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#43)
23/12/04 21:34:48 INFO CodeGenerator: Code generated in 8.811682 ms
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.25.86.80:34641 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO SparkContext: Created broadcast 37 from collect at Ast.scala:298
23/12/04 21:34:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:34:48 INFO CodeGenerator: Code generated in 3.497262 ms
23/12/04 21:34:48 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:34:48 INFO DAGScheduler: Got job 23 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:34:48 INFO DAGScheduler: Final stage: ResultStage 26 (collect at Ast.scala:298)
23/12/04 21:34:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:34:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:34:48 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[82] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 25.1 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.25.86.80:34641 (size: 9.8 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1580
23/12/04 21:34:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[82] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:34:48 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
23/12/04 21:34:48 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 23) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:34:48 INFO Executor: Running task 0.0 in stage 26.0 (TID 23)
23/12/04 21:34:48 INFO CodeGenerator: Code generated in 10.799979 ms
23/12/04 21:34:48 INFO CodeGenerator: Code generated in 4.488583 ms
23/12/04 21:34:48 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:34:48 INFO Executor: Finished task 0.0 in stage 26.0 (TID 23). 2331 bytes result sent to driver
23/12/04 21:34:48 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 23) in 38 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:34:48 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
23/12/04 21:34:48 INFO DAGScheduler: ResultStage 26 (collect at Ast.scala:298) finished in 0.045 s
23/12/04 21:34:48 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:34:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
23/12/04 21:34:48 INFO DAGScheduler: Job 23 finished: collect at Ast.scala:298, took 0.048388 s
23/12/04 21:34:48 INFO DAGScheduler: Registering RDD 83 (collect at Ast.scala:298) as input to shuffle 3
23/12/04 21:34:48 INFO DAGScheduler: Got map stage job 24 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:34:48 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (collect at Ast.scala:298)
23/12/04 21:34:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:34:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:34:48 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[83] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 27.2 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.25.86.80:34641 (size: 10.9 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1580
23/12/04 21:34:48 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[83] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:34:48 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
23/12/04 21:34:48 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 24) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8468 bytes) 
23/12/04 21:34:48 INFO Executor: Running task 0.0 in stage 27.0 (TID 24)
23/12/04 21:34:48 INFO CodeGenerator: Code generated in 3.571229 ms
23/12/04 21:34:48 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:34:48 INFO Executor: Finished task 0.0 in stage 27.0 (TID 24). 2298 bytes result sent to driver
23/12/04 21:34:48 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 24) in 18 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:34:48 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
23/12/04 21:34:48 INFO DAGScheduler: ShuffleMapStage 27 (collect at Ast.scala:298) finished in 0.024 s
23/12/04 21:34:48 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:34:48 INFO DAGScheduler: running: HashSet()
23/12/04 21:34:48 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:34:48 INFO DAGScheduler: failed: HashSet()
23/12/04 21:34:48 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:34:48 INFO CodeGenerator: Code generated in 5.408057 ms
23/12/04 21:34:48 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:34:48 INFO DAGScheduler: Got job 25 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:34:48 INFO DAGScheduler: Final stage: ResultStage 29 (collect at Ast.scala:298)
23/12/04 21:34:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
23/12/04 21:34:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:34:48 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[86] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 38.1 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.25.86.80:34641 (size: 16.4 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1580
23/12/04 21:34:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[86] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:34:48 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
23/12/04 21:34:48 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 25) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:34:48 INFO Executor: Running task 0.0 in stage 29.0 (TID 25)
23/12/04 21:34:48 INFO ShuffleBlockFetcherIterator: Getting 1 (432.0 B) non-empty blocks including 1 (432.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:34:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 21:34:48 INFO CodeGenerator: Code generated in 5.940109 ms
23/12/04 21:34:48 INFO CodeGenerator: Code generated in 6.776484 ms
23/12/04 21:34:48 INFO Executor: Finished task 0.0 in stage 29.0 (TID 25). 5956 bytes result sent to driver
23/12/04 21:34:48 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 25) in 27 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:34:48 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
23/12/04 21:34:48 INFO DAGScheduler: ResultStage 29 (collect at Ast.scala:298) finished in 0.031 s
23/12/04 21:34:48 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:34:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
23/12/04 21:34:48 INFO DAGScheduler: Job 25 finished: collect at Ast.scala:298, took 0.033134 s
23/12/04 21:34:48 INFO CodeGenerator: Code generated in 7.939267 ms
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 10.25.86.80:34641 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 10.25.86.80:34641 in memory (size: 10.9 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.25.86.80:34641 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 10.25.86.80:34641 in memory (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 10.25.86.80:34641 in memory (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 10.25.86.80:34641 in memory (size: 16.4 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 10.25.86.80:34641 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 10.25.86.80:34641 in memory (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 10.25.86.80:34641 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 10.25.86.80:34641 in memory (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:34:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#43)
23/12/04 21:34:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:34:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#51)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 10.25.86.80:34641 in memory (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:34:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:34:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#59)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 10.25.86.80:34641 in memory (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 10.25.86.80:34641 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 10.25.86.80:34641 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.25.86.80:34641 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.25.86.80:34641 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 10.25.86.80:34641 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 10.25.86.80:34641 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 10.25.86.80:34641 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 10.25.86.80:34641 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 10.25.86.80:34641 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 10.25.86.80:34641 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 10.25.86.80:34641 in memory (size: 9.8 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 10.25.86.80:34641 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.25.86.80:34641 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO SparkContext: Created broadcast 41 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:34:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 10.25.86.80:34641 in memory (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 10.25.86.80:34641 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 10.25.86.80:34641 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 10.25.86.80:34641 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.25.86.80:34641 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:34:48 INFO DAGScheduler: Got job 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:34:48 INFO DAGScheduler: Final stage: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:34:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:34:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:34:48 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[90] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 10.25.86.80:34641 in memory (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 10.25.86.80:34641 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.25.86.80:34641 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1580
23/12/04 21:34:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[90] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:34:48 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.25.86.80:34641 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 26) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:34:48 INFO Executor: Running task 0.0 in stage 30.0 (TID 26)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.25.86.80:34641 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 10.25.86.80:34641 in memory (size: 8.6 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:34:48 INFO Executor: Finished task 0.0 in stage 30.0 (TID 26). 2007 bytes result sent to driver
23/12/04 21:34:48 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 26) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:34:48 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
23/12/04 21:34:48 INFO DAGScheduler: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.014 s
23/12/04 21:34:48 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:34:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
23/12/04 21:34:48 INFO DAGScheduler: Job 26 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.015776 s
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.25.86.80:34641 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:34:48 INFO SparkContext: Created broadcast 43 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:34:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:34:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#43)
23/12/04 21:34:48 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/04 21:34:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/12/04 21:34:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/12/04 21:34:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/04 21:34:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/12/04 21:34:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/12/04 21:34:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.25.86.80:34641 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO SparkContext: Created broadcast 44 from parquet at Ast.scala:308
23/12/04 21:34:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:34:48 INFO SparkContext: Starting job: parquet at Ast.scala:308
23/12/04 21:34:48 INFO DAGScheduler: Got job 27 (parquet at Ast.scala:308) with 1 output partitions
23/12/04 21:34:48 INFO DAGScheduler: Final stage: ResultStage 31 (parquet at Ast.scala:308)
23/12/04 21:34:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:34:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:34:48 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[95] at parquet at Ast.scala:308), which has no missing parents
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 223.9 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 79.6 KiB, free 9.1 GiB)
23/12/04 21:34:48 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.25.86.80:34641 (size: 79.6 KiB, free: 9.1 GiB)
23/12/04 21:34:48 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1580
23/12/04 21:34:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[95] at parquet at Ast.scala:308) (first 15 tasks are for partitions Vector(0))
23/12/04 21:34:48 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
23/12/04 21:34:48 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 27) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8809 bytes) 
23/12/04 21:34:48 INFO Executor: Running task 0.0 in stage 31.0 (TID 27)
23/12/04 21:34:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/12/04 21:34:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/12/04 21:34:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/04 21:34:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/12/04 21:34:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/12/04 21:34:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/12/04 21:34:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "index",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "doubleField",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "stringField",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "intField",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 index;
  optional double doubleField;
  optional binary stringField (STRING);
  optional int32 intField;
}

       
23/12/04 21:34:48 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:34:48 INFO FileOutputCommitter: Saved output of task 'attempt_202312042134486892937523548847155_0031_m_000000_27' to file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/6ed6590ed5b0d3977c42a362e765ef102e2b3acec91e0482e0ab60862c44369d.parquet/_temporary/0/task_202312042134486892937523548847155_0031_m_000000
23/12/04 21:34:48 INFO SparkHadoopMapRedUtil: attempt_202312042134486892937523548847155_0031_m_000000_27: Committed. Elapsed time: 0 ms.
23/12/04 21:34:48 INFO Executor: Finished task 0.0 in stage 31.0 (TID 27). 3204 bytes result sent to driver
23/12/04 21:34:48 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 27) in 32 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:34:48 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
23/12/04 21:34:48 INFO DAGScheduler: ResultStage 31 (parquet at Ast.scala:308) finished in 0.050 s
23/12/04 21:34:48 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:34:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
23/12/04 21:34:48 INFO DAGScheduler: Job 27 finished: parquet at Ast.scala:308, took 0.051567 s
23/12/04 21:34:48 INFO FileFormatWriter: Start to commit write Job 69b55abd-16b1-4a78-8bfd-0c9388e8a37e.
23/12/04 21:34:48 INFO FileFormatWriter: Write Job 69b55abd-16b1-4a78-8bfd-0c9388e8a37e committed. Elapsed time: 17 ms.
23/12/04 21:34:48 INFO FileFormatWriter: Finished processing stats for write job 69b55abd-16b1-4a78-8bfd-0c9388e8a37e.
23/12/04 21:35:14 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
23/12/04 21:35:14 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:35:14 INFO DAGScheduler: Got job 28 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:35:14 INFO DAGScheduler: Final stage: ResultStage 32 (parquet at Ast.scala:324)
23/12/04 21:35:14 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:35:14 INFO DAGScheduler: Missing parents: List()
23/12/04 21:35:14 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[97] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:35:14 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:35:14 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:35:14 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.25.86.80:34641 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:35:14 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1580
23/12/04 21:35:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[97] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:35:14 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
23/12/04 21:35:14 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 28) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:35:14 INFO Executor: Running task 0.0 in stage 32.0 (TID 28)
23/12/04 21:35:14 INFO Executor: Finished task 0.0 in stage 32.0 (TID 28). 2042 bytes result sent to driver
23/12/04 21:35:14 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 28) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:35:14 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
23/12/04 21:35:14 INFO DAGScheduler: ResultStage 32 (parquet at Ast.scala:324) finished in 0.019 s
23/12/04 21:35:14 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:35:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
23/12/04 21:35:14 INFO DAGScheduler: Job 28 finished: parquet at Ast.scala:324, took 0.020366 s
23/12/04 21:36:21 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:36:21 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:36:21 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:36:21 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:36:21 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.25.86.80:34641 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:36:21 INFO SparkContext: Created broadcast 47 from show at IntegrationSuite.scala:252
23/12/04 21:36:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:36:21 INFO SparkContext: Starting job: show at IntegrationSuite.scala:252
23/12/04 21:36:21 INFO DAGScheduler: Got job 29 (show at IntegrationSuite.scala:252) with 1 output partitions
23/12/04 21:36:21 INFO DAGScheduler: Final stage: ResultStage 33 (show at IntegrationSuite.scala:252)
23/12/04 21:36:21 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:36:21 INFO DAGScheduler: Missing parents: List()
23/12/04 21:36:21 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[101] at show at IntegrationSuite.scala:252), which has no missing parents
23/12/04 21:36:21 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:36:21 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:36:21 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.25.86.80:34641 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:36:21 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1580
23/12/04 21:36:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[101] at show at IntegrationSuite.scala:252) (first 15 tasks are for partitions Vector(0))
23/12/04 21:36:21 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
23/12/04 21:36:21 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 29) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:36:21 INFO Executor: Running task 0.0 in stage 33.0 (TID 29)
23/12/04 21:36:21 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/6ed6590ed5b0d3977c42a362e765ef102e2b3acec91e0482e0ab60862c44369d.parquet/part-00000-2be52c4c-cf66-40fd-bb80-45a28c37972e-c000.snappy.parquet, range: 0-1302, partition values: [empty row]
23/12/04 21:36:21 INFO Executor: Finished task 0.0 in stage 33.0 (TID 29). 1961 bytes result sent to driver
23/12/04 21:36:21 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 29) in 7 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:36:21 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
23/12/04 21:36:21 INFO DAGScheduler: ResultStage 33 (show at IntegrationSuite.scala:252) finished in 0.011 s
23/12/04 21:36:21 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:36:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
23/12/04 21:36:21 INFO DAGScheduler: Job 29 finished: show at IntegrationSuite.scala:252, took 0.011848 s
23/12/04 21:36:39 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:36:39 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:36:39 INFO CodeGenerator: Code generated in 5.019218 ms
23/12/04 21:36:39 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 200.4 KiB, free 9.1 GiB)
23/12/04 21:36:39 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 9.1 GiB)
23/12/04 21:36:39 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.25.86.80:34641 (size: 34.8 KiB, free: 9.1 GiB)
23/12/04 21:36:39 INFO SparkContext: Created broadcast 49 from count at IntegrationSuite.scala:254
23/12/04 21:36:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:36:39 INFO DAGScheduler: Registering RDD 105 (count at IntegrationSuite.scala:254) as input to shuffle 4
23/12/04 21:36:39 INFO DAGScheduler: Got map stage job 30 (count at IntegrationSuite.scala:254) with 1 output partitions
23/12/04 21:36:39 INFO DAGScheduler: Final stage: ShuffleMapStage 34 (count at IntegrationSuite.scala:254)
23/12/04 21:36:39 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:36:39 INFO DAGScheduler: Missing parents: List()
23/12/04 21:36:39 INFO DAGScheduler: Submitting ShuffleMapStage 34 (MapPartitionsRDD[105] at count at IntegrationSuite.scala:254), which has no missing parents
23/12/04 21:36:39 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 17.5 KiB, free 9.1 GiB)
23/12/04 21:36:39 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 9.1 GiB)
23/12/04 21:36:39 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.25.86.80:34641 (size: 7.9 KiB, free: 9.1 GiB)
23/12/04 21:36:39 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1580
23/12/04 21:36:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[105] at count at IntegrationSuite.scala:254) (first 15 tasks are for partitions Vector(0))
23/12/04 21:36:39 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
23/12/04 21:36:39 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 30) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8468 bytes) 
23/12/04 21:36:39 INFO Executor: Running task 0.0 in stage 34.0 (TID 30)
23/12/04 21:36:39 INFO CodeGenerator: Code generated in 4.322172 ms
23/12/04 21:36:39 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/6ed6590ed5b0d3977c42a362e765ef102e2b3acec91e0482e0ab60862c44369d.parquet/part-00000-2be52c4c-cf66-40fd-bb80-45a28c37972e-c000.snappy.parquet, range: 0-1302, partition values: [empty row]
23/12/04 21:36:39 INFO Executor: Finished task 0.0 in stage 34.0 (TID 30). 2224 bytes result sent to driver
23/12/04 21:36:39 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 30) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:36:39 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
23/12/04 21:36:39 INFO DAGScheduler: ShuffleMapStage 34 (count at IntegrationSuite.scala:254) finished in 0.017 s
23/12/04 21:36:39 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:36:39 INFO DAGScheduler: running: HashSet()
23/12/04 21:36:39 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:36:39 INFO DAGScheduler: failed: HashSet()
23/12/04 21:36:39 INFO CodeGenerator: Code generated in 4.028001 ms
23/12/04 21:36:39 INFO SparkContext: Starting job: count at IntegrationSuite.scala:254
23/12/04 21:36:39 INFO DAGScheduler: Got job 31 (count at IntegrationSuite.scala:254) with 1 output partitions
23/12/04 21:36:39 INFO DAGScheduler: Final stage: ResultStage 36 (count at IntegrationSuite.scala:254)
23/12/04 21:36:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 35)
23/12/04 21:36:39 INFO DAGScheduler: Missing parents: List()
23/12/04 21:36:39 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[108] at count at IntegrationSuite.scala:254), which has no missing parents
23/12/04 21:36:39 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 13.1 KiB, free 9.1 GiB)
23/12/04 21:36:39 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 9.1 GiB)
23/12/04 21:36:39 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.25.86.80:34641 (size: 6.1 KiB, free: 9.1 GiB)
23/12/04 21:36:39 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1580
23/12/04 21:36:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[108] at count at IntegrationSuite.scala:254) (first 15 tasks are for partitions Vector(0))
23/12/04 21:36:39 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
23/12/04 21:36:39 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 31) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:36:39 INFO Executor: Running task 0.0 in stage 36.0 (TID 31)
23/12/04 21:36:39 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:36:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 21:36:39 INFO CodeGenerator: Code generated in 4.445938 ms
23/12/04 21:36:39 INFO Executor: Finished task 0.0 in stage 36.0 (TID 31). 4084 bytes result sent to driver
23/12/04 21:36:39 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 31) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:36:39 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
23/12/04 21:36:39 INFO DAGScheduler: ResultStage 36 (count at IntegrationSuite.scala:254) finished in 0.013 s
23/12/04 21:36:39 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:36:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
23/12/04 21:36:39 INFO DAGScheduler: Job 31 finished: count at IntegrationSuite.scala:254, took 0.014514 s
23/12/04 21:36:39 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 21:36:39 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 21:36:39 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 21:36:39 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4040
23/12/04 21:36:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 21:36:39 INFO MemoryStore: MemoryStore cleared
23/12/04 21:36:39 INFO BlockManager: BlockManager stopped
23/12/04 21:36:39 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 21:36:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 21:36:39 INFO SparkContext: Successfully stopped SparkContext
23/12/04 21:36:39 INFO ShutdownHookManager: Shutdown hook called
23/12/04 21:36:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-23b32231-c2e9-41b5-99a2-61ee274e8e84
23/12/04 21:37:42 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 21:37:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 21:37:42 INFO SparkContext: Running Spark version 3.5.0
23/12/04 21:37:42 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 21:37:42 INFO SparkContext: Java version 11.0.20
23/12/04 21:37:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 21:37:42 INFO ResourceUtils: ==============================================================
23/12/04 21:37:42 INFO ResourceUtils: No custom resources configured for spark.driver.
23/12/04 21:37:42 INFO ResourceUtils: ==============================================================
23/12/04 21:37:42 INFO SparkContext: Submitted application: Spark dataframes for processing in startup.ast
23/12/04 21:37:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/12/04 21:37:42 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/12/04 21:37:42 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/12/04 21:37:42 INFO SecurityManager: Changing view acls to: bsoleille
23/12/04 21:37:42 INFO SecurityManager: Changing modify acls to: bsoleille
23/12/04 21:37:42 INFO SecurityManager: Changing view acls groups to: 
23/12/04 21:37:42 INFO SecurityManager: Changing modify acls groups to: 
23/12/04 21:37:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: bsoleille; groups with view permissions: EMPTY; users with modify permissions: bsoleille; groups with modify permissions: EMPTY
23/12/04 21:37:42 INFO Utils: Successfully started service 'sparkDriver' on port 42683.
23/12/04 21:37:42 INFO SparkEnv: Registering MapOutputTracker
23/12/04 21:37:42 INFO SparkEnv: Registering BlockManagerMaster
23/12/04 21:37:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/12/04 21:37:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/12/04 21:37:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/12/04 21:37:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fbd1ef83-b54e-4921-b7d0-84d86b2ba1c3
23/12/04 21:37:42 INFO MemoryStore: MemoryStore started with capacity 9.1 GiB
23/12/04 21:37:42 INFO SparkEnv: Registering OutputCommitCoordinator
23/12/04 21:37:42 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/12/04 21:37:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/12/04 21:37:43 INFO Executor: Starting executor ID driver on host 10.25.86.80
23/12/04 21:37:43 INFO Executor: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 21:37:43 INFO Executor: Java version 11.0.20
23/12/04 21:37:43 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/12/04 21:37:43 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@40023e5c for default.
23/12/04 21:37:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45457.
23/12/04 21:37:43 INFO NettyBlockTransferService: Server created on 10.25.86.80:45457
23/12/04 21:37:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/12/04 21:37:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.25.86.80, 45457, None)
23/12/04 21:37:43 INFO BlockManagerMasterEndpoint: Registering block manager 10.25.86.80:45457 with 9.1 GiB RAM, BlockManagerId(driver, 10.25.86.80, 45457, None)
23/12/04 21:37:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.25.86.80, 45457, None)
23/12/04 21:37:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.25.86.80, 45457, None)
23/12/04 21:37:43 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 21:37:43 INFO xnio: XNIO version 3.8.7.Final
23/12/04 21:37:43 INFO nio: XNIO NIO Implementation Version 3.8.7.Final
23/12/04 21:37:43 INFO threads: JBoss Threads version 3.1.0.Final
23/12/04 21:37:43 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 21:37:43 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 21:37:43 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 21:37:43 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 21:37:43 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 21:37:43 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 21:37:43 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 21:37:43 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 21:37:43 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 21:37:43 INFO undertow: starting server: Undertow - 2.2.20.Final
23/12/04 21:37:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/12/04 21:37:43 INFO SharedState: Warehouse path is 'file:/home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/spark-warehouse'.
23/12/04 21:37:45 INFO CodeGenerator: Code generated in 126.452784 ms
23/12/04 21:37:45 INFO CodeGenerator: Code generated in 14.259764 ms
23/12/04 21:37:45 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:37:45 INFO DAGScheduler: Got job 0 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:37:45 INFO DAGScheduler: Final stage: ResultStage 0 (collect at Ast.scala:298)
23/12/04 21:37:45 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:45 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:37:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 21:37:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 21:37:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.25.86.80:45457 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:37:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/12/04 21:37:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 21:37:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/12/04 21:37:45 INFO CodeGenerator: Code generated in 8.046246 ms
23/12/04 21:37:45 INFO CodeGenerator: Code generated in 6.471521 ms
23/12/04 21:37:46 INFO CodeGenerator: Code generated in 33.644952 ms
23/12/04 21:37:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1869 bytes result sent to driver
23/12/04 21:37:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 738 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/12/04 21:37:46 INFO DAGScheduler: ResultStage 0 (collect at Ast.scala:298) finished in 0.832 s
23/12/04 21:37:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/12/04 21:37:46 INFO DAGScheduler: Job 0 finished: collect at Ast.scala:298, took 0.860244 s
23/12/04 21:37:46 INFO DAGScheduler: Registering RDD 8 (collect at Ast.scala:298) as input to shuffle 0
23/12/04 21:37:46 INFO DAGScheduler: Got map stage job 1 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:37:46 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (collect at Ast.scala:298)
23/12/04 21:37:46 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:46 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:46 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.25.86.80:45457 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:37:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:46 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/12/04 21:37:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 21:37:46 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/12/04 21:37:46 INFO CodeGenerator: Code generated in 8.096068 ms
23/12/04 21:37:46 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1836 bytes result sent to driver
23/12/04 21:37:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 64 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:46 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/12/04 21:37:46 INFO DAGScheduler: ShuffleMapStage 1 (collect at Ast.scala:298) finished in 0.081 s
23/12/04 21:37:46 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:37:46 INFO DAGScheduler: running: HashSet()
23/12/04 21:37:46 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:37:46 INFO DAGScheduler: failed: HashSet()
23/12/04 21:37:46 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:37:46 INFO CodeGenerator: Code generated in 9.429707 ms
23/12/04 21:37:46 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:37:46 INFO DAGScheduler: Got job 2 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:37:46 INFO DAGScheduler: Final stage: ResultStage 3 (collect at Ast.scala:298)
23/12/04 21:37:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
23/12/04 21:37:46 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:46 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.25.86.80:45457 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:37:46 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:46 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
23/12/04 21:37:46 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:37:46 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
23/12/04 21:37:46 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:37:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
23/12/04 21:37:46 INFO CodeGenerator: Code generated in 6.7383 ms
23/12/04 21:37:46 INFO CodeGenerator: Code generated in 4.844705 ms
23/12/04 21:37:46 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4537 bytes result sent to driver
23/12/04 21:37:46 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 92 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:46 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
23/12/04 21:37:46 INFO DAGScheduler: ResultStage 3 (collect at Ast.scala:298) finished in 0.107 s
23/12/04 21:37:46 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
23/12/04 21:37:46 INFO DAGScheduler: Job 2 finished: collect at Ast.scala:298, took 0.120124 s
23/12/04 21:37:46 INFO CodeGenerator: Code generated in 8.306575 ms
23/12/04 21:37:46 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:37:46 INFO DAGScheduler: Got job 3 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:37:46 INFO DAGScheduler: Final stage: ResultStage 4 (collect at Ast.scala:298)
23/12/04 21:37:46 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:46 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:46 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.25.86.80:45457 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:37:46 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:46 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/12/04 21:37:46 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8149 bytes) 
23/12/04 21:37:46 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
23/12/04 21:37:46 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 1826 bytes result sent to driver
23/12/04 21:37:46 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 15 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:46 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/12/04 21:37:46 INFO DAGScheduler: ResultStage 4 (collect at Ast.scala:298) finished in 0.021 s
23/12/04 21:37:46 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/12/04 21:37:46 INFO DAGScheduler: Job 3 finished: collect at Ast.scala:298, took 0.024511 s
23/12/04 21:37:46 INFO DAGScheduler: Registering RDD 16 (collect at Ast.scala:298) as input to shuffle 1
23/12/04 21:37:46 INFO DAGScheduler: Got map stage job 4 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:37:46 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (collect at Ast.scala:298)
23/12/04 21:37:46 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:46 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:46 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.25.86.80:45457 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:37:46 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[16] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:46 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
23/12/04 21:37:46 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8138 bytes) 
23/12/04 21:37:46 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
23/12/04 21:37:46 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1793 bytes result sent to driver
23/12/04 21:37:46 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 14 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:46 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
23/12/04 21:37:46 INFO DAGScheduler: ShuffleMapStage 5 (collect at Ast.scala:298) finished in 0.023 s
23/12/04 21:37:46 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:37:46 INFO DAGScheduler: running: HashSet()
23/12/04 21:37:46 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:37:46 INFO DAGScheduler: failed: HashSet()
23/12/04 21:37:46 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:37:46 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:37:46 INFO DAGScheduler: Got job 5 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:37:46 INFO DAGScheduler: Final stage: ResultStage 7 (collect at Ast.scala:298)
23/12/04 21:37:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
23/12/04 21:37:46 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:46 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.25.86.80:45457 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:37:46 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:46 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
23/12/04 21:37:46 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:37:46 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)
23/12/04 21:37:46 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:37:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 21:37:46 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 4494 bytes result sent to driver
23/12/04 21:37:46 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:46 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
23/12/04 21:37:46 INFO DAGScheduler: ResultStage 7 (collect at Ast.scala:298) finished in 0.019 s
23/12/04 21:37:46 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
23/12/04 21:37:46 INFO DAGScheduler: Job 5 finished: collect at Ast.scala:298, took 0.021433 s
23/12/04 21:37:46 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:37:46 INFO DAGScheduler: Got job 6 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:37:46 INFO DAGScheduler: Final stage: ResultStage 8 (collect at Ast.scala:298)
23/12/04 21:37:46 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:46 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:46 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 17.4 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.25.86.80:45457 (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:37:46 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[23] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:46 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
23/12/04 21:37:46 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8154 bytes) 
23/12/04 21:37:46 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)
23/12/04 21:37:46 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 1826 bytes result sent to driver
23/12/04 21:37:46 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:46 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
23/12/04 21:37:46 INFO DAGScheduler: ResultStage 8 (collect at Ast.scala:298) finished in 0.018 s
23/12/04 21:37:46 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
23/12/04 21:37:46 INFO DAGScheduler: Job 6 finished: collect at Ast.scala:298, took 0.019843 s
23/12/04 21:37:46 INFO DAGScheduler: Registering RDD 24 (collect at Ast.scala:298) as input to shuffle 2
23/12/04 21:37:46 INFO DAGScheduler: Got map stage job 7 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:37:46 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (collect at Ast.scala:298)
23/12/04 21:37:46 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:46 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:46 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 18.9 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.25.86.80:45457 (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:37:46 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:46 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
23/12/04 21:37:46 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8143 bytes) 
23/12/04 21:37:46 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)
23/12/04 21:37:46 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 1793 bytes result sent to driver
23/12/04 21:37:46 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:46 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
23/12/04 21:37:46 INFO DAGScheduler: ShuffleMapStage 9 (collect at Ast.scala:298) finished in 0.020 s
23/12/04 21:37:46 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:37:46 INFO DAGScheduler: running: HashSet()
23/12/04 21:37:46 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:37:46 INFO DAGScheduler: failed: HashSet()
23/12/04 21:37:46 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:37:46 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:37:46 INFO DAGScheduler: Got job 8 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:37:46 INFO DAGScheduler: Final stage: ResultStage 11 (collect at Ast.scala:298)
23/12/04 21:37:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
23/12/04 21:37:46 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:46 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.8 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 9.1 GiB)
23/12/04 21:37:46 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.25.86.80:45457 (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:37:46 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[27] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:46 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
23/12/04 21:37:46 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:37:46 INFO Executor: Running task 0.0 in stage 11.0 (TID 8)
23/12/04 21:37:46 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:37:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 21:37:46 INFO Executor: Finished task 0.0 in stage 11.0 (TID 8). 4494 bytes result sent to driver
23/12/04 21:37:46 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:46 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
23/12/04 21:37:46 INFO DAGScheduler: ResultStage 11 (collect at Ast.scala:298) finished in 0.017 s
23/12/04 21:37:46 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
23/12/04 21:37:46 INFO DAGScheduler: Job 8 finished: collect at Ast.scala:298, took 0.019450 s
23/12/04 21:37:46 INFO InMemoryFileIndex: It took 18 ms to list leaf files for 1 paths.
23/12/04 21:37:47 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:37:47 INFO DAGScheduler: Got job 9 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:37:47 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at Ast.scala:324)
23/12/04 21:37:47 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:47 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:47 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:37:47 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:37:47 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:37:47 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.25.86.80:45457 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[29] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:47 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
23/12/04 21:37:47 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:37:47 INFO Executor: Running task 0.0 in stage 12.0 (TID 9)
23/12/04 21:37:47 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.25.86.80:45457 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.25.86.80:45457 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.25.86.80:45457 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.25.86.80:45457 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.25.86.80:45457 in memory (size: 8.1 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.25.86.80:45457 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.25.86.80:45457 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.25.86.80:45457 in memory (size: 10.7 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.25.86.80:45457 in memory (size: 7.3 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO Executor: Finished task 0.0 in stage 12.0 (TID 9). 2128 bytes result sent to driver
23/12/04 21:37:47 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 256 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:47 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
23/12/04 21:37:47 INFO DAGScheduler: ResultStage 12 (parquet at Ast.scala:324) finished in 0.268 s
23/12/04 21:37:47 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
23/12/04 21:37:47 INFO DAGScheduler: Job 9 finished: parquet at Ast.scala:324, took 0.269598 s
23/12/04 21:37:47 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/12/04 21:37:47 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:37:47 INFO DAGScheduler: Got job 10 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:37:47 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at Ast.scala:324)
23/12/04 21:37:47 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:47 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:47 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[31] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:37:47 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:37:47 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:37:47 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.25.86.80:45457 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[31] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:47 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
23/12/04 21:37:47 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:37:47 INFO Executor: Running task 0.0 in stage 13.0 (TID 10)
23/12/04 21:37:47 INFO Executor: Finished task 0.0 in stage 13.0 (TID 10). 2042 bytes result sent to driver
23/12/04 21:37:47 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 10 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:47 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
23/12/04 21:37:47 INFO DAGScheduler: ResultStage 13 (parquet at Ast.scala:324) finished in 0.020 s
23/12/04 21:37:47 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
23/12/04 21:37:47 INFO DAGScheduler: Job 10 finished: parquet at Ast.scala:324, took 0.023764 s
23/12/04 21:37:47 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/12/04 21:37:47 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:37:47 INFO DAGScheduler: Got job 11 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:37:47 INFO DAGScheduler: Final stage: ResultStage 14 (parquet at Ast.scala:324)
23/12/04 21:37:47 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:47 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:47 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[33] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:37:47 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:37:47 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:37:47 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.25.86.80:45457 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[33] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:47 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
23/12/04 21:37:47 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:37:47 INFO Executor: Running task 0.0 in stage 14.0 (TID 11)
23/12/04 21:37:47 INFO Executor: Finished task 0.0 in stage 14.0 (TID 11). 2042 bytes result sent to driver
23/12/04 21:37:47 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:47 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
23/12/04 21:37:47 INFO DAGScheduler: ResultStage 14 (parquet at Ast.scala:324) finished in 0.023 s
23/12/04 21:37:47 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
23/12/04 21:37:47 INFO DAGScheduler: Job 11 finished: parquet at Ast.scala:324, took 0.027496 s
23/12/04 21:37:47 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:37:47 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:37:47 INFO CodeGenerator: Code generated in 14.803628 ms
23/12/04 21:37:47 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:37:47 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:37:47 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.25.86.80:45457 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO SparkContext: Created broadcast 12 from show at Ast.scala:95
23/12/04 21:37:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:37:47 INFO SparkContext: Starting job: show at Ast.scala:95
23/12/04 21:37:47 INFO DAGScheduler: Got job 12 (show at Ast.scala:95) with 1 output partitions
23/12/04 21:37:47 INFO DAGScheduler: Final stage: ResultStage 15 (show at Ast.scala:95)
23/12/04 21:37:47 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:47 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:47 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[37] at show at Ast.scala:95), which has no missing parents
23/12/04 21:37:47 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:37:47 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:37:47 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.25.86.80:45457 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[37] at show at Ast.scala:95) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:47 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
23/12/04 21:37:47 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:37:47 INFO Executor: Running task 0.0 in stage 15.0 (TID 12)
23/12/04 21:37:47 INFO CodeGenerator: Code generated in 9.131232 ms
23/12/04 21:37:47 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:37:47 INFO CodecPool: Got brand-new decompressor [.snappy]
23/12/04 21:37:47 INFO Executor: Finished task 0.0 in stage 15.0 (TID 12). 1928 bytes result sent to driver
23/12/04 21:37:47 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 224 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:47 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
23/12/04 21:37:47 INFO DAGScheduler: ResultStage 15 (show at Ast.scala:95) finished in 0.238 s
23/12/04 21:37:47 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
23/12/04 21:37:47 INFO DAGScheduler: Job 12 finished: show at Ast.scala:95, took 0.241192 s
23/12/04 21:37:47 INFO CodeGenerator: Code generated in 6.341942 ms
23/12/04 21:37:47 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:37:47 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:37:47 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:37:47 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:37:47 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.25.86.80:45457 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO SparkContext: Created broadcast 14 from show at Ast.scala:96
23/12/04 21:37:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:37:47 INFO SparkContext: Starting job: show at Ast.scala:96
23/12/04 21:37:47 INFO DAGScheduler: Got job 13 (show at Ast.scala:96) with 1 output partitions
23/12/04 21:37:47 INFO DAGScheduler: Final stage: ResultStage 16 (show at Ast.scala:96)
23/12/04 21:37:47 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:47 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:47 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[41] at show at Ast.scala:96), which has no missing parents
23/12/04 21:37:47 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:37:47 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:37:47 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.25.86.80:45457 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:47 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[41] at show at Ast.scala:96) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:47 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
23/12/04 21:37:47 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 13) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:37:47 INFO Executor: Running task 0.0 in stage 16.0 (TID 13)
23/12/04 21:37:47 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:37:47 INFO Executor: Finished task 0.0 in stage 16.0 (TID 13). 1928 bytes result sent to driver
23/12/04 21:37:47 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 13) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:47 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
23/12/04 21:37:47 INFO DAGScheduler: ResultStage 16 (show at Ast.scala:96) finished in 0.017 s
23/12/04 21:37:47 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
23/12/04 21:37:47 INFO DAGScheduler: Job 13 finished: show at Ast.scala:96, took 0.020161 s
23/12/04 21:37:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:37:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:37:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:37:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 6.807111 ms
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.25.86.80:45457 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 16 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:37:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:37:48 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:37:48 INFO DAGScheduler: Got job 14 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:37:48 INFO DAGScheduler: Final stage: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:37:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:48 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[45] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.25.86.80:45457 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[45] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:48 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
23/12/04 21:37:48 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 14) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:37:48 INFO Executor: Running task 0.0 in stage 17.0 (TID 14)
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 8.905861 ms
23/12/04 21:37:48 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:37:48 INFO Executor: Finished task 0.0 in stage 17.0 (TID 14). 2007 bytes result sent to driver
23/12/04 21:37:48 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 14) in 62 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:48 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
23/12/04 21:37:48 INFO DAGScheduler: ResultStage 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.066 s
23/12/04 21:37:48 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
23/12/04 21:37:48 INFO DAGScheduler: Job 14 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.067756 s
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 3.697451 ms
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.25.86.80:45457 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 18 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:37:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:37:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 13.745966 ms
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.25.86.80:45457 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 19 from show at Ast.scala:108
23/12/04 21:37:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:37:48 INFO SparkContext: Starting job: show at Ast.scala:108
23/12/04 21:37:48 INFO DAGScheduler: Got job 15 (show at Ast.scala:108) with 1 output partitions
23/12/04 21:37:48 INFO DAGScheduler: Final stage: ResultStage 18 (show at Ast.scala:108)
23/12/04 21:37:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:48 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[49] at show at Ast.scala:108), which has no missing parents
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 19.6 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.25.86.80:45457 (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[49] at show at Ast.scala:108) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:48 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
23/12/04 21:37:48 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 15) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:37:48 INFO Executor: Running task 0.0 in stage 18.0 (TID 15)
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 12.47268 ms
23/12/04 21:37:48 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:37:48 INFO Executor: Finished task 0.0 in stage 18.0 (TID 15). 2118 bytes result sent to driver
23/12/04 21:37:48 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 15) in 25 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:48 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
23/12/04 21:37:48 INFO DAGScheduler: ResultStage 18 (show at Ast.scala:108) finished in 0.032 s
23/12/04 21:37:48 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
23/12/04 21:37:48 INFO DAGScheduler: Job 15 finished: show at Ast.scala:108, took 0.033230 s
23/12/04 21:37:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:37:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:37:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:37:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.25.86.80:45457 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 21 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:37:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:37:48 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:37:48 INFO DAGScheduler: Got job 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:37:48 INFO DAGScheduler: Final stage: ResultStage 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:37:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:48 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[53] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.25.86.80:45457 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[53] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:48 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
23/12/04 21:37:48 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 16) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:37:48 INFO Executor: Running task 0.0 in stage 19.0 (TID 16)
23/12/04 21:37:48 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:37:48 INFO Executor: Finished task 0.0 in stage 19.0 (TID 16). 2007 bytes result sent to driver
23/12/04 21:37:48 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 16) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:48 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
23/12/04 21:37:48 INFO DAGScheduler: ResultStage 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.017 s
23/12/04 21:37:48 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
23/12/04 21:37:48 INFO DAGScheduler: Job 16 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.018783 s
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.25.86.80:45457 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 23 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:37:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:37:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.25.86.80:45457 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 24 from show at Ast.scala:112
23/12/04 21:37:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:37:48 INFO SparkContext: Starting job: show at Ast.scala:112
23/12/04 21:37:48 INFO DAGScheduler: Got job 17 (show at Ast.scala:112) with 1 output partitions
23/12/04 21:37:48 INFO DAGScheduler: Final stage: ResultStage 20 (show at Ast.scala:112)
23/12/04 21:37:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:48 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[57] at show at Ast.scala:112), which has no missing parents
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 19.6 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.25.86.80:45457 (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[57] at show at Ast.scala:112) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:48 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
23/12/04 21:37:48 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 17) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:37:48 INFO Executor: Running task 0.0 in stage 20.0 (TID 17)
23/12/04 21:37:48 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:37:48 INFO Executor: Finished task 0.0 in stage 20.0 (TID 17). 2118 bytes result sent to driver
23/12/04 21:37:48 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 17) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:48 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
23/12/04 21:37:48 INFO DAGScheduler: ResultStage 20 (show at Ast.scala:112) finished in 0.013 s
23/12/04 21:37:48 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
23/12/04 21:37:48 INFO DAGScheduler: Job 17 finished: show at Ast.scala:112, took 0.014913 s
23/12/04 21:37:48 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:37:48 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.25.86.80:45457 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 26 from show at Ast.scala:113
23/12/04 21:37:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:37:48 INFO SparkContext: Starting job: show at Ast.scala:113
23/12/04 21:37:48 INFO DAGScheduler: Got job 18 (show at Ast.scala:113) with 1 output partitions
23/12/04 21:37:48 INFO DAGScheduler: Final stage: ResultStage 21 (show at Ast.scala:113)
23/12/04 21:37:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:48 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[61] at show at Ast.scala:113), which has no missing parents
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.25.86.80:45457 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[61] at show at Ast.scala:113) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:48 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
23/12/04 21:37:48 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 18) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:37:48 INFO Executor: Running task 0.0 in stage 21.0 (TID 18)
23/12/04 21:37:48 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:37:48 INFO Executor: Finished task 0.0 in stage 21.0 (TID 18). 1928 bytes result sent to driver
23/12/04 21:37:48 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 18) in 12 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:48 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
23/12/04 21:37:48 INFO DAGScheduler: ResultStage 21 (show at Ast.scala:113) finished in 0.016 s
23/12/04 21:37:48 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
23/12/04 21:37:48 INFO DAGScheduler: Job 18 finished: show at Ast.scala:113, took 0.018704 s
23/12/04 21:37:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:37:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:37:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:37:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 21:37:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:37:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#55)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.25.86.80:45457 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 28 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:37:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:37:48 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:37:48 INFO DAGScheduler: Got job 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:37:48 INFO DAGScheduler: Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:37:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:48 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[65] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.25.86.80:45457 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[65] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:48 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
23/12/04 21:37:48 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 19) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:37:48 INFO Executor: Running task 0.0 in stage 22.0 (TID 19)
23/12/04 21:37:48 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:37:48 INFO Executor: Finished task 0.0 in stage 22.0 (TID 19). 2007 bytes result sent to driver
23/12/04 21:37:48 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 19) in 13 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:48 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
23/12/04 21:37:48 INFO DAGScheduler: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.021 s
23/12/04 21:37:48 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
23/12/04 21:37:48 INFO DAGScheduler: Job 19 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.021983 s
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.25.86.80:45457 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 30 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:37:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:37:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 15.712081 ms
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.25.86.80:45457 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 31 from show at Ast.scala:126
23/12/04 21:37:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:37:48 INFO SparkContext: Starting job: show at Ast.scala:126
23/12/04 21:37:48 INFO DAGScheduler: Got job 20 (show at Ast.scala:126) with 1 output partitions
23/12/04 21:37:48 INFO DAGScheduler: Final stage: ResultStage 23 (show at Ast.scala:126)
23/12/04 21:37:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:48 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[69] at show at Ast.scala:126), which has no missing parents
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 22.4 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.25.86.80:45457 (size: 8.6 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[69] at show at Ast.scala:126) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:48 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
23/12/04 21:37:48 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 20) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:37:48 INFO Executor: Running task 0.0 in stage 23.0 (TID 20)
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 15.619702 ms
23/12/04 21:37:48 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:37:48 INFO Executor: Finished task 0.0 in stage 23.0 (TID 20). 2172 bytes result sent to driver
23/12/04 21:37:48 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 20) in 27 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:48 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
23/12/04 21:37:48 INFO DAGScheduler: ResultStage 23 (show at Ast.scala:126) finished in 0.032 s
23/12/04 21:37:48 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
23/12/04 21:37:48 INFO DAGScheduler: Job 20 finished: show at Ast.scala:126, took 0.034253 s
23/12/04 21:37:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:37:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:37:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:37:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#47)
23/12/04 21:37:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:37:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#55)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.25.86.80:45457 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 33 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:37:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:37:48 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:37:48 INFO DAGScheduler: Got job 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
23/12/04 21:37:48 INFO DAGScheduler: Final stage: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
23/12/04 21:37:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:48 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[73] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.25.86.80:45457 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[73] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:48 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
23/12/04 21:37:48 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 21) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:37:48 INFO Executor: Running task 0.0 in stage 24.0 (TID 21)
23/12/04 21:37:48 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:37:48 INFO Executor: Finished task 0.0 in stage 24.0 (TID 21). 2007 bytes result sent to driver
23/12/04 21:37:48 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 21) in 11 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:48 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
23/12/04 21:37:48 INFO DAGScheduler: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.016 s
23/12/04 21:37:48 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
23/12/04 21:37:48 INFO DAGScheduler: Job 21 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.017255 s
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 1024.1 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 318.0 B, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.25.86.80:45457 (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 35 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
23/12/04 21:37:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(index)
23/12/04 21:37:48 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(index#39)
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 8.40463 ms
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.25.86.80:45457 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 36 from collect at Ast.scala:298
23/12/04 21:37:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 4.51998 ms
23/12/04 21:37:48 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:37:48 INFO DAGScheduler: Got job 22 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:37:48 INFO DAGScheduler: Final stage: ResultStage 25 (collect at Ast.scala:298)
23/12/04 21:37:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:48 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[79] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 25.1 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.25.86.80:45457 (size: 9.8 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[79] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:48 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
23/12/04 21:37:48 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 22) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:37:48 INFO Executor: Running task 0.0 in stage 25.0 (TID 22)
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 12.396814 ms
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 4.680211 ms
23/12/04 21:37:48 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:37:48 INFO Executor: Finished task 0.0 in stage 25.0 (TID 22). 2331 bytes result sent to driver
23/12/04 21:37:48 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 22) in 31 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:48 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
23/12/04 21:37:48 INFO DAGScheduler: ResultStage 25 (collect at Ast.scala:298) finished in 0.037 s
23/12/04 21:37:48 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
23/12/04 21:37:48 INFO DAGScheduler: Job 22 finished: collect at Ast.scala:298, took 0.038539 s
23/12/04 21:37:48 INFO DAGScheduler: Registering RDD 80 (collect at Ast.scala:298) as input to shuffle 3
23/12/04 21:37:48 INFO DAGScheduler: Got map stage job 23 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:37:48 INFO DAGScheduler: Final stage: ShuffleMapStage 26 (collect at Ast.scala:298)
23/12/04 21:37:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:48 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[80] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 27.2 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.25.86.80:45457 (size: 10.9 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[80] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:48 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
23/12/04 21:37:48 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 23) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8468 bytes) 
23/12/04 21:37:48 INFO Executor: Running task 0.0 in stage 26.0 (TID 23)
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 3.469599 ms
23/12/04 21:37:48 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/fea0b5c8d657767c37269b0871dc034e4e03a3284577d3c205a95465b7c24df8.parquet/part-00000-2c0bed7f-ef32-4a8d-b015-314c9362bb56-c000.snappy.parquet, range: 0-1246, partition values: [empty row]
23/12/04 21:37:48 INFO Executor: Finished task 0.0 in stage 26.0 (TID 23). 2298 bytes result sent to driver
23/12/04 21:37:48 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 23) in 16 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:48 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
23/12/04 21:37:48 INFO DAGScheduler: ShuffleMapStage 26 (collect at Ast.scala:298) finished in 0.021 s
23/12/04 21:37:48 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:37:48 INFO DAGScheduler: running: HashSet()
23/12/04 21:37:48 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:37:48 INFO DAGScheduler: failed: HashSet()
23/12/04 21:37:48 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 6.81745 ms
23/12/04 21:37:48 INFO SparkContext: Starting job: collect at Ast.scala:298
23/12/04 21:37:48 INFO DAGScheduler: Got job 24 (collect at Ast.scala:298) with 1 output partitions
23/12/04 21:37:48 INFO DAGScheduler: Final stage: ResultStage 28 (collect at Ast.scala:298)
23/12/04 21:37:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
23/12/04 21:37:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:48 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[83] at collect at Ast.scala:298), which has no missing parents
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 38.1 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.25.86.80:45457 (size: 16.4 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[83] at collect at Ast.scala:298) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:48 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
23/12/04 21:37:48 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 24) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:37:48 INFO Executor: Running task 0.0 in stage 28.0 (TID 24)
23/12/04 21:37:48 INFO ShuffleBlockFetcherIterator: Getting 1 (432.0 B) non-empty blocks including 1 (432.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:37:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 5.297751 ms
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 3.559044 ms
23/12/04 21:37:48 INFO Executor: Finished task 0.0 in stage 28.0 (TID 24). 5956 bytes result sent to driver
23/12/04 21:37:48 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 24) in 19 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:48 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
23/12/04 21:37:48 INFO DAGScheduler: ResultStage 28 (collect at Ast.scala:298) finished in 0.026 s
23/12/04 21:37:48 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
23/12/04 21:37:48 INFO DAGScheduler: Job 24 finished: collect at Ast.scala:298, took 0.027518 s
23/12/04 21:37:48 INFO CodeGenerator: Code generated in 6.610473 ms
23/12/04 21:37:48 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
23/12/04 21:37:48 INFO SparkContext: Starting job: parquet at Ast.scala:324
23/12/04 21:37:48 INFO DAGScheduler: Got job 25 (parquet at Ast.scala:324) with 1 output partitions
23/12/04 21:37:48 INFO DAGScheduler: Final stage: ResultStage 29 (parquet at Ast.scala:324)
23/12/04 21:37:48 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:48 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:48 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[85] at parquet at Ast.scala:324), which has no missing parents
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 103.0 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 9.1 GiB)
23/12/04 21:37:48 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.25.86.80:45457 (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:37:48 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[85] at parquet at Ast.scala:324) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:48 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
23/12/04 21:37:48 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 25) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8073 bytes) 
23/12/04 21:37:48 INFO Executor: Running task 0.0 in stage 29.0 (TID 25)
23/12/04 21:37:48 INFO Executor: Finished task 0.0 in stage 29.0 (TID 25). 2042 bytes result sent to driver
23/12/04 21:37:49 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 25) in 8 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:49 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
23/12/04 21:37:49 INFO DAGScheduler: ResultStage 29 (parquet at Ast.scala:324) finished in 0.017 s
23/12/04 21:37:49 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
23/12/04 21:37:49 INFO DAGScheduler: Job 25 finished: parquet at Ast.scala:324, took 0.018335 s
23/12/04 21:37:49 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:37:49 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:37:49 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 200.9 KiB, free 9.1 GiB)
23/12/04 21:37:49 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.25.86.80:45457 (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO SparkContext: Created broadcast 41 from show at IntegrationSuite.scala:252
23/12/04 21:37:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:37:49 INFO SparkContext: Starting job: show at IntegrationSuite.scala:252
23/12/04 21:37:49 INFO DAGScheduler: Got job 26 (show at IntegrationSuite.scala:252) with 1 output partitions
23/12/04 21:37:49 INFO DAGScheduler: Final stage: ResultStage 30 (show at IntegrationSuite.scala:252)
23/12/04 21:37:49 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:49 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:49 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[89] at show at IntegrationSuite.scala:252), which has no missing parents
23/12/04 21:37:49 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 16.6 KiB, free 9.1 GiB)
23/12/04 21:37:49 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.25.86.80:45457 (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[89] at show at IntegrationSuite.scala:252) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:49 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
23/12/04 21:37:49 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 26) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8479 bytes) 
23/12/04 21:37:49 INFO Executor: Running task 0.0 in stage 30.0 (TID 26)
23/12/04 21:37:49 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/6ed6590ed5b0d3977c42a362e765ef102e2b3acec91e0482e0ab60862c44369d.parquet/part-00000-2be52c4c-cf66-40fd-bb80-45a28c37972e-c000.snappy.parquet, range: 0-1302, partition values: [empty row]
23/12/04 21:37:49 INFO Executor: Finished task 0.0 in stage 30.0 (TID 26). 1961 bytes result sent to driver
23/12/04 21:37:49 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 26) in 9 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:49 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
23/12/04 21:37:49 INFO DAGScheduler: ResultStage 30 (show at IntegrationSuite.scala:252) finished in 0.013 s
23/12/04 21:37:49 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
23/12/04 21:37:49 INFO DAGScheduler: Job 26 finished: show at IntegrationSuite.scala:252, took 0.015055 s
23/12/04 21:37:49 INFO FileSourceStrategy: Pushed Filters: 
23/12/04 21:37:49 INFO FileSourceStrategy: Post-Scan Filters: 
23/12/04 21:37:49 INFO CodeGenerator: Code generated in 10.658757 ms
23/12/04 21:37:49 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 200.4 KiB, free 9.1 GiB)
23/12/04 21:37:49 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.25.86.80:45457 (size: 34.8 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO SparkContext: Created broadcast 43 from count at IntegrationSuite.scala:254
23/12/04 21:37:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/12/04 21:37:49 INFO DAGScheduler: Registering RDD 93 (count at IntegrationSuite.scala:254) as input to shuffle 4
23/12/04 21:37:49 INFO DAGScheduler: Got map stage job 27 (count at IntegrationSuite.scala:254) with 1 output partitions
23/12/04 21:37:49 INFO DAGScheduler: Final stage: ShuffleMapStage 31 (count at IntegrationSuite.scala:254)
23/12/04 21:37:49 INFO DAGScheduler: Parents of final stage: List()
23/12/04 21:37:49 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:49 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[93] at count at IntegrationSuite.scala:254), which has no missing parents
23/12/04 21:37:49 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 17.5 KiB, free 9.1 GiB)
23/12/04 21:37:49 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.25.86.80:45457 (size: 7.9 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[93] at count at IntegrationSuite.scala:254) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:49 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
23/12/04 21:37:49 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 27) (10.25.86.80, executor driver, partition 0, PROCESS_LOCAL, 8468 bytes) 
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 10.25.86.80:45457 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO Executor: Running task 0.0 in stage 31.0 (TID 27)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 10.25.86.80:45457 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 10.25.86.80:45457 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 10.25.86.80:45457 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 10.25.86.80:45457 in memory (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 10.25.86.80:45457 in memory (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 10.25.86.80:45457 in memory (size: 16.4 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO CodeGenerator: Code generated in 6.588078 ms
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 10.25.86.80:45457 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO FileScanRDD: Reading File path: file:///home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/dataframes/6ed6590ed5b0d3977c42a362e765ef102e2b3acec91e0482e0ab60862c44369d.parquet/part-00000-2be52c4c-cf66-40fd-bb80-45a28c37972e-c000.snappy.parquet, range: 0-1302, partition values: [empty row]
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 10.25.86.80:45457 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 10.25.86.80:45457 in memory (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 10.25.86.80:45457 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 10.25.86.80:45457 in memory (size: 9.8 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO Executor: Finished task 0.0 in stage 31.0 (TID 27). 2224 bytes result sent to driver
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 10.25.86.80:45457 in memory (size: 10.9 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 27) in 21 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:49 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
23/12/04 21:37:49 INFO DAGScheduler: ShuffleMapStage 31 (count at IntegrationSuite.scala:254) finished in 0.046 s
23/12/04 21:37:49 INFO DAGScheduler: looking for newly runnable stages
23/12/04 21:37:49 INFO DAGScheduler: running: HashSet()
23/12/04 21:37:49 INFO DAGScheduler: waiting: HashSet()
23/12/04 21:37:49 INFO DAGScheduler: failed: HashSet()
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 10.25.86.80:45457 in memory (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 10.25.86.80:45457 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 10.25.86.80:45457 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 10.25.86.80:45457 in memory (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 10.25.86.80:45457 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO CodeGenerator: Code generated in 7.813387 ms
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 10.25.86.80:45457 in memory (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 10.25.86.80:45457 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 10.25.86.80:45457 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 10.25.86.80:45457 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 10.25.86.80:45457 in memory (size: 8.6 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 10.25.86.80:45457 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 10.25.86.80:45457 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 10.25.86.80:45457 in memory (size: 6.8 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.25.86.80:45457 in memory (size: 37.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 10.25.86.80:45457 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO SparkContext: Starting job: count at IntegrationSuite.scala:254
23/12/04 21:37:49 INFO DAGScheduler: Got job 28 (count at IntegrationSuite.scala:254) with 1 output partitions
23/12/04 21:37:49 INFO DAGScheduler: Final stage: ResultStage 33 (count at IntegrationSuite.scala:254)
23/12/04 21:37:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
23/12/04 21:37:49 INFO DAGScheduler: Missing parents: List()
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 10.25.86.80:45457 in memory (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:37:49 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[96] at count at IntegrationSuite.scala:254), which has no missing parents
23/12/04 21:37:49 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 13.1 KiB, free 9.1 GiB)
23/12/04 21:37:49 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.25.86.80:45457 (size: 6.1 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1580
23/12/04 21:37:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[96] at count at IntegrationSuite.scala:254) (first 15 tasks are for partitions Vector(0))
23/12/04 21:37:49 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
23/12/04 21:37:49 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 28) (10.25.86.80, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 10.25.86.80:45457 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO Executor: Running task 0.0 in stage 33.0 (TID 28)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 10.25.86.80:45457 in memory (size: 318.0 B, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 10.25.86.80:45457 in memory (size: 8.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 10.25.86.80:45457 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/12/04 21:37:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
23/12/04 21:37:49 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 10.25.86.80:45457 in memory (size: 35.0 KiB, free: 9.1 GiB)
23/12/04 21:37:49 INFO CodeGenerator: Code generated in 8.820721 ms
23/12/04 21:37:49 INFO Executor: Finished task 0.0 in stage 33.0 (TID 28). 4084 bytes result sent to driver
23/12/04 21:37:49 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 28) in 16 ms on 10.25.86.80 (executor driver) (1/1)
23/12/04 21:37:49 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
23/12/04 21:37:49 INFO DAGScheduler: ResultStage 33 (count at IntegrationSuite.scala:254) finished in 0.018 s
23/12/04 21:37:49 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
23/12/04 21:37:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
23/12/04 21:37:49 INFO DAGScheduler: Job 28 finished: count at IntegrationSuite.scala:254, took 0.020449 s
23/12/04 21:37:49 INFO undertow: stopping server: Undertow - 2.2.20.Final
23/12/04 21:37:49 INFO SparkContext: Invoking stop() from shutdown hook
23/12/04 21:37:49 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/12/04 21:37:49 INFO SparkUI: Stopped Spark web UI at http://10.25.86.80:4040
23/12/04 21:37:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/12/04 21:37:49 INFO MemoryStore: MemoryStore cleared
23/12/04 21:37:49 INFO BlockManager: BlockManager stopped
23/12/04 21:37:49 INFO BlockManagerMaster: BlockManagerMaster stopped
23/12/04 21:37:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/12/04 21:37:49 INFO SparkContext: Successfully stopped SparkContext
23/12/04 21:37:49 INFO ShutdownHookManager: Shutdown hook called
23/12/04 21:37:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-18d947e3-ce91-41d8-a270-d360db403536
23/12/04 21:53:04 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 21:53:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 21:53:04 INFO SparkContext: Running Spark version 3.5.0
23/12/04 21:53:04 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 21:53:04 INFO SparkContext: Java version 11.0.20
23/12/04 21:53:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 21:57:01 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 21:57:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 21:57:01 INFO SparkContext: Running Spark version 3.5.0
23/12/04 21:57:01 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 21:57:01 INFO SparkContext: Java version 11.0.20
23/12/04 21:57:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 21:57:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 21:59:46 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 21:59:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 21:59:46 INFO SparkContext: Running Spark version 3.5.0
23/12/04 21:59:46 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 21:59:46 INFO SparkContext: Java version 11.0.20
23/12/04 21:59:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 21:59:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/04 22:02:16 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_d8fb67e0/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/04 22:02:29 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 22:02:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 22:02:29 INFO SparkContext: Running Spark version 3.5.0
23/12/04 22:02:29 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 22:02:29 INFO SparkContext: Java version 11.0.20
23/12/04 22:02:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 22:43:50 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_7247f2a7/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/04 22:44:08 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 22:44:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 22:44:08 INFO SparkContext: Running Spark version 3.5.0
23/12/04 22:44:08 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 22:44:08 INFO SparkContext: Java version 11.0.20
23/12/04 22:44:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 22:44:45 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_e56de8cc/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/04 22:44:54 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 22:44:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 22:44:54 INFO SparkContext: Running Spark version 3.5.0
23/12/04 22:44:54 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 22:44:54 INFO SparkContext: Java version 11.0.20
23/12/04 22:44:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 22:50:32 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_5fca3390/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/04 22:50:47 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 22:50:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 22:50:47 INFO SparkContext: Running Spark version 3.5.0
23/12/04 22:50:47 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 22:50:47 INFO SparkContext: Java version 11.0.20
23/12/04 22:50:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 22:59:51 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_b575399f/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/04 23:02:17 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 23:02:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 23:02:17 INFO SparkContext: Running Spark version 3.5.0
23/12/04 23:02:17 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 23:02:17 INFO SparkContext: Java version 11.0.20
23/12/04 23:02:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 23:07:16 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_8769ddb5/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/04 23:07:25 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 23:07:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 23:07:25 INFO SparkContext: Running Spark version 3.5.0
23/12/04 23:07:25 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 23:07:25 INFO SparkContext: Java version 11.0.20
23/12/04 23:07:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 23:10:31 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_66cffd32/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/04 23:10:51 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 23:10:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 23:10:51 INFO SparkContext: Running Spark version 3.5.0
23/12/04 23:10:51 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 23:10:51 INFO SparkContext: Java version 11.0.20
23/12/04 23:10:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 23:19:02 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_6181aae2/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/04 23:19:10 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 23:19:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 23:19:11 INFO SparkContext: Running Spark version 3.5.0
23/12/04 23:19:11 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 23:19:11 INFO SparkContext: Java version 11.0.20
23/12/04 23:19:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 23:23:13 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_c62fbf03/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/04 23:23:28 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 23:23:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 23:23:29 INFO SparkContext: Running Spark version 3.5.0
23/12/04 23:23:29 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 23:23:29 INFO SparkContext: Java version 11.0.20
23/12/04 23:23:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 23:27:37 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_3c3e490/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/04 23:27:48 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 23:27:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 23:27:48 INFO SparkContext: Running Spark version 3.5.0
23/12/04 23:27:48 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 23:27:48 INFO SparkContext: Java version 11.0.20
23/12/04 23:27:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 23:31:10 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_84406d90/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/04 23:31:18 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 23:31:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 23:31:18 INFO SparkContext: Running Spark version 3.5.0
23/12/04 23:31:18 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 23:31:18 INFO SparkContext: Java version 11.0.20
23/12/04 23:31:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/04 23:33:33 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_125d8a67/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/04 23:33:42 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/04 23:33:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/04 23:33:42 INFO SparkContext: Running Spark version 3.5.0
23/12/04 23:33:42 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/04 23:33:42 INFO SparkContext: Java version 11.0.20
23/12/04 23:33:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 11:33:59 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_8c6bbde9/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 11:34:08 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 11:34:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 11:34:08 INFO SparkContext: Running Spark version 3.5.0
23/12/05 11:34:08 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 11:34:08 INFO SparkContext: Java version 11.0.20
23/12/05 11:34:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 11:37:36 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_99b0fa60/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 11:37:44 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 11:37:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 11:37:44 INFO SparkContext: Running Spark version 3.5.0
23/12/05 11:37:44 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 11:37:44 INFO SparkContext: Java version 11.0.20
23/12/05 11:37:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 11:44:44 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_2414babc/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 11:44:55 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 11:44:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 11:44:55 INFO SparkContext: Running Spark version 3.5.0
23/12/05 11:44:55 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 11:44:55 INFO SparkContext: Java version 11.0.20
23/12/05 11:44:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 11:57:09 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_756aaf06/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 11:57:20 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 11:57:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 11:57:20 INFO SparkContext: Running Spark version 3.5.0
23/12/05 11:57:20 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 11:57:20 INFO SparkContext: Java version 11.0.20
23/12/05 11:57:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 14:16:09 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 14:16:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 14:16:10 INFO SparkContext: Running Spark version 3.5.0
23/12/05 14:16:10 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 14:16:10 INFO SparkContext: Java version 11.0.20
23/12/05 14:16:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 14:16:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/05 14:46:19 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_7092edca/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 14:46:33 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 14:46:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 14:46:34 INFO SparkContext: Running Spark version 3.5.0
23/12/05 14:46:34 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 14:46:34 INFO SparkContext: Java version 11.0.20
23/12/05 14:46:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 15:06:35 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_73851c03/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 15:06:45 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 15:06:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 15:06:45 INFO SparkContext: Running Spark version 3.5.0
23/12/05 15:06:45 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 15:06:45 INFO SparkContext: Java version 11.0.20
23/12/05 15:06:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 15:13:58 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_6a4875b1/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 15:14:07 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 15:14:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 15:14:08 INFO SparkContext: Running Spark version 3.5.0
23/12/05 15:14:08 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 15:14:08 INFO SparkContext: Java version 11.0.20
23/12/05 15:14:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 15:26:57 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_2da339ab/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 15:30:24 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 15:30:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 15:30:25 INFO SparkContext: Running Spark version 3.5.0
23/12/05 15:30:25 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 15:30:25 INFO SparkContext: Java version 11.0.20
23/12/05 15:30:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 15:43:01 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_85fd97e0/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 15:43:10 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 15:43:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 15:43:10 INFO SparkContext: Running Spark version 3.5.0
23/12/05 15:43:10 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 15:43:10 INFO SparkContext: Java version 11.0.20
23/12/05 15:43:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 15:49:17 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_447a4b27/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 15:49:33 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 15:49:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 15:49:33 INFO SparkContext: Running Spark version 3.5.0
23/12/05 15:49:33 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 15:49:33 INFO SparkContext: Java version 11.0.20
23/12/05 15:49:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 15:56:31 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_ad27c499/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 15:56:46 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 15:56:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 15:56:46 INFO SparkContext: Running Spark version 3.5.0
23/12/05 15:56:46 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 15:56:46 INFO SparkContext: Java version 11.0.20
23/12/05 15:56:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 15:58:49 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 15:58:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 15:58:49 INFO SparkContext: Running Spark version 3.5.0
23/12/05 15:58:49 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 15:58:49 INFO SparkContext: Java version 11.0.20
23/12/05 15:58:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 15:58:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/05 16:07:54 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_302fc5c7/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 16:08:11 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 16:08:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 16:08:11 INFO SparkContext: Running Spark version 3.5.0
23/12/05 16:08:11 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 16:08:11 INFO SparkContext: Java version 11.0.20
23/12/05 16:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 16:19:42 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_7a4c7346/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 16:19:51 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 16:19:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 16:19:51 INFO SparkContext: Running Spark version 3.5.0
23/12/05 16:19:51 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 16:19:51 INFO SparkContext: Java version 11.0.20
23/12/05 16:19:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 16:38:26 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_13019815/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 16:38:40 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 16:38:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 16:38:40 INFO SparkContext: Running Spark version 3.5.0
23/12/05 16:38:40 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 16:38:40 INFO SparkContext: Java version 11.0.20
23/12/05 16:38:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 16:41:33 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_f8944452/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 16:41:43 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 16:41:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 16:41:43 INFO SparkContext: Running Spark version 3.5.0
23/12/05 16:41:43 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 16:41:43 INFO SparkContext: Java version 11.0.20
23/12/05 16:41:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 16:50:24 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_b78454df/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 16:50:33 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 16:50:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 16:50:33 INFO SparkContext: Running Spark version 3.5.0
23/12/05 16:50:33 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 16:50:33 INFO SparkContext: Java version 11.0.20
23/12/05 16:50:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 16:55:11 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 16:55:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 16:55:11 INFO SparkContext: Running Spark version 3.5.0
23/12/05 16:55:11 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 16:55:11 INFO SparkContext: Java version 11.0.20
23/12/05 16:55:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 16:55:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/05 17:07:16 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_8c9151c7/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 17:12:52 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 17:12:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 17:12:52 INFO SparkContext: Running Spark version 3.5.0
23/12/05 17:12:52 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 17:12:52 INFO SparkContext: Java version 11.0.20
23/12/05 17:12:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 17:19:08 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_919221c6/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 17:19:40 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 17:19:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 17:19:45 INFO SparkContext: Running Spark version 3.5.0
23/12/05 17:19:45 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 17:19:45 INFO SparkContext: Java version 11.0.20
23/12/05 17:19:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 17:25:13 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_53d6084e/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 17:26:23 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 17:26:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 17:26:23 INFO SparkContext: Running Spark version 3.5.0
23/12/05 17:26:23 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 17:26:23 INFO SparkContext: Java version 11.0.20
23/12/05 17:26:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 17:32:08 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_ab819af2/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 17:32:17 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 17:32:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 17:32:17 INFO SparkContext: Running Spark version 3.5.0
23/12/05 17:32:17 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 17:32:17 INFO SparkContext: Java version 11.0.20
23/12/05 17:32:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 17:38:56 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_40781a49/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 17:39:04 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 17:39:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 17:39:04 INFO SparkContext: Running Spark version 3.5.0
23/12/05 17:39:04 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 17:39:04 INFO SparkContext: Java version 11.0.20
23/12/05 17:39:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 17:46:44 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_fb01822b/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 17:46:54 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 17:46:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 17:46:54 INFO SparkContext: Running Spark version 3.5.0
23/12/05 17:46:54 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 17:46:54 INFO SparkContext: Java version 11.0.20
23/12/05 17:46:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 18:25:43 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 18:25:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 18:25:43 INFO SparkContext: Running Spark version 3.5.0
23/12/05 18:25:43 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 18:25:43 INFO SparkContext: Java version 11.0.20
23/12/05 18:25:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 18:25:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/05 18:30:48 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_bfe44658/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 18:30:57 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 18:30:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 18:30:57 INFO SparkContext: Running Spark version 3.5.0
23/12/05 18:30:57 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 18:30:57 INFO SparkContext: Java version 11.0.20
23/12/05 18:30:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 18:52:10 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_cffd2d9b/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 18:52:56 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 18:52:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 18:52:56 INFO SparkContext: Running Spark version 3.5.0
23/12/05 18:52:56 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 18:52:56 INFO SparkContext: Java version 11.0.20
23/12/05 18:52:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 18:58:45 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_61141e12/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 18:58:58 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 18:58:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 18:58:58 INFO SparkContext: Running Spark version 3.5.0
23/12/05 18:58:58 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 18:58:58 INFO SparkContext: Java version 11.0.20
23/12/05 18:58:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 19:01:31 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 19:01:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 19:01:31 INFO SparkContext: Running Spark version 3.5.0
23/12/05 19:01:31 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 19:01:31 INFO SparkContext: Java version 11.0.20
23/12/05 19:01:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 19:01:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/05 19:04:37 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_6d1a41ed/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 19:04:52 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 19:04:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 19:04:53 INFO SparkContext: Running Spark version 3.5.0
23/12/05 19:04:53 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 19:04:53 INFO SparkContext: Java version 11.0.20
23/12/05 19:04:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 19:08:01 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_7445dfe5/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 19:09:15 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 19:09:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 19:09:15 INFO SparkContext: Running Spark version 3.5.0
23/12/05 19:09:15 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 19:09:15 INFO SparkContext: Java version 11.0.20
23/12/05 19:09:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 19:11:41 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 19:11:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 19:11:42 INFO SparkContext: Running Spark version 3.5.0
23/12/05 19:11:42 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 19:11:42 INFO SparkContext: Java version 11.0.20
23/12/05 19:11:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 19:13:58 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 19:13:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 19:13:58 INFO SparkContext: Running Spark version 3.5.0
23/12/05 19:13:58 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 19:13:58 INFO SparkContext: Java version 11.0.20
23/12/05 19:13:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 19:14:19 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 19:14:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 19:14:19 INFO SparkContext: Running Spark version 3.5.0
23/12/05 19:14:19 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 19:14:19 INFO SparkContext: Java version 11.0.20
23/12/05 19:14:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 19:21:15 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 19:21:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 19:21:15 INFO SparkContext: Running Spark version 3.5.0
23/12/05 19:21:15 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 19:21:15 INFO SparkContext: Java version 11.0.20
23/12/05 19:21:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 19:21:47 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 19:21:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 19:21:47 INFO SparkContext: Running Spark version 3.5.0
23/12/05 19:21:47 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 19:21:47 INFO SparkContext: Java version 11.0.20
23/12/05 19:21:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 19:22:01 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 19:22:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 19:22:01 INFO SparkContext: Running Spark version 3.5.0
23/12/05 19:22:01 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 19:22:01 INFO SparkContext: Java version 11.0.20
23/12/05 19:22:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 19:22:38 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 19:22:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 19:22:39 INFO SparkContext: Running Spark version 3.5.0
23/12/05 19:22:39 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 19:22:39 INFO SparkContext: Java version 11.0.20
23/12/05 19:22:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 19:59:30 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 19:59:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 19:59:30 INFO SparkContext: Running Spark version 3.5.0
23/12/05 19:59:30 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 19:59:30 INFO SparkContext: Java version 11.0.20
23/12/05 19:59:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 20:15:12 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 20:15:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 20:15:12 INFO SparkContext: Running Spark version 3.5.0
23/12/05 20:15:12 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 20:15:12 INFO SparkContext: Java version 11.0.20
23/12/05 20:15:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 23:08:39 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_6358f051/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 23:09:04 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 23:09:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 23:09:04 INFO SparkContext: Running Spark version 3.5.0
23/12/05 23:09:04 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 23:09:04 INFO SparkContext: Java version 11.0.20
23/12/05 23:09:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 23:11:47 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_3d25ba2d/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 23:11:57 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 23:11:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 23:11:58 INFO SparkContext: Running Spark version 3.5.0
23/12/05 23:11:58 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 23:11:58 INFO SparkContext: Java version 11.0.20
23/12/05 23:11:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/05 23:16:04 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_5052a9c3/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/05 23:16:12 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/05 23:16:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/05 23:16:13 INFO SparkContext: Running Spark version 3.5.0
23/12/05 23:16:13 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/05 23:16:13 INFO SparkContext: Java version 11.0.20
23/12/05 23:16:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 10:23:32 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_21edad7a/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 10:23:46 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 10:23:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 10:23:46 INFO SparkContext: Running Spark version 3.5.0
23/12/06 10:23:46 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 10:23:46 INFO SparkContext: Java version 11.0.20
23/12/06 10:23:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 10:24:20 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_80ee964d/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 10:24:32 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 10:24:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 10:24:32 INFO SparkContext: Running Spark version 3.5.0
23/12/06 10:24:32 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 10:24:32 INFO SparkContext: Java version 11.0.20
23/12/06 10:24:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 10:26:30 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_7d3edd72/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 10:26:44 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 10:26:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 10:26:44 INFO SparkContext: Running Spark version 3.5.0
23/12/06 10:26:44 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 10:26:44 INFO SparkContext: Java version 11.0.20
23/12/06 10:26:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 10:27:23 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_fbc742c0/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 10:27:53 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 10:27:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 10:27:53 INFO SparkContext: Running Spark version 3.5.0
23/12/06 10:27:53 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 10:27:53 INFO SparkContext: Java version 11.0.20
23/12/06 10:27:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 10:30:27 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 10:30:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 10:30:27 INFO SparkContext: Running Spark version 3.5.0
23/12/06 10:30:27 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 10:30:27 INFO SparkContext: Java version 11.0.20
23/12/06 10:30:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 10:33:58 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_3c268622/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 10:37:16 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 10:37:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 10:37:16 INFO SparkContext: Running Spark version 3.5.0
23/12/06 10:37:16 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 10:37:16 INFO SparkContext: Java version 11.0.20
23/12/06 10:37:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 10:44:44 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_5017a91a/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 10:45:00 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 10:45:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 10:45:00 INFO SparkContext: Running Spark version 3.5.0
23/12/06 10:45:00 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 10:45:00 INFO SparkContext: Java version 11.0.20
23/12/06 10:45:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 10:46:52 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_bb37df24/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 10:47:01 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 10:47:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 10:47:01 INFO SparkContext: Running Spark version 3.5.0
23/12/06 10:47:01 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 10:47:01 INFO SparkContext: Java version 11.0.20
23/12/06 10:47:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 10:47:49 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_7f7267c2/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 10:47:59 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 10:47:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 10:47:59 INFO SparkContext: Running Spark version 3.5.0
23/12/06 10:47:59 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 10:47:59 INFO SparkContext: Java version 11.0.20
23/12/06 10:48:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 10:49:10 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_adbd956c/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 10:49:19 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 10:49:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 10:49:19 INFO SparkContext: Running Spark version 3.5.0
23/12/06 10:49:19 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 10:49:19 INFO SparkContext: Java version 11.0.20
23/12/06 10:49:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 10:51:44 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_7d14a1ff/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 10:51:53 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 10:51:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 10:51:53 INFO SparkContext: Running Spark version 3.5.0
23/12/06 10:51:53 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 10:51:53 INFO SparkContext: Java version 11.0.20
23/12/06 10:51:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 10:53:27 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_ece4450f/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 10:53:36 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 10:53:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 10:53:36 INFO SparkContext: Running Spark version 3.5.0
23/12/06 10:53:36 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 10:53:36 INFO SparkContext: Java version 11.0.20
23/12/06 10:53:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 11:31:47 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_ecbeb35f/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 11:31:56 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 11:31:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 11:31:56 INFO SparkContext: Running Spark version 3.5.0
23/12/06 11:31:56 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 11:31:56 INFO SparkContext: Java version 11.0.20
23/12/06 11:31:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 11:54:37 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_75661d16/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 11:54:46 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 11:54:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 11:54:46 INFO SparkContext: Running Spark version 3.5.0
23/12/06 11:54:46 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 11:54:46 INFO SparkContext: Java version 11.0.20
23/12/06 11:54:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 12:06:03 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_24ca4609/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 12:06:27 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 12:06:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 12:06:27 INFO SparkContext: Running Spark version 3.5.0
23/12/06 12:06:27 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 12:06:27 INFO SparkContext: Java version 11.0.20
23/12/06 12:06:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 12:14:01 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_fcbbb759/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 12:14:12 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 12:14:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 12:14:12 INFO SparkContext: Running Spark version 3.5.0
23/12/06 12:14:12 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 12:14:12 INFO SparkContext: Java version 11.0.20
23/12/06 12:14:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 12:15:35 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_6e24f30b/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 12:17:10 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 12:17:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 12:17:10 INFO SparkContext: Running Spark version 3.5.0
23/12/06 12:17:10 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 12:17:10 INFO SparkContext: Java version 11.0.20
23/12/06 12:17:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 12:19:16 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 12:19:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 12:19:16 INFO SparkContext: Running Spark version 3.5.0
23/12/06 12:19:16 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 12:19:16 INFO SparkContext: Java version 11.0.20
23/12/06 12:19:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 12:22:10 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_65f5d2bc/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 12:22:23 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 12:22:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 12:22:23 INFO SparkContext: Running Spark version 3.5.0
23/12/06 12:22:23 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 12:22:23 INFO SparkContext: Java version 11.0.20
23/12/06 12:22:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 16:40:44 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 16:40:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 16:40:45 INFO SparkContext: Running Spark version 3.5.0
23/12/06 16:40:45 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 16:40:45 INFO SparkContext: Java version 11.0.20
23/12/06 16:40:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 16:51:50 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 16:51:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 16:51:50 INFO SparkContext: Running Spark version 3.5.0
23/12/06 16:51:50 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 16:51:50 INFO SparkContext: Java version 11.0.20
23/12/06 16:51:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 16:57:58 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 16:57:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 16:57:59 INFO SparkContext: Running Spark version 3.5.0
23/12/06 16:57:59 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 16:57:59 INFO SparkContext: Java version 11.0.20
23/12/06 16:57:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 16:57:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/06 17:00:22 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 17:00:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 17:00:22 INFO SparkContext: Running Spark version 3.5.0
23/12/06 17:00:22 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 17:00:22 INFO SparkContext: Java version 11.0.20
23/12/06 17:00:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 17:00:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/06 17:01:34 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 17:01:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 17:01:34 INFO SparkContext: Running Spark version 3.5.0
23/12/06 17:01:34 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 17:01:34 INFO SparkContext: Java version 11.0.20
23/12/06 17:01:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 17:01:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/12/06 17:13:36 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 17:13:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 17:13:36 INFO SparkContext: Running Spark version 3.5.0
23/12/06 17:13:36 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 17:13:36 INFO SparkContext: Java version 11.0.20
23/12/06 17:13:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 17:15:29 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 17:15:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 17:15:29 INFO SparkContext: Running Spark version 3.5.0
23/12/06 17:15:29 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 17:15:29 INFO SparkContext: Java version 11.0.20
23/12/06 17:15:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 17:26:52 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 17:26:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 17:26:52 INFO SparkContext: Running Spark version 3.5.0
23/12/06 17:26:52 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 17:26:52 INFO SparkContext: Java version 11.0.20
23/12/06 17:26:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 17:27:57 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 17:27:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 17:27:58 INFO SparkContext: Running Spark version 3.5.0
23/12/06 17:27:58 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 17:27:58 INFO SparkContext: Java version 11.0.20
23/12/06 17:27:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 17:35:29 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 17:35:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 17:35:29 INFO SparkContext: Running Spark version 3.5.0
23/12/06 17:35:29 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 17:35:29 INFO SparkContext: Java version 11.0.20
23/12/06 17:35:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 17:36:27 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 17:36:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 17:36:27 INFO SparkContext: Running Spark version 3.5.0
23/12/06 17:36:27 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 17:36:27 INFO SparkContext: Java version 11.0.20
23/12/06 17:36:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 17:41:35 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 17:41:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 17:41:35 INFO SparkContext: Running Spark version 3.5.0
23/12/06 17:41:35 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 17:41:35 INFO SparkContext: Java version 11.0.20
23/12/06 17:41:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 18:00:18 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_af6dad8e/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
23/12/06 18:00:51 WARN Utils: Your hostname, CN3126CH011005 resolves to a loopback address: 127.0.1.1; using 10.25.86.80 instead (on interface enp0s31f6)
23/12/06 18:00:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/12/06 18:00:51 INFO SparkContext: Running Spark version 3.5.0
23/12/06 18:00:51 INFO SparkContext: OS info Linux, 6.2.0-37-generic, amd64
23/12/06 18:00:51 INFO SparkContext: Java version 11.0.20
23/12/06 18:00:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/12/06 18:02:50 ERROR Configuration: error parsing conf core-default.xml
java.nio.file.NoSuchFileException: /home/bsoleille/gitea/project-effective-scala-c1-s14-3039-4085/target/bg-jobs/sbt_d6570d30/target/a96afb48/359669fc/hadoop-client-api-3.3.4.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1381)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:840)
	at java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:866)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:855)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.jar.JarFile.<init>(JarFile.java:348)
	at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)
	at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)
	at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
